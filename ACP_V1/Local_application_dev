import json
import sys
import os
from pathlib import Path

# Add Ingest Pipeline to path to use its embedding engine
sys.path.append(os.path.join(os.getcwd(), "LCD_port", "Ingest_pipeline_V4r"))
from core.retrieval_controller import RetrievalController

def hydrate_from_scan(scan_id):
    print(f"üåä Hydrating Memory from Scan: {scan_id}")
    
    # 1. Setup Paths
    scan_dir = Path(f"directory_bundler/bundler_scans/{scan_id}") # Adjust path if needed
    if not scan_dir.exists():
        # Try looking in current dir just in case
        scan_dir = Path(scan_id)
    
    if not scan_dir.exists():
        print(f"‚ùå Scan directory not found: {scan_dir}")
        return

    # 2. Load Metadata
    try:
        with open(scan_dir / "tree.json", "r", encoding="utf-8") as f:
            tree = json.load(f)
        print(f"   - Loaded File Tree")
    except Exception as e:
        print(f"‚ùå Could not load tree.json: {e}")
        return

    # 3. Initialize Memory Controller
    try:
        # This will create/load memory/chroma_db/chroma.sqlite3
        mem = RetrievalController() 
        print("   - Vector Database Connected")
    except Exception as e:
        print(f"‚ùå Failed to initialize RetrievalController: {e}")
        return

    # 4. Ingest Files
    count = 0
    
    # Helper to traverse tree
    def process_node(node):
        nonlocal count
        if node['type'] == 'file':
            file_id = node.get('file_id')
            if file_id:
                process_file(file_id, node['path'], node['name'])
        elif node['type'] == 'directory':
            for child in node.get('children', []):
                process_node(child)

    def process_file(file_id, rel_path, filename):
        nonlocal count
        # Load file metadata
        try:
            meta_path = scan_dir / "files" / f"{file_id}.json"
            if not meta_path.exists(): return
            
            with open(meta_path, "r", encoding="utf-8") as f:
                meta = json.load(f)
                
            chunk_id = meta.get("chunk_id")
            if not chunk_id: return

            # Load actual content from chunks
            # Note: This implies looking up the content in the chunk file
            # For simplicity in this script, we assume strict mapping or 
            # we skip if complex. 
            # *CRITICAL*: If your bundler splits content, we need the chunk loader.
            # Here we will try to find the chunk file.
            
            chunk_path = scan_dir / "chunks" / f"{chunk_id}.json"
            if chunk_path.exists():
                with open(chunk_path, "r", encoding="utf-8") as cf:
                    chunk_data = json.load(cf)
                    # Assuming chunk_data is a dict { file_id: content } or similar
                    # Check specific format of your chunks
                    content = chunk_data.get(file_id, {}).get("content") or chunk_data.get(file_id)
                    
                    if content:
                        mem.add_document(
                            source=str(rel_path),
                            content=content,
                            metadata={"filename": filename, "scan_id": scan_id}
                        )
                        count += 1
                        print(f"     + Ingested: {filename}")
        except Exception as e:
            print(f"     ! Skipped {filename}: {e}")

    # Start Processing
    for node in tree:
        process_node(node)

    print(f"\n‚úÖ Hydration Complete. {count} files added to Long-Term Memory.")

if __name__ == "__main__":
    # Point to the uploaded folder name
    hydrate_from_scan("7ef0ace7")