{
  "project": "RAG Ingestion Snapshot",
  "chunk": 1,
  "uid": "74bc588f",
  "files": [
    {
      "path": ".env",
      "content_block": "\"\"\".env\nMONGO_URI=mongodb://localhost:27017\nLM_STUDIO_URL=http://localhost:1234/v1\n# API Key is optional for local LM Studio, but typically required by the client code structure\nOPENAI_API_KEY=lm-studio\n\"\"\"",
      "raw_content": "MONGO_URI=mongodb://localhost:27017\nLM_STUDIO_URL=http://localhost:1234/v1\n# API Key is optional for local LM Studio, but typically required by the client code structure\nOPENAI_API_KEY=lm-studio",
      "size_bytes": 206,
      "labels": {
        "file_type": "config",
        "file_extension": "",
        "path_hash": "f579cccc964135c7d644c7b2d3b0d3ec"
      },
      "analysis": null
    },
    {
      "path": "orchestrator.py",
      "content_block": "\"\"\"orchestrator.py\nimport argparse\nimport sys\nimport re\nimport logging\nfrom pathlib import Path\n\n# --- PATH CORRECTION ---\n# Ensure project root is in sys.path so 'core' and 'utils' can be imported\n# regardless of where the script is run from.\nproject_root = Path(__file__).resolve().parent\nif str(project_root) not in sys.path:\n    sys.path.append(str(project_root))\n\nfrom core.ingest_manager import IngestManager\nfrom core.retrieval_controller import RetrievalController\n\n# Configure logging if not already configured\nif not logging.getLogger().handlers:\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n        handlers=[\n            logging.StreamHandler()\n        ]\n    )\n\ndef sanitize_input(text: str) -> str:\n    \"\"\"\n    Removes potentially problematic characters from query strings.\n    \"\"\"\n    if not text: return \"\"\n    return re.sub(r'[^\\w\\s\\.\\-\\?\\!]', '', text).strip()\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"Aletheia RAG CLI - Technical Enhancements Build\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\n        \"mode\", \n        choices=[\"ingest\", \"ask\"], \n        help=\"System mode: 'ingest' to process documents, 'ask' to query the brain.\"\n    )\n    parser.add_argument(\n        \"--q\", \n        help=\"The research question for Aletheia (required for 'ask' mode)\"\n    )\n    args = parser.parse_args()\n\n    if args.mode == \"ingest\":\n        print(\"\\n[INIT] Starting Aletheia Ingestion Engine...\")\n        print(\"[INFO] Scanning 'data/raw_landing' for new intelligence...\")\n        try:\n            manager = IngestManager()\n            manager.process_all()\n            print(\"\\n[SUCCESS] Ingestion cycle complete.\\n\")\n        except Exception as e:\n            # Catch fatal errors (config issues, missing folders)\n            logging.error(f\"Ingestion failed: {e}\")\n            print(f\"\\n[CRITICAL] System failure during ingestion: {e}\")\n            sys.exit(1)\n    \n    elif args.mode == \"ask\":\n        if not args.q:\n            print(\"\\n[ERROR] 'ask' mode requires a query. Use: --q 'your question'\")\n            sys.exit(1)\n            \n        clean_q = sanitize_input(args.q)\n        print(f\"\\n[QUERY] Researching: '{clean_q}'\")\n        print(\"[INFO] Accessing semantic memory and canonical truth...\")\n        \n        try:\n            controller = RetrievalController()\n            answer = controller.query(clean_q)\n            \n            print(\"\\n\" + \"=\"*60)\n            print(\" ALETHEIA EXPERT RESPONSE\")\n            print(\"=\"*60)\n            print(answer)\n            print(\"=\"*60 + \"\\n\")\n        except Exception as e:\n            logging.error(f\"Retrieval failed: {e}\")\n            print(f\"\\n[CRITICAL] Inference engine error: {e}\")\n            sys.exit(1)\n\nif __name__ == \"__main__\":\n    try:\n        main()\n    except KeyboardInterrupt:\n        print(\"\\n[HALT] Shutdown signal received. Exiting gracefully.\")\n        sys.exit(0)\n    except Exception as e:\n        print(f\"\\n[FATAL] Unhandled error: {e}\")\n        sys.exit(1)\n\"\"\"",
      "size_bytes": 3116,
      "labels": {
        "ast_node_count": 412,
        "function_count": 2,
        "class_count": 0,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "2aa227cc2c894cc2b12ebfdf445352bd"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 412,
        "function_count": 2,
        "class_count": 0,
        "imports": [
          "argparse",
          "core",
          "logging",
          "pathlib",
          "re",
          "sys"
        ],
        "dangerous_calls": [],
        "io_functions": [
          "print"
        ],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "RAG_System_Bundler.py",
      "content_block": "\"\"\"RAG_System_Bundler.py\nimport os\nimport json\nfrom pathlib import Path\n\ndef create_verification_snapshot(output_name=\"RAG_System_Deep_Snapshot.json\"):\n    \"\"\"\n    Scans all project files and their contents for code and telemetry verification.\n    Includes logs and config files usually ignored in standard builds.\n    \"\"\"\n    snapshot = {\n        \"project\": \"RAG Ingestion Pipeline (V4)\",\n        \"purpose\": \"Code & Telemetry Verification\",\n        \"directory_structure\": [],\n        \"files\": []\n    }\n\n    # Pruned ignore list: We now WANT to see logs and env files\n    ignore_dirs = {'__pycache__', '.vs', '.git', '.idea', 'venv', 'env'}\n    # Only skip actual heavy binaries that can't be read as text\n    binary_extensions = {'.pyc', '.exe', '.dll', '.lib', '.pdf', '.zip', '.sqlite', '.h5'}\n\n    base_dir = Path(__file__).parent.resolve()\n    print(f\"--- Initiating Deep Verification Scan ---\")\n    print(f\"Scanning: {base_dir}\")\n\n    file_count = 0\n    \n    for root, dirs, files in os.walk(base_dir):\n        # Prune basic system dirs\n        dirs[:] = [d for d in dirs if d not in ignore_dirs]\n        \n        relative_root = Path(root).relative_to(base_dir)\n        depth = len(relative_root.parts)\n        indent = \"  \" * depth\n        \n        if root != str(base_dir):\n            snapshot[\"directory_structure\"].append(f\"{indent}[DIR] {relative_root.as_posix()}\")\n\n        for file in files:\n            path = Path(root) / file\n            rel_path = path.relative_to(base_dir).as_posix()\n            \n            # Map the structure\n            file_indent = \"  \" * (depth + 1)\n            snapshot[\"directory_structure\"].append(f\"{file_indent}[FILE] {file}\")\n\n            # Skip binaries, but read everything else (logs, env, py, json)\n            if path.suffix.lower() in binary_extensions or file == output_name:\n                continue\n                \n            try:\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n                    content = f.read()\n                \n                # Determine module or category\n                parts = Path(rel_path).parts\n                category = parts[0] if len(parts) > 1 else \"root\"\n\n                print(f\"Indexing for Verification: {rel_path}\")\n\n                snapshot[\"files\"].append({\n                    \"path\": rel_path,\n                    \"category\": category,\n                    \"content\": content,\n                    \"size_chars\": len(content)\n                })\n                file_count += 1\n                \n            except Exception as e:\n                print(f\"Could not read {rel_path}: {e}\")\n\n    # Save the exhaustive snapshot\n    try:\n        output_path = base_dir / output_name\n        with open(output_path, 'w', encoding='utf-8') as f:\n            json.dump(snapshot, f, indent=2)\n        print(f\"\\n--- Scan Complete ---\")\n        print(f\"Verification file created: {output_path}\")\n        print(f\"Total source/log files captured: {file_count}\")\n    except Exception as e:\n        print(f\"Critical error writing snapshot: {e}\")\n\nif __name__ == \"__main__\":\n    create_verification_snapshot()\n\"\"\"",
      "size_bytes": 3129,
      "labels": {
        "ast_node_count": 443,
        "function_count": 1,
        "class_count": 0,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "d9a556184a57524803e3b2769c5aa9d8"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 443,
        "function_count": 1,
        "class_count": 0,
        "imports": [
          "json",
          "os",
          "pathlib"
        ],
        "dangerous_calls": [],
        "io_functions": [
          "open",
          "print"
        ],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "report_summary_bce8782c.json",
      "content_block": "\"\"\"report_summary_bce8782c.json\n{\n  \"uid\": \"bce8782c\",\n  \"total_files_scanned\": 12,\n  \"total_chunks_created\": 1,\n  \"scan_timestamp\": \"2026-01-31T00:29:37.621473\",\n  \"analysis_summary\": \"Analysis data not aggregated in summary.\"\n}\n\"\"\"",
      "raw_content": "{\n  \"uid\": \"bce8782c\",\n  \"total_files_scanned\": 12,\n  \"total_chunks_created\": 1,\n  \"scan_timestamp\": \"2026-01-31T00:29:37.621473\",\n  \"analysis_summary\": \"Analysis data not aggregated in summary.\"\n}",
      "size_bytes": 233,
      "labels": {
        "file_type": "config",
        "file_extension": ".json",
        "path_hash": "00e356d273dbac06b14c43014749e531"
      },
      "analysis": null
    },
    {
      "path": "scan_DIRECTORY_MAP_bce8782c.json",
      "content_block": "\"\"\"scan_DIRECTORY_MAP_bce8782c.json\n[\n  \"   [FILE] .env\",\n  \"   [FILE] orchestrator.py\",\n  \"   [FILE] RAG_System_Bundler.py\",\n  \"   [DIR] config\",\n  \"      [FILE] settings.py\",\n  \"   [DIR] core\",\n  \"      [FILE] codebase_processor.py\",\n  \"      [FILE] ingest_manager.py\",\n  \"      [FILE] pdf_processor.py\",\n  \"      [FILE] retrieval_controller.py\",\n  \"   [DIR] data\",\n  \"      [DIR] data/processed_archive\",\n  \"         [DIR] data/processed_archive/backups\",\n  \"      [DIR] data/raw_landing\",\n  \"   [DIR] logs\",\n  \"   [DIR] settings\",\n  \"      [FILE] init.py\",\n  \"   [DIR] utils\",\n  \"      [FILE] embedding_client.py\",\n  \"      [FILE] metadata_extractor.py\",\n  \"      [FILE] ocr_service.py\"\n]\n\"\"\"",
      "raw_content": "[\n  \"   [FILE] .env\",\n  \"   [FILE] orchestrator.py\",\n  \"   [FILE] RAG_System_Bundler.py\",\n  \"   [DIR] config\",\n  \"      [FILE] settings.py\",\n  \"   [DIR] core\",\n  \"      [FILE] codebase_processor.py\",\n  \"      [FILE] ingest_manager.py\",\n  \"      [FILE] pdf_processor.py\",\n  \"      [FILE] retrieval_controller.py\",\n  \"   [DIR] data\",\n  \"      [DIR] data/processed_archive\",\n  \"         [DIR] data/processed_archive/backups\",\n  \"      [DIR] data/raw_landing\",\n  \"   [DIR] logs\",\n  \"   [DIR] settings\",\n  \"      [FILE] init.py\",\n  \"   [DIR] utils\",\n  \"      [FILE] embedding_client.py\",\n  \"      [FILE] metadata_extractor.py\",\n  \"      [FILE] ocr_service.py\"\n]",
      "size_bytes": 696,
      "labels": {
        "file_type": "config",
        "file_extension": ".json",
        "path_hash": "408d0d35f887944da1b489677e871676"
      },
      "analysis": null
    },
    {
      "path": "scan_part1_bce8782c.json",
      "content_block": "\"\"\"scan_part1_bce8782c.json\n{\n  \"project\": \"RAG Ingestion Snapshot\",\n  \"chunk\": 1,\n  \"uid\": \"bce8782c\",\n  \"files\": [\n    {\n      \"path\": \".env\",\n      \"content_block\": \"\\\"\\\"\\\".env\\nMONGO_URI=mongodb://localhost:27017\\nLM_STUDIO_URL=http://localhost:1234/v1\\n# API Key is optional for local LM Studio, but typically required by the client code structure\\nOPENAI_API_KEY=lm-studio\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"MONGO_URI=mongodb://localhost:27017\\nLM_STUDIO_URL=http://localhost:1234/v1\\n# API Key is optional for local LM Studio, but typically required by the client code structure\\nOPENAI_API_KEY=lm-studio\",\n      \"size_bytes\": 206,\n      \"labels\": {\n        \"file_type\": \"config\",\n        \"file_extension\": \"\",\n        \"path_hash\": \"f579cccc964135c7d644c7b2d3b0d3ec\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"orchestrator.py\",\n      \"content_block\": \"\\\"\\\"\\\"orchestrator.py\\nimport argparse\\nimport sys\\nimport re\\nimport logging\\nfrom pathlib import Path\\n\\n# --- PATH CORRECTION ---\\n# Ensure project root is in sys.path so 'core' and 'utils' can be imported\\n# regardless of where the script is run from.\\nproject_root = Path(__file__).resolve().parent\\nif str(project_root) not in sys.path:\\n    sys.path.append(str(project_root))\\n\\nfrom core.ingest_manager import IngestManager\\nfrom core.retrieval_controller import RetrievalController\\n\\n# Configure logging if not already configured\\nif not logging.getLogger().handlers:\\n    logging.basicConfig(\\n        level=logging.INFO,\\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n        handlers=[\\n            logging.StreamHandler()\\n        ]\\n    )\\n\\ndef sanitize_input(text: str) -> str:\\n    \\\"\\\"\\\"\\n    Removes potentially problematic characters from query strings.\\n    \\\"\\\"\\\"\\n    if not text: return \\\"\\\"\\n    return re.sub(r'[^\\\\w\\\\s\\\\.\\\\-\\\\?\\\\!]', '', text).strip()\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Aletheia RAG CLI - Technical Enhancements Build\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n    parser.add_argument(\\n        \\\"mode\\\", \\n        choices=[\\\"ingest\\\", \\\"ask\\\"], \\n        help=\\\"System mode: 'ingest' to process documents, 'ask' to query the brain.\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--q\\\", \\n        help=\\\"The research question for Aletheia (required for 'ask' mode)\\\"\\n    )\\n    args = parser.parse_args()\\n\\n    if args.mode == \\\"ingest\\\":\\n        print(\\\"\\\\n[INIT] Starting Aletheia Ingestion Engine...\\\")\\n        print(\\\"[INFO] Scanning 'data/raw_landing' for new intelligence...\\\")\\n        try:\\n            manager = IngestManager()\\n            manager.process_all()\\n            print(\\\"\\\\n[SUCCESS] Ingestion cycle complete.\\\\n\\\")\\n        except Exception as e:\\n            # Catch fatal errors (config issues, missing folders)\\n            logging.error(f\\\"Ingestion failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] System failure during ingestion: {e}\\\")\\n            sys.exit(1)\\n    \\n    elif args.mode == \\\"ask\\\":\\n        if not args.q:\\n            print(\\\"\\\\n[ERROR] 'ask' mode requires a query. Use: --q 'your question'\\\")\\n            sys.exit(1)\\n            \\n        clean_q = sanitize_input(args.q)\\n        print(f\\\"\\\\n[QUERY] Researching: '{clean_q}'\\\")\\n        print(\\\"[INFO] Accessing semantic memory and canonical truth...\\\")\\n        \\n        try:\\n            controller = RetrievalController()\\n            answer = controller.query(clean_q)\\n            \\n            print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n            print(\\\" ALETHEIA EXPERT RESPONSE\\\")\\n            print(\\\"=\\\"*60)\\n            print(answer)\\n            print(\\\"=\\\"*60 + \\\"\\\\n\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Retrieval failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] Inference engine error: {e}\\\")\\n            sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    try:\\n        main()\\n    except KeyboardInterrupt:\\n        print(\\\"\\\\n[HALT] Shutdown signal received. Exiting gracefully.\\\")\\n        sys.exit(0)\\n    except Exception as e:\\n        print(f\\\"\\\\n[FATAL] Unhandled error: {e}\\\")\\n        sys.exit(1)\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import argparse\\nimport sys\\nimport re\\nimport logging\\nfrom pathlib import Path\\n\\n# --- PATH CORRECTION ---\\n# Ensure project root is in sys.path so 'core' and 'utils' can be imported\\n# regardless of where the script is run from.\\nproject_root = Path(__file__).resolve().parent\\nif str(project_root) not in sys.path:\\n    sys.path.append(str(project_root))\\n\\nfrom core.ingest_manager import IngestManager\\nfrom core.retrieval_controller import RetrievalController\\n\\n# Configure logging if not already configured\\nif not logging.getLogger().handlers:\\n    logging.basicConfig(\\n        level=logging.INFO,\\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n        handlers=[\\n            logging.StreamHandler()\\n        ]\\n    )\\n\\ndef sanitize_input(text: str) -> str:\\n    \\\"\\\"\\\"\\n    Removes potentially problematic characters from query strings.\\n    \\\"\\\"\\\"\\n    if not text: return \\\"\\\"\\n    return re.sub(r'[^\\\\w\\\\s\\\\.\\\\-\\\\?\\\\!]', '', text).strip()\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Aletheia RAG CLI - Technical Enhancements Build\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n    parser.add_argument(\\n        \\\"mode\\\", \\n        choices=[\\\"ingest\\\", \\\"ask\\\"], \\n        help=\\\"System mode: 'ingest' to process documents, 'ask' to query the brain.\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--q\\\", \\n        help=\\\"The research question for Aletheia (required for 'ask' mode)\\\"\\n    )\\n    args = parser.parse_args()\\n\\n    if args.mode == \\\"ingest\\\":\\n        print(\\\"\\\\n[INIT] Starting Aletheia Ingestion Engine...\\\")\\n        print(\\\"[INFO] Scanning 'data/raw_landing' for new intelligence...\\\")\\n        try:\\n            manager = IngestManager()\\n            manager.process_all()\\n            print(\\\"\\\\n[SUCCESS] Ingestion cycle complete.\\\\n\\\")\\n        except Exception as e:\\n            # Catch fatal errors (config issues, missing folders)\\n            logging.error(f\\\"Ingestion failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] System failure during ingestion: {e}\\\")\\n            sys.exit(1)\\n    \\n    elif args.mode == \\\"ask\\\":\\n        if not args.q:\\n            print(\\\"\\\\n[ERROR] 'ask' mode requires a query. Use: --q 'your question'\\\")\\n            sys.exit(1)\\n            \\n        clean_q = sanitize_input(args.q)\\n        print(f\\\"\\\\n[QUERY] Researching: '{clean_q}'\\\")\\n        print(\\\"[INFO] Accessing semantic memory and canonical truth...\\\")\\n        \\n        try:\\n            controller = RetrievalController()\\n            answer = controller.query(clean_q)\\n            \\n            print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n            print(\\\" ALETHEIA EXPERT RESPONSE\\\")\\n            print(\\\"=\\\"*60)\\n            print(answer)\\n            print(\\\"=\\\"*60 + \\\"\\\\n\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Retrieval failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] Inference engine error: {e}\\\")\\n            sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    try:\\n        main()\\n    except KeyboardInterrupt:\\n        print(\\\"\\\\n[HALT] Shutdown signal received. Exiting gracefully.\\\")\\n        sys.exit(0)\\n    except Exception as e:\\n        print(f\\\"\\\\n[FATAL] Unhandled error: {e}\\\")\\n        sys.exit(1)\",\n      \"size_bytes\": 3116,\n      \"labels\": {\n        \"ast_node_count\": 412,\n        \"function_count\": 2,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"2aa227cc2c894cc2b12ebfdf445352bd\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"RAG_System_Bundler.py\",\n      \"content_block\": \"\\\"\\\"\\\"RAG_System_Bundler.py\\nimport os\\nimport json\\nfrom pathlib import Path\\n\\ndef create_verification_snapshot(output_name=\\\"RAG_System_Deep_Snapshot.json\\\"):\\n    \\\"\\\"\\\"\\n    Scans all project files and their contents for code and telemetry verification.\\n    Includes logs and config files usually ignored in standard builds.\\n    \\\"\\\"\\\"\\n    snapshot = {\\n        \\\"project\\\": \\\"RAG Ingestion Pipeline (V4)\\\",\\n        \\\"purpose\\\": \\\"Code & Telemetry Verification\\\",\\n        \\\"directory_structure\\\": [],\\n        \\\"files\\\": []\\n    }\\n\\n    # Pruned ignore list: We now WANT to see logs and env files\\n    ignore_dirs = {'__pycache__', '.vs', '.git', '.idea', 'venv', 'env'}\\n    # Only skip actual heavy binaries that can't be read as text\\n    binary_extensions = {'.pyc', '.exe', '.dll', '.lib', '.pdf', '.zip', '.sqlite', '.h5'}\\n\\n    base_dir = Path(__file__).parent.resolve()\\n    print(f\\\"--- Initiating Deep Verification Scan ---\\\")\\n    print(f\\\"Scanning: {base_dir}\\\")\\n\\n    file_count = 0\\n    \\n    for root, dirs, files in os.walk(base_dir):\\n        # Prune basic system dirs\\n        dirs[:] = [d for d in dirs if d not in ignore_dirs]\\n        \\n        relative_root = Path(root).relative_to(base_dir)\\n        depth = len(relative_root.parts)\\n        indent = \\\"  \\\" * depth\\n        \\n        if root != str(base_dir):\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{indent}[DIR] {relative_root.as_posix()}\\\")\\n\\n        for file in files:\\n            path = Path(root) / file\\n            rel_path = path.relative_to(base_dir).as_posix()\\n            \\n            # Map the structure\\n            file_indent = \\\"  \\\" * (depth + 1)\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{file_indent}[FILE] {file}\\\")\\n\\n            # Skip binaries, but read everything else (logs, env, py, json)\\n            if path.suffix.lower() in binary_extensions or file == output_name:\\n                continue\\n                \\n            try:\\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\\n                    content = f.read()\\n                \\n                # Determine module or category\\n                parts = Path(rel_path).parts\\n                category = parts[0] if len(parts) > 1 else \\\"root\\\"\\n\\n                print(f\\\"Indexing for Verification: {rel_path}\\\")\\n\\n                snapshot[\\\"files\\\"].append({\\n                    \\\"path\\\": rel_path,\\n                    \\\"category\\\": category,\\n                    \\\"content\\\": content,\\n                    \\\"size_chars\\\": len(content)\\n                })\\n                file_count += 1\\n                \\n            except Exception as e:\\n                print(f\\\"Could not read {rel_path}: {e}\\\")\\n\\n    # Save the exhaustive snapshot\\n    try:\\n        output_path = base_dir / output_name\\n        with open(output_path, 'w', encoding='utf-8') as f:\\n            json.dump(snapshot, f, indent=2)\\n        print(f\\\"\\\\n--- Scan Complete ---\\\")\\n        print(f\\\"Verification file created: {output_path}\\\")\\n        print(f\\\"Total source/log files captured: {file_count}\\\")\\n    except Exception as e:\\n        print(f\\\"Critical error writing snapshot: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    create_verification_snapshot()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import os\\nimport json\\nfrom pathlib import Path\\n\\ndef create_verification_snapshot(output_name=\\\"RAG_System_Deep_Snapshot.json\\\"):\\n    \\\"\\\"\\\"\\n    Scans all project files and their contents for code and telemetry verification.\\n    Includes logs and config files usually ignored in standard builds.\\n    \\\"\\\"\\\"\\n    snapshot = {\\n        \\\"project\\\": \\\"RAG Ingestion Pipeline (V4)\\\",\\n        \\\"purpose\\\": \\\"Code & Telemetry Verification\\\",\\n        \\\"directory_structure\\\": [],\\n        \\\"files\\\": []\\n    }\\n\\n    # Pruned ignore list: We now WANT to see logs and env files\\n    ignore_dirs = {'__pycache__', '.vs', '.git', '.idea', 'venv', 'env'}\\n    # Only skip actual heavy binaries that can't be read as text\\n    binary_extensions = {'.pyc', '.exe', '.dll', '.lib', '.pdf', '.zip', '.sqlite', '.h5'}\\n\\n    base_dir = Path(__file__).parent.resolve()\\n    print(f\\\"--- Initiating Deep Verification Scan ---\\\")\\n    print(f\\\"Scanning: {base_dir}\\\")\\n\\n    file_count = 0\\n    \\n    for root, dirs, files in os.walk(base_dir):\\n        # Prune basic system dirs\\n        dirs[:] = [d for d in dirs if d not in ignore_dirs]\\n        \\n        relative_root = Path(root).relative_to(base_dir)\\n        depth = len(relative_root.parts)\\n        indent = \\\"  \\\" * depth\\n        \\n        if root != str(base_dir):\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{indent}[DIR] {relative_root.as_posix()}\\\")\\n\\n        for file in files:\\n            path = Path(root) / file\\n            rel_path = path.relative_to(base_dir).as_posix()\\n            \\n            # Map the structure\\n            file_indent = \\\"  \\\" * (depth + 1)\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{file_indent}[FILE] {file}\\\")\\n\\n            # Skip binaries, but read everything else (logs, env, py, json)\\n            if path.suffix.lower() in binary_extensions or file == output_name:\\n                continue\\n                \\n            try:\\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\\n                    content = f.read()\\n                \\n                # Determine module or category\\n                parts = Path(rel_path).parts\\n                category = parts[0] if len(parts) > 1 else \\\"root\\\"\\n\\n                print(f\\\"Indexing for Verification: {rel_path}\\\")\\n\\n                snapshot[\\\"files\\\"].append({\\n                    \\\"path\\\": rel_path,\\n                    \\\"category\\\": category,\\n                    \\\"content\\\": content,\\n                    \\\"size_chars\\\": len(content)\\n                })\\n                file_count += 1\\n                \\n            except Exception as e:\\n                print(f\\\"Could not read {rel_path}: {e}\\\")\\n\\n    # Save the exhaustive snapshot\\n    try:\\n        output_path = base_dir / output_name\\n        with open(output_path, 'w', encoding='utf-8') as f:\\n            json.dump(snapshot, f, indent=2)\\n        print(f\\\"\\\\n--- Scan Complete ---\\\")\\n        print(f\\\"Verification file created: {output_path}\\\")\\n        print(f\\\"Total source/log files captured: {file_count}\\\")\\n    except Exception as e:\\n        print(f\\\"Critical error writing snapshot: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    create_verification_snapshot()\",\n      \"size_bytes\": 3129,\n      \"labels\": {\n        \"ast_node_count\": 443,\n        \"function_count\": 1,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"d9a556184a57524803e3b2769c5aa9d8\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"config/settings.py\",\n      \"content_block\": \"\\\"\\\"\\\"config/settings.py\\nimport os\\nimport logging\\nfrom pathlib import Path\\nfrom typing import Final, Optional\\nfrom dotenv import load_dotenv\\n\\n# Load environmental variables\\nload_dotenv()\\n\\n# Global Logging Configuration\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n    handlers=[\\n        logging.FileHandler('aletheia_system.log'),\\n        logging.StreamHandler()\\n    ]\\n)\\n\\nclass Settings:\\n    \\\"\\\"\\\"\\n    Centralized configuration engine for Aletheia RAG Infrastructure.\\n    \\\"\\\"\\\"\\n    # Section 1: Directory Management\\n    # Resolves to the parent of 'config', which is the root 'Ingest_pipeline_V2'\\n    BASE_DIR: Final[Path] = Path(__file__).resolve().parent.parent\\n    \\n    DATA_DIR: Final[Path] = BASE_DIR / \\\"data\\\"\\n    RAW_LANDING_DIR: Final[Path] = DATA_DIR / \\\"raw_landing\\\"\\n    PROCESSED_ARCHIVE_DIR: Final[Path] = DATA_DIR / \\\"processed_archive\\\"\\n    BACKUP_DIR: Final[Path] = PROCESSED_ARCHIVE_DIR / \\\"backups\\\"\\n    \\n    # Section 2: Storage Paths\\n    CHROMA_DB_PATH: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\"chroma_db\\\"\\n    EMBEDDING_CACHE_DIR: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\".embedding_cache\\\"\\n    USAGE_LOG_PATH: Final[Path] = BASE_DIR / \\\"logs\\\" / \\\"usage_stats.json\\\"\\n\\n    # Section 3: Database (MongoDB)\\n    MONGO_URI: Final[str] = os.getenv(\\\"MONGO_URI\\\", \\\"mongodb://localhost:27017\\\")\\n    DB_NAME: Final[str] = \\\"aletheia_memory\\\"\\n    COLLECTION_TRUTH: Final[str] = \\\"canonical_truth\\\"\\n    COLLECTION_TRACES: Final[str] = \\\"reasoning_traces\\\"\\n\\n    # Section 4: Inference (LM Studio)\\n    LM_STUDIO_BASE_URL: Final[str] = os.getenv(\\\"LM_STUDIO_URL\\\", \\\"http://localhost:1234/v1\\\")\\n    EMBEDDING_MODEL: Final[str] = \\\"nomic-ai/nomic-embed-text-v1.5-GGUF\\\"\\n    NOMIC_PREFIX: Final[str] = \\\"search_document: \\\" \\n\\n    # Section 5: RAG & OCR Logic\\n    CHUNK_SIZE: Final[int] = 1500 \\n    CHUNK_OVERLAP: Final[int] = 200\\n    OCR_TEXT_DENSITY_THRESHOLD: int = 50 # Characters per page below which OCR is triggered\\n    NUM_RETRIEVAL_RESULTS: int = 5\\n\\n    def validate_settings(self):\\n        \\\"\\\"\\\"Ensures directories exist and critical settings are present.\\\"\\\"\\\"\\n        paths = [\\n            self.DATA_DIR, self.RAW_LANDING_DIR, self.PROCESSED_ARCHIVE_DIR, \\n            self.BACKUP_DIR, self.CHROMA_DB_PATH, self.USAGE_LOG_PATH.parent,\\n            self.EMBEDDING_CACHE_DIR\\n        ]\\n        for p in paths:\\n            p.mkdir(parents=True, exist_ok=True)\\n        \\n        if not self.MONGO_URI:\\n            raise ValueError(\\\"MONGO_URI environment variable is missing.\\\")\\n\\nsettings = Settings()\\nsettings.validate_settings()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import os\\nimport logging\\nfrom pathlib import Path\\nfrom typing import Final, Optional\\nfrom dotenv import load_dotenv\\n\\n# Load environmental variables\\nload_dotenv()\\n\\n# Global Logging Configuration\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n    handlers=[\\n        logging.FileHandler('aletheia_system.log'),\\n        logging.StreamHandler()\\n    ]\\n)\\n\\nclass Settings:\\n    \\\"\\\"\\\"\\n    Centralized configuration engine for Aletheia RAG Infrastructure.\\n    \\\"\\\"\\\"\\n    # Section 1: Directory Management\\n    # Resolves to the parent of 'config', which is the root 'Ingest_pipeline_V2'\\n    BASE_DIR: Final[Path] = Path(__file__).resolve().parent.parent\\n    \\n    DATA_DIR: Final[Path] = BASE_DIR / \\\"data\\\"\\n    RAW_LANDING_DIR: Final[Path] = DATA_DIR / \\\"raw_landing\\\"\\n    PROCESSED_ARCHIVE_DIR: Final[Path] = DATA_DIR / \\\"processed_archive\\\"\\n    BACKUP_DIR: Final[Path] = PROCESSED_ARCHIVE_DIR / \\\"backups\\\"\\n    \\n    # Section 2: Storage Paths\\n    CHROMA_DB_PATH: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\"chroma_db\\\"\\n    EMBEDDING_CACHE_DIR: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\".embedding_cache\\\"\\n    USAGE_LOG_PATH: Final[Path] = BASE_DIR / \\\"logs\\\" / \\\"usage_stats.json\\\"\\n\\n    # Section 3: Database (MongoDB)\\n    MONGO_URI: Final[str] = os.getenv(\\\"MONGO_URI\\\", \\\"mongodb://localhost:27017\\\")\\n    DB_NAME: Final[str] = \\\"aletheia_memory\\\"\\n    COLLECTION_TRUTH: Final[str] = \\\"canonical_truth\\\"\\n    COLLECTION_TRACES: Final[str] = \\\"reasoning_traces\\\"\\n\\n    # Section 4: Inference (LM Studio)\\n    LM_STUDIO_BASE_URL: Final[str] = os.getenv(\\\"LM_STUDIO_URL\\\", \\\"http://localhost:1234/v1\\\")\\n    EMBEDDING_MODEL: Final[str] = \\\"nomic-ai/nomic-embed-text-v1.5-GGUF\\\"\\n    NOMIC_PREFIX: Final[str] = \\\"search_document: \\\" \\n\\n    # Section 5: RAG & OCR Logic\\n    CHUNK_SIZE: Final[int] = 1500 \\n    CHUNK_OVERLAP: Final[int] = 200\\n    OCR_TEXT_DENSITY_THRESHOLD: int = 50 # Characters per page below which OCR is triggered\\n    NUM_RETRIEVAL_RESULTS: int = 5\\n\\n    def validate_settings(self):\\n        \\\"\\\"\\\"Ensures directories exist and critical settings are present.\\\"\\\"\\\"\\n        paths = [\\n            self.DATA_DIR, self.RAW_LANDING_DIR, self.PROCESSED_ARCHIVE_DIR, \\n            self.BACKUP_DIR, self.CHROMA_DB_PATH, self.USAGE_LOG_PATH.parent,\\n            self.EMBEDDING_CACHE_DIR\\n        ]\\n        for p in paths:\\n            p.mkdir(parents=True, exist_ok=True)\\n        \\n        if not self.MONGO_URI:\\n            raise ValueError(\\\"MONGO_URI environment variable is missing.\\\")\\n\\nsettings = Settings()\\nsettings.validate_settings()\",\n      \"size_bytes\": 2558,\n      \"labels\": {\n        \"ast_node_count\": 367,\n        \"function_count\": 1,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"1323dcc6d85cb5bceac7402cff7ddfa6\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/codebase_processor.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/codebase_processor.py\\nimport logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass CodebaseProcessor:\\n    \\\"\\\"\\\"\\n    Handles processing of text-based files (Python, JSON, Markdown, etc.).\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Reads text/code files directly and chunks them.\\\"\\\"\\\"\\n        documents = []\\n        try:\\n            # Use errors='ignore' to prevent crashing on non-UTF-8 binary artifacts\\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\\n                raw_text = f.read()\\n            \\n            if raw_text.strip():\\n                return self._chunk_text(raw_text, str(file_path), file_path.name)\\n        except Exception as e:\\n            logger.error(f\\\"Error processing text file {file_path.name}: {e}\\\")\\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Splits text into sliding window chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": 0, # Not applicable for flat text files\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"codebase\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass CodebaseProcessor:\\n    \\\"\\\"\\\"\\n    Handles processing of text-based files (Python, JSON, Markdown, etc.).\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Reads text/code files directly and chunks them.\\\"\\\"\\\"\\n        documents = []\\n        try:\\n            # Use errors='ignore' to prevent crashing on non-UTF-8 binary artifacts\\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\\n                raw_text = f.read()\\n            \\n            if raw_text.strip():\\n                return self._chunk_text(raw_text, str(file_path), file_path.name)\\n        except Exception as e:\\n            logger.error(f\\\"Error processing text file {file_path.name}: {e}\\\")\\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Splits text into sliding window chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": 0, # Not applicable for flat text files\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"codebase\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\",\n      \"size_bytes\": 1980,\n      \"labels\": {\n        \"ast_node_count\": 273,\n        \"function_count\": 3,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"78287a2d7d09009729b2d1c6e333de43\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/ingest_manager.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/ingest_manager.py\\nimport logging\\nfrom pathlib import Path\\nfrom pymongo import MongoClient\\nfrom pymongo.errors import BulkWriteError\\nimport chromadb\\nfrom datetime import datetime\\nfrom config.settings import settings\\n# FIX: Consistent imports\\nfrom core.pdf_processor import PDFProcessor\\nfrom core.codebase_processor import CodebaseProcessor  # Matches lowercase filename\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass IngestManager:\\n    \\\"\\\"\\\"\\n    Manages the complete ingestion pipeline for PDF and Text/Code documents.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        # Initialize Databases\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n        \\n        # Initialize ChromaDB\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # Initialize Core Engines\\n        self.pdf_processor = PDFProcessor()\\n        self.codebase_processor = CodebaseProcessor()\\n        self.embedder = EmbeddingClient()\\n        \\n    def process_file(self, file_path: Path) -> bool:\\n        \\\"\\\"\\\"\\n        Processes a single file through the ingestion pipeline.\\n        Routes to the appropriate processor based on file type.\\n        \\\"\\\"\\\"\\n        try:\\n            logger.info(f\\\"Processing: {file_path.name}\\\")\\n            \\n            # 1. Select Processor Strategy\\n            if file_path.suffix.lower() == '.pdf':\\n                chunks = list(self.pdf_processor.process_file(file_path))\\n            else:\\n                # Fallback to codebase processor for .py, .txt, .md, .json, etc.\\n                chunks = list(self.codebase_processor.process_file(file_path))\\n            \\n            if not chunks:\\n                logger.warning(f\\\"No usable content found in {file_path.name}\\\")\\n                return False\\n            \\n            # 2. Vectorization and Persistence\\n            chroma_ids = []\\n            chroma_embeddings = []\\n            chroma_metadatas = []\\n            mongo_docs = []\\n            \\n            for i, chunk in enumerate(chunks):\\n                content_text = chunk[\\\"content\\\"]\\n                chunk_meta = chunk[\\\"metadata\\\"]\\n                \\n                # Generate unique ID\\n                file_hash = chunk_meta.get('file_name', file_path.name)\\n                doc_id = f\\\"{file_hash}_{i}\\\"\\n                \\n                # Get Embedding\\n                vector = self.embedder.get_embedding(content_text)\\n                if not vector:\\n                    continue\\n                \\n                # Prepare Mongo Document\\n                mongo_docs.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"content\\\": content_text,\\n                    \\\"metadata\\\": chunk_meta,\\n                    \\\"ingested_at\\\": datetime.utcnow().isoformat()\\n                })\\n\\n                # Prepare Chroma Data\\n                chroma_ids.append(doc_id)\\n                chroma_embeddings.append(vector)\\n                chroma_metadatas.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"page\\\": chunk_meta.get('page_number', 0),\\n                    \\\"file_name\\\": chunk_meta.get('file_name', 'unknown')\\n                })\\n\\n            # Bulk Write to Mongo (Robust Duplicate Handling)\\n            if mongo_docs:\\n                try:\\n                    # ordered=False continues processing even if one insert fails (e.g. duplicate)\\n                    self.collection_truth.insert_many(mongo_docs, ordered=False)\\n                except BulkWriteError as bwe:\\n                    # Log duplicates as info, actual errors as warning\\n                    duplicates = [e for e in bwe.details['writeErrors'] if e['code'] == 11000]\\n                    if len(duplicates) == len(mongo_docs):\\n                        logger.info(f\\\"Skipping {file_path.name}: All chunks already exist in DB.\\\")\\n                        return True\\n                    elif duplicates:\\n                        logger.info(f\\\"Partial insert for {file_path.name}: {len(duplicates)} duplicates skipped.\\\")\\n                    else:\\n                        # Sanitize error message to prevent UnicodeEncodeError in Windows consoles\\n                        error_msg = str(bwe).encode('ascii', 'replace').decode('ascii')\\n                        logger.warning(f\\\"MongoDB Bulk Write Error: {error_msg}\\\")\\n\\n            # Bulk Write to Chroma\\n            if chroma_ids:\\n                try:\\n                    self.collection_index.add(\\n                        ids=chroma_ids,\\n                        embeddings=chroma_embeddings,\\n                        metadatas=chroma_metadatas,\\n                        documents=[d['content'] for d in mongo_docs]\\n                    )\\n                except Exception as e:\\n                    # Chroma might error on duplicates, but usually updates/upserts.\\n                    # If it fails, log and continue.\\n                    logger.warning(f\\\"ChromaDB Write Warning for {file_path.name}: {e}\\\")\\n                    \\n            logger.info(f\\\"Successfully processed: {file_path.name}\\\")\\n            return True\\n            \\n        except Exception as e:\\n            # Catch-all to ensure one bad file doesn't crash the whole batch\\n            # Sanitize error message to prevent UnicodeEncodeError\\n            safe_error = str(e).encode('ascii', 'replace').decode('ascii')\\n            logger.error(f\\\"Error processing file {file_path.name}: {safe_error}\\\")\\n            return False\\n    \\n    def process_all(self):\\n        \\\"\\\"\\\"Processes all supported files in the raw landing directory recursively.\\\"\\\"\\\"\\n        extensions = [\\\"*.pdf\\\", \\\"*.txt\\\", \\\"*.py\\\", \\\"*.md\\\", \\\"*.json\\\", \\\"*.sh\\\", \\\"*.ps1\\\"]\\n        all_files = []\\n        \\n        for ext in extensions:\\n            all_files.extend(list(settings.RAW_LANDING_DIR.rglob(ext)))\\n            \\n        if not all_files:\\n            logger.info(f\\\"No supported files found in {settings.RAW_LANDING_DIR}\\\")\\n            return\\n            \\n        logger.info(f\\\"Starting ingestion of {len(all_files)} files.\\\")\\n        processed_count = sum(1 for f in all_files if self.process_file(f))\\n        logger.info(f\\\"Ingestion completed. Processed {processed_count}/{len(all_files)}.\\\")\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import logging\\nfrom pathlib import Path\\nfrom pymongo import MongoClient\\nfrom pymongo.errors import BulkWriteError\\nimport chromadb\\nfrom datetime import datetime\\nfrom config.settings import settings\\n# FIX: Consistent imports\\nfrom core.pdf_processor import PDFProcessor\\nfrom core.codebase_processor import CodebaseProcessor  # Matches lowercase filename\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass IngestManager:\\n    \\\"\\\"\\\"\\n    Manages the complete ingestion pipeline for PDF and Text/Code documents.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        # Initialize Databases\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n        \\n        # Initialize ChromaDB\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # Initialize Core Engines\\n        self.pdf_processor = PDFProcessor()\\n        self.codebase_processor = CodebaseProcessor()\\n        self.embedder = EmbeddingClient()\\n        \\n    def process_file(self, file_path: Path) -> bool:\\n        \\\"\\\"\\\"\\n        Processes a single file through the ingestion pipeline.\\n        Routes to the appropriate processor based on file type.\\n        \\\"\\\"\\\"\\n        try:\\n            logger.info(f\\\"Processing: {file_path.name}\\\")\\n            \\n            # 1. Select Processor Strategy\\n            if file_path.suffix.lower() == '.pdf':\\n                chunks = list(self.pdf_processor.process_file(file_path))\\n            else:\\n                # Fallback to codebase processor for .py, .txt, .md, .json, etc.\\n                chunks = list(self.codebase_processor.process_file(file_path))\\n            \\n            if not chunks:\\n                logger.warning(f\\\"No usable content found in {file_path.name}\\\")\\n                return False\\n            \\n            # 2. Vectorization and Persistence\\n            chroma_ids = []\\n            chroma_embeddings = []\\n            chroma_metadatas = []\\n            mongo_docs = []\\n            \\n            for i, chunk in enumerate(chunks):\\n                content_text = chunk[\\\"content\\\"]\\n                chunk_meta = chunk[\\\"metadata\\\"]\\n                \\n                # Generate unique ID\\n                file_hash = chunk_meta.get('file_name', file_path.name)\\n                doc_id = f\\\"{file_hash}_{i}\\\"\\n                \\n                # Get Embedding\\n                vector = self.embedder.get_embedding(content_text)\\n                if not vector:\\n                    continue\\n                \\n                # Prepare Mongo Document\\n                mongo_docs.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"content\\\": content_text,\\n                    \\\"metadata\\\": chunk_meta,\\n                    \\\"ingested_at\\\": datetime.utcnow().isoformat()\\n                })\\n\\n                # Prepare Chroma Data\\n                chroma_ids.append(doc_id)\\n                chroma_embeddings.append(vector)\\n                chroma_metadatas.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"page\\\": chunk_meta.get('page_number', 0),\\n                    \\\"file_name\\\": chunk_meta.get('file_name', 'unknown')\\n                })\\n\\n            # Bulk Write to Mongo (Robust Duplicate Handling)\\n            if mongo_docs:\\n                try:\\n                    # ordered=False continues processing even if one insert fails (e.g. duplicate)\\n                    self.collection_truth.insert_many(mongo_docs, ordered=False)\\n                except BulkWriteError as bwe:\\n                    # Log duplicates as info, actual errors as warning\\n                    duplicates = [e for e in bwe.details['writeErrors'] if e['code'] == 11000]\\n                    if len(duplicates) == len(mongo_docs):\\n                        logger.info(f\\\"Skipping {file_path.name}: All chunks already exist in DB.\\\")\\n                        return True\\n                    elif duplicates:\\n                        logger.info(f\\\"Partial insert for {file_path.name}: {len(duplicates)} duplicates skipped.\\\")\\n                    else:\\n                        # Sanitize error message to prevent UnicodeEncodeError in Windows consoles\\n                        error_msg = str(bwe).encode('ascii', 'replace').decode('ascii')\\n                        logger.warning(f\\\"MongoDB Bulk Write Error: {error_msg}\\\")\\n\\n            # Bulk Write to Chroma\\n            if chroma_ids:\\n                try:\\n                    self.collection_index.add(\\n                        ids=chroma_ids,\\n                        embeddings=chroma_embeddings,\\n                        metadatas=chroma_metadatas,\\n                        documents=[d['content'] for d in mongo_docs]\\n                    )\\n                except Exception as e:\\n                    # Chroma might error on duplicates, but usually updates/upserts.\\n                    # If it fails, log and continue.\\n                    logger.warning(f\\\"ChromaDB Write Warning for {file_path.name}: {e}\\\")\\n                    \\n            logger.info(f\\\"Successfully processed: {file_path.name}\\\")\\n            return True\\n            \\n        except Exception as e:\\n            # Catch-all to ensure one bad file doesn't crash the whole batch\\n            # Sanitize error message to prevent UnicodeEncodeError\\n            safe_error = str(e).encode('ascii', 'replace').decode('ascii')\\n            logger.error(f\\\"Error processing file {file_path.name}: {safe_error}\\\")\\n            return False\\n    \\n    def process_all(self):\\n        \\\"\\\"\\\"Processes all supported files in the raw landing directory recursively.\\\"\\\"\\\"\\n        extensions = [\\\"*.pdf\\\", \\\"*.txt\\\", \\\"*.py\\\", \\\"*.md\\\", \\\"*.json\\\", \\\"*.sh\\\", \\\"*.ps1\\\"]\\n        all_files = []\\n        \\n        for ext in extensions:\\n            all_files.extend(list(settings.RAW_LANDING_DIR.rglob(ext)))\\n            \\n        if not all_files:\\n            logger.info(f\\\"No supported files found in {settings.RAW_LANDING_DIR}\\\")\\n            return\\n            \\n        logger.info(f\\\"Starting ingestion of {len(all_files)} files.\\\")\\n        processed_count = sum(1 for f in all_files if self.process_file(f))\\n        logger.info(f\\\"Ingestion completed. Processed {processed_count}/{len(all_files)}.\\\")\",\n      \"size_bytes\": 6451,\n      \"labels\": {\n        \"ast_node_count\": 732,\n        \"function_count\": 3,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"c569701e1f7964941f7deeebb9ea7283\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/pdf_processor.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/pdf_processor.py\\nimport fitz # PyMuPDF\\nimport logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\nfrom utils import ocr_service\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass PDFProcessor:\\n    \\\"\\\"\\\"\\n    Specialized processor for PDF documents with OCR capabilities.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"\\n        Extracts text from PDF page-by-page, applying OCR if text density is low.\\n        \\\"\\\"\\\"\\n        documents = []\\n        try:\\n            doc = fitz.open(file_path)\\n            for page_num, page in enumerate(doc):\\n                raw_text = page.get_text()\\n\\n                # Decision Gate: Check for Scanned Pages\\n                if len(raw_text.strip()) < self.settings.OCR_TEXT_DENSITY_THRESHOLD:\\n                    logger.warning(f\\\"Low text density on page {page_num + 1} of {file_path.name}. Checking OCR...\\\")\\n                    try:\\n                        image = ocr_service.convert_page_to_image(str(file_path), page_num + 1)\\n                        if image:\\n                            ocr_text = ocr_service.extract_text_from_image(image)\\n                            # Only use OCR if it yielded more info than the raw extraction\\n                            if len(ocr_text.strip()) > len(raw_text.strip()):\\n                                raw_text = ocr_text\\n                                logger.info(f\\\"OCR improved text yield for page {page_num + 1}.\\\")\\n                    except Exception as ocr_e:\\n                        logger.error(f\\\"OCR failed for page {page_num + 1}: {ocr_e}\\\")\\n\\n                # Chunking\\n                if raw_text.strip():\\n                    page_docs = self._chunk_text(raw_text, str(file_path), file_path.name, page_num + 1)\\n                    documents.extend(page_docs)\\n            \\n            doc.close()\\n        except Exception as e:\\n            logger.error(f\\\"Error processing PDF {file_path}: {e}\\\")\\n            \\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str, page_num: int) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Helper to split text into chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": page_num,\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"pdf\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import fitz # PyMuPDF\\nimport logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\nfrom utils import ocr_service\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass PDFProcessor:\\n    \\\"\\\"\\\"\\n    Specialized processor for PDF documents with OCR capabilities.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"\\n        Extracts text from PDF page-by-page, applying OCR if text density is low.\\n        \\\"\\\"\\\"\\n        documents = []\\n        try:\\n            doc = fitz.open(file_path)\\n            for page_num, page in enumerate(doc):\\n                raw_text = page.get_text()\\n\\n                # Decision Gate: Check for Scanned Pages\\n                if len(raw_text.strip()) < self.settings.OCR_TEXT_DENSITY_THRESHOLD:\\n                    logger.warning(f\\\"Low text density on page {page_num + 1} of {file_path.name}. Checking OCR...\\\")\\n                    try:\\n                        image = ocr_service.convert_page_to_image(str(file_path), page_num + 1)\\n                        if image:\\n                            ocr_text = ocr_service.extract_text_from_image(image)\\n                            # Only use OCR if it yielded more info than the raw extraction\\n                            if len(ocr_text.strip()) > len(raw_text.strip()):\\n                                raw_text = ocr_text\\n                                logger.info(f\\\"OCR improved text yield for page {page_num + 1}.\\\")\\n                    except Exception as ocr_e:\\n                        logger.error(f\\\"OCR failed for page {page_num + 1}: {ocr_e}\\\")\\n\\n                # Chunking\\n                if raw_text.strip():\\n                    page_docs = self._chunk_text(raw_text, str(file_path), file_path.name, page_num + 1)\\n                    documents.extend(page_docs)\\n            \\n            doc.close()\\n        except Exception as e:\\n            logger.error(f\\\"Error processing PDF {file_path}: {e}\\\")\\n            \\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str, page_num: int) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Helper to split text into chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": page_num,\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"pdf\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\",\n      \"size_bytes\": 3040,\n      \"labels\": {\n        \"ast_node_count\": 438,\n        \"function_count\": 3,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"9197055079c29b5f00fe2c764f13fe9a\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/retrieval_controller.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/retrieval_controller.py\\nimport logging\\nimport chromadb\\nfrom pymongo import MongoClient\\nfrom config.settings import settings\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass RetrievalController:\\n    def __init__(self):\\n        self.embedding_client = EmbeddingClient()\\n        \\n        # ChromaDB (Index)\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # MongoDB (Canonical Truth)\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n\\n    def query(self, query: str) -> str:\\n        \\\"\\\"\\\"Retrieves context and generates a response.\\\"\\\"\\\"\\n        # 1. Embed Query\\n        query_embedding = self.embedding_client.get_embedding(query)\\n        if not query_embedding:\\n            return \\\"Error: Could not process query.\\\"\\n\\n        # 2. Retrieve from ChromaDB\\n        results = self.collection_index.query(\\n            query_embeddings=[query_embedding],\\n            n_results=settings.NUM_RETRIEVAL_RESULTS,\\n            include=['metadatas']\\n        )\\n\\n        # 3. Fetch Full Content from MongoDB (Canonical Truth)\\n        # We rely on the index to find *where* the data is, but fetch the *clean* data from Mongo.\\n        context_docs = []\\n        if results and results['metadatas'] and results['metadatas'][0]:\\n            for meta in results['metadatas'][0]:\\n                file_hash = meta.get('file_hash')\\n                chunk_index = meta.get('chunk_index')\\n                \\n                record = self.collection_truth.find_one({\\n                    \\\"file_hash\\\": file_hash, \\n                    \\\"chunk_index\\\": chunk_index\\n                })\\n                \\n                if record:\\n                    context_docs.append(record['content'])\\n        \\n        if not context_docs:\\n            return \\\"No relevant information found in the archives.\\\"\\n\\n        # 4. Construct Prompt\\n        context_text = \\\"\\\\n\\\\n---\\\\n\\\\n\\\".join(context_docs)\\n        return f\\\"Based on the following research:\\\\n\\\\n{context_text}\\\\n\\\\nAnswer: {query}\\\"\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import logging\\nimport chromadb\\nfrom pymongo import MongoClient\\nfrom config.settings import settings\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass RetrievalController:\\n    def __init__(self):\\n        self.embedding_client = EmbeddingClient()\\n        \\n        # ChromaDB (Index)\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # MongoDB (Canonical Truth)\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n\\n    def query(self, query: str) -> str:\\n        \\\"\\\"\\\"Retrieves context and generates a response.\\\"\\\"\\\"\\n        # 1. Embed Query\\n        query_embedding = self.embedding_client.get_embedding(query)\\n        if not query_embedding:\\n            return \\\"Error: Could not process query.\\\"\\n\\n        # 2. Retrieve from ChromaDB\\n        results = self.collection_index.query(\\n            query_embeddings=[query_embedding],\\n            n_results=settings.NUM_RETRIEVAL_RESULTS,\\n            include=['metadatas']\\n        )\\n\\n        # 3. Fetch Full Content from MongoDB (Canonical Truth)\\n        # We rely on the index to find *where* the data is, but fetch the *clean* data from Mongo.\\n        context_docs = []\\n        if results and results['metadatas'] and results['metadatas'][0]:\\n            for meta in results['metadatas'][0]:\\n                file_hash = meta.get('file_hash')\\n                chunk_index = meta.get('chunk_index')\\n                \\n                record = self.collection_truth.find_one({\\n                    \\\"file_hash\\\": file_hash, \\n                    \\\"chunk_index\\\": chunk_index\\n                })\\n                \\n                if record:\\n                    context_docs.append(record['content'])\\n        \\n        if not context_docs:\\n            return \\\"No relevant information found in the archives.\\\"\\n\\n        # 4. Construct Prompt\\n        context_text = \\\"\\\\n\\\\n---\\\\n\\\\n\\\".join(context_docs)\\n        return f\\\"Based on the following research:\\\\n\\\\n{context_text}\\\\n\\\\nAnswer: {query}\\\"\",\n      \"size_bytes\": 2270,\n      \"labels\": {\n        \"ast_node_count\": 269,\n        \"function_count\": 2,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"f29ac7b2821ef048e5b25928f8f2380d\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"settings/init.py\",\n      \"content_block\": \"\\\"\\\"\\\"settings/init.py\\nimport pymongo\\nimport sys\\nfrom pathlib import Path\\n\\n# Fix path to ensure imports work from top-level directory\\nsys.path.append(str(Path(__file__).resolve().parents[1]))\\n\\nfrom config.settings import settings\\n\\ndef init():\\n    try:\\n        client = pymongo.MongoClient(settings.MONGO_URI)\\n        db = client[settings.DB_NAME]\\n        \\n        colls = [settings.COLLECTION_TRUTH, settings.COLLECTION_TRACES]\\n        for c in colls:\\n            if c not in db.list_collection_names():\\n                db.create_collection(c)\\n                print(f\\\"Provisioned: {c}\\\")\\n                \\n        # Create unique index on file_hash and chunk_index pair for granular retrieval\\n        db[settings.COLLECTION_TRUTH].create_index(\\n            [(\\\"file_hash\\\", pymongo.ASCENDING), (\\\"chunk_index\\\", pymongo.ASCENDING)], \\n            unique=True\\n        )\\n        print(\\\"Aletheia Memory initialized successfully.\\\")\\n        \\n    except Exception as e:\\n        print(f\\\"Initialization failed: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    init()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import pymongo\\nimport sys\\nfrom pathlib import Path\\n\\n# Fix path to ensure imports work from top-level directory\\nsys.path.append(str(Path(__file__).resolve().parents[1]))\\n\\nfrom config.settings import settings\\n\\ndef init():\\n    try:\\n        client = pymongo.MongoClient(settings.MONGO_URI)\\n        db = client[settings.DB_NAME]\\n        \\n        colls = [settings.COLLECTION_TRUTH, settings.COLLECTION_TRACES]\\n        for c in colls:\\n            if c not in db.list_collection_names():\\n                db.create_collection(c)\\n                print(f\\\"Provisioned: {c}\\\")\\n                \\n        # Create unique index on file_hash and chunk_index pair for granular retrieval\\n        db[settings.COLLECTION_TRUTH].create_index(\\n            [(\\\"file_hash\\\", pymongo.ASCENDING), (\\\"chunk_index\\\", pymongo.ASCENDING)], \\n            unique=True\\n        )\\n        print(\\\"Aletheia Memory initialized successfully.\\\")\\n        \\n    except Exception as e:\\n        print(f\\\"Initialization failed: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    init()\",\n      \"size_bytes\": 1041,\n      \"labels\": {\n        \"ast_node_count\": 161,\n        \"function_count\": 1,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"c08000706176c215599fd3275903e9ef\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"utils/embedding_client.py\",\n      \"content_block\": \"\\\"\\\"\\\"utils/embedding_client.py\\nimport requests\\nimport logging\\nimport time\\nfrom typing import List, Optional\\nfrom functools import lru_cache\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass EmbeddingClient:\\n    \\\"\\\"\\\"\\n    Interface for local LM Studio embeddings with caching and resource awareness.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.base_url = f\\\"{settings.LM_STUDIO_BASE_URL}/embeddings\\\"\\n        self.last_activity = time.time()\\n\\n    def _check_resource_status(self):\\n        \\\"\\\"\\\"\\n        Placeholder for checking system health or triggering model unloads.\\n        Could be extended to use LM Studio's /v1/models endpoint to check TTL.\\n        \\\"\\\"\\\"\\n        self.last_activity = time.time()\\n        # In a JIT strategy, we could ping a custom management script here\\n        pass\\n\\n    @lru_cache(maxsize=2048) # Increased cache size for better performance\\n    def get_embedding(self, text: str) -> Optional[List[float]]:\\n        \\\"\\\"\\\"\\n        Generates a vector with LRU caching.\\n        Note: Nomic models require the 'search_document: ' prefix.\\n        \\\"\\\"\\\"\\n        self._check_resource_status()\\n        \\n        prefixed_text = f\\\"{settings.NOMIC_PREFIX}{text}\\\"\\n        payload = {\\\"input\\\": prefixed_text, \\\"model\\\": settings.EMBEDDING_MODEL}\\n        \\n        # Implement internal retry logic\\n        for attempt in range(3):\\n            try:\\n                response = requests.post(self.base_url, json=payload, timeout=30)\\n                response.raise_for_status()\\n                return response.json()[\\\"data\\\"][0][\\\"embedding\\\"]\\n            except Exception as e:\\n                wait = (attempt + 1) * 2\\n                logger.warning(f\\\"Embedding failed (Attempt {attempt+1}): {e}. Retrying in {wait}s...\\\")\\n                time.sleep(wait)\\n        \\n        logger.error(f\\\"Failed to retrieve embedding after retries for text snippet.\\\")\\n        return None\\n\\n    def clear_cache(self):\\n        \\\"\\\"\\\"Clears the embedding cache.\\\"\\\"\\\"\\n        self.get_embedding.cache_clear()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import requests\\nimport logging\\nimport time\\nfrom typing import List, Optional\\nfrom functools import lru_cache\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass EmbeddingClient:\\n    \\\"\\\"\\\"\\n    Interface for local LM Studio embeddings with caching and resource awareness.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.base_url = f\\\"{settings.LM_STUDIO_BASE_URL}/embeddings\\\"\\n        self.last_activity = time.time()\\n\\n    def _check_resource_status(self):\\n        \\\"\\\"\\\"\\n        Placeholder for checking system health or triggering model unloads.\\n        Could be extended to use LM Studio's /v1/models endpoint to check TTL.\\n        \\\"\\\"\\\"\\n        self.last_activity = time.time()\\n        # In a JIT strategy, we could ping a custom management script here\\n        pass\\n\\n    @lru_cache(maxsize=2048) # Increased cache size for better performance\\n    def get_embedding(self, text: str) -> Optional[List[float]]:\\n        \\\"\\\"\\\"\\n        Generates a vector with LRU caching.\\n        Note: Nomic models require the 'search_document: ' prefix.\\n        \\\"\\\"\\\"\\n        self._check_resource_status()\\n        \\n        prefixed_text = f\\\"{settings.NOMIC_PREFIX}{text}\\\"\\n        payload = {\\\"input\\\": prefixed_text, \\\"model\\\": settings.EMBEDDING_MODEL}\\n        \\n        # Implement internal retry logic\\n        for attempt in range(3):\\n            try:\\n                response = requests.post(self.base_url, json=payload, timeout=30)\\n                response.raise_for_status()\\n                return response.json()[\\\"data\\\"][0][\\\"embedding\\\"]\\n            except Exception as e:\\n                wait = (attempt + 1) * 2\\n                logger.warning(f\\\"Embedding failed (Attempt {attempt+1}): {e}. Retrying in {wait}s...\\\")\\n                time.sleep(wait)\\n        \\n        logger.error(f\\\"Failed to retrieve embedding after retries for text snippet.\\\")\\n        return None\\n\\n    def clear_cache(self):\\n        \\\"\\\"\\\"Clears the embedding cache.\\\"\\\"\\\"\\n        self.get_embedding.cache_clear()\",\n      \"size_bytes\": 2004,\n      \"labels\": {\n        \"ast_node_count\": 235,\n        \"function_count\": 4,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"838a4572ee81d2fba1369be708ac5bc1\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"utils/metadata_extractor.py\",\n      \"content_block\": \"\\\"\\\"\\\"utils/metadata_extractor.py\\nimport hashlib\\nimport logging\\nimport re\\nfrom pathlib import Path\\nfrom typing import Dict, Any, Optional\\nfrom datetime import datetime, timezone\\nimport PyPDF2\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef generate_file_hash(file_path: Path) -> str:\\n    \\\"\\\"\\\"Generates a SHA-256 hash of the file.\\\"\\\"\\\"\\n    sha256_hash = hashlib.sha256()\\n    try:\\n        with open(file_path, \\\"rb\\\") as f:\\n            for byte_block in iter(lambda: f.read(4096), b\\\"\\\"):\\n                sha256_hash.update(byte_block)\\n        return sha256_hash.hexdigest()\\n    except Exception as e:\\n        logger.error(f\\\"Hash generation failed for {file_path}: {e}\\\")\\n        return \\\"error_hash\\\"\\n\\ndef _parse_pdf_date(date_str: Optional[str]) -> Optional[str]:\\n    \\\"\\\"\\\"Converts PDF-style date strings into ISO format.\\\"\\\"\\\"\\n    if not date_str or not isinstance(date_str, str):\\n        return None\\n    \\n    clean_date = re.sub(r'[^0-9]', '', date_str)\\n    try:\\n        if len(clean_date) >= 8:\\n            return f\\\"{clean_date[0:4]}-{clean_date[4:6]}-{clean_date[6:8]}\\\"\\n    except Exception:\\n        pass\\n    return date_str\\n\\ndef _sanitize_string(text: Any) -> str:\\n    if not text or not isinstance(text, str):\\n        return \\\"Unknown\\\"\\n    clean_text = \\\"\\\".join(char for char in text if char.isprintable())\\n    return \\\" \\\".join(clean_text.split())\\n\\ndef extract_document_metadata(reader: PyPDF2.PdfReader, file_path: Path, content: str = \\\"\\\") -> Dict[str, Any]:\\n    \\\"\\\"\\\"Generates an enriched document profile.\\\"\\\"\\\"\\n    try:\\n        meta = reader.metadata\\n    except Exception:\\n        meta = None\\n\\n    word_count = len(content.split()) if content else 0\\n\\n    return {\\n        \\\"file_hash\\\": generate_file_hash(file_path),\\n        \\\"file_name\\\": file_path.name,\\n        \\\"internal_title\\\": _sanitize_string(meta.title if meta and meta.title else file_path.stem),\\n        \\\"author\\\": _sanitize_string(meta.author if meta and meta.author else \\\"Unknown Author\\\"),\\n        \\\"total_pages\\\": len(reader.pages),\\n        \\\"word_count\\\": word_count,\\n        \\\"creation_date\\\": _parse_pdf_date(meta.get('/CreationDate') if meta else None),\\n        \\\"ingested_at\\\": datetime.now(timezone.utc).isoformat(),\\n        \\\"version\\\": \\\"2.2\\\"\\n    }\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import hashlib\\nimport logging\\nimport re\\nfrom pathlib import Path\\nfrom typing import Dict, Any, Optional\\nfrom datetime import datetime, timezone\\nimport PyPDF2\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef generate_file_hash(file_path: Path) -> str:\\n    \\\"\\\"\\\"Generates a SHA-256 hash of the file.\\\"\\\"\\\"\\n    sha256_hash = hashlib.sha256()\\n    try:\\n        with open(file_path, \\\"rb\\\") as f:\\n            for byte_block in iter(lambda: f.read(4096), b\\\"\\\"):\\n                sha256_hash.update(byte_block)\\n        return sha256_hash.hexdigest()\\n    except Exception as e:\\n        logger.error(f\\\"Hash generation failed for {file_path}: {e}\\\")\\n        return \\\"error_hash\\\"\\n\\ndef _parse_pdf_date(date_str: Optional[str]) -> Optional[str]:\\n    \\\"\\\"\\\"Converts PDF-style date strings into ISO format.\\\"\\\"\\\"\\n    if not date_str or not isinstance(date_str, str):\\n        return None\\n    \\n    clean_date = re.sub(r'[^0-9]', '', date_str)\\n    try:\\n        if len(clean_date) >= 8:\\n            return f\\\"{clean_date[0:4]}-{clean_date[4:6]}-{clean_date[6:8]}\\\"\\n    except Exception:\\n        pass\\n    return date_str\\n\\ndef _sanitize_string(text: Any) -> str:\\n    if not text or not isinstance(text, str):\\n        return \\\"Unknown\\\"\\n    clean_text = \\\"\\\".join(char for char in text if char.isprintable())\\n    return \\\" \\\".join(clean_text.split())\\n\\ndef extract_document_metadata(reader: PyPDF2.PdfReader, file_path: Path, content: str = \\\"\\\") -> Dict[str, Any]:\\n    \\\"\\\"\\\"Generates an enriched document profile.\\\"\\\"\\\"\\n    try:\\n        meta = reader.metadata\\n    except Exception:\\n        meta = None\\n\\n    word_count = len(content.split()) if content else 0\\n\\n    return {\\n        \\\"file_hash\\\": generate_file_hash(file_path),\\n        \\\"file_name\\\": file_path.name,\\n        \\\"internal_title\\\": _sanitize_string(meta.title if meta and meta.title else file_path.stem),\\n        \\\"author\\\": _sanitize_string(meta.author if meta and meta.author else \\\"Unknown Author\\\"),\\n        \\\"total_pages\\\": len(reader.pages),\\n        \\\"word_count\\\": word_count,\\n        \\\"creation_date\\\": _parse_pdf_date(meta.get('/CreationDate') if meta else None),\\n        \\\"ingested_at\\\": datetime.now(timezone.utc).isoformat(),\\n        \\\"version\\\": \\\"2.2\\\"\\n    }\",\n      \"size_bytes\": 2186,\n      \"labels\": {\n        \"ast_node_count\": 400,\n        \"function_count\": 4,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"2232312cac37c9b553b7e7eb97aad369\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"utils/ocr_service.py\",\n      \"content_block\": \"\\\"\\\"\\\"utils/ocr_service.py\\nfrom PIL import Image\\nimport pytesseract\\nimport logging\\nfrom pdf2image import convert_from_path\\nimport os\\nimport sys\\n\\nlogger = logging.getLogger(__name__)\\n\\n# --- CONFIGURATION ---\\n# 1. POPPLER PATH (For PDF -> Image conversion)\\n# Updated to match your specific installation:\\nPOPPLER_PATH = r\\\"C:\\\\Users\\\\jakem\\\\Documents\\\\poppler\\\\poppler-25.12.0\\\\Library\\\\bin\\\"\\n\\n# 2. TESSERACT PATH (For Image -> Text OCR)\\n# CRITICAL FOR WINDOWS: Point this to your tesseract.exe\\n# If you haven't installed it, download from: https://github.com/UB-Mannheim/tesseract/wiki\\npytesseract.pytesseract.tesseract_cmd = r\\\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\\\"\\n\\ndef _get_poppler_path():\\n    \\\"\\\"\\\"\\n    Attempts to locate poppler path or returns None to let system PATH handle it.\\n    \\\"\\\"\\\"\\n    if os.name == 'nt': # Only for Windows\\n        if os.path.exists(POPPLER_PATH):\\n            return POPPLER_PATH\\n        \\n        # Check if user put it in the project folder for ease of use\\n        local_poppler = os.path.join(os.getcwd(), 'poppler', 'bin')\\n        if os.path.exists(local_poppler):\\n            return local_poppler\\n            \\n    return None # Default to system PATH\\n\\ndef extract_text_from_image(image_path_or_object) -> str:\\n    \\\"\\\"\\\"Extracts text from an image using pytesseract.\\\"\\\"\\\"\\n    try:\\n        if isinstance(image_path_or_object, str):\\n            img = Image.open(image_path_or_object)\\n        else:\\n            img = image_path_or_object\\n        return pytesseract.image_to_string(img)\\n    except Exception as e:\\n        # Check for common Tesseract \\\"not found\\\" errors\\n        if \\\"tesseract is not installed\\\" in str(e).lower() or \\\"not in your path\\\" in str(e).lower():\\n             logger.error(\\\"Tesseract not found! Please install it and check the path in utils/ocr_service.py\\\")\\n        else:\\n            logger.error(f\\\"Error during OCR text extraction: {e}\\\")\\n        return \\\"\\\"\\n\\ndef convert_page_to_image(pdf_path, page_number):\\n    \\\"\\\"\\\"Converts a specific page of a PDF into a PIL Image object using pdf2image.\\\"\\\"\\\"\\n    try:\\n        poppler_path = _get_poppler_path()\\n        \\n        # pdf2image uses 1-based indexing for first_page/last_page\\n        images = convert_from_path(\\n            pdf_path, \\n            first_page=page_number, \\n            last_page=page_number,\\n            poppler_path=poppler_path # Explicitly pass the path\\n        )\\n        if images:\\n            return images[0]\\n        return None\\n    except Exception as e:\\n        if \\\"poppler\\\" in str(e).lower():\\n            logger.error(f\\\"Poppler not found. Please update POPPLER_PATH in utils/ocr_service.py. Error: {e}\\\")\\n        else:\\n            logger.error(f\\\"Error converting PDF page {page_number} to image: {e}\\\")\\n        return None\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"from PIL import Image\\nimport pytesseract\\nimport logging\\nfrom pdf2image import convert_from_path\\nimport os\\nimport sys\\n\\nlogger = logging.getLogger(__name__)\\n\\n# --- CONFIGURATION ---\\n# 1. POPPLER PATH (For PDF -> Image conversion)\\n# Updated to match your specific installation:\\nPOPPLER_PATH = r\\\"C:\\\\Users\\\\jakem\\\\Documents\\\\poppler\\\\poppler-25.12.0\\\\Library\\\\bin\\\"\\n\\n# 2. TESSERACT PATH (For Image -> Text OCR)\\n# CRITICAL FOR WINDOWS: Point this to your tesseract.exe\\n# If you haven't installed it, download from: https://github.com/UB-Mannheim/tesseract/wiki\\npytesseract.pytesseract.tesseract_cmd = r\\\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\\\"\\n\\ndef _get_poppler_path():\\n    \\\"\\\"\\\"\\n    Attempts to locate poppler path or returns None to let system PATH handle it.\\n    \\\"\\\"\\\"\\n    if os.name == 'nt': # Only for Windows\\n        if os.path.exists(POPPLER_PATH):\\n            return POPPLER_PATH\\n        \\n        # Check if user put it in the project folder for ease of use\\n        local_poppler = os.path.join(os.getcwd(), 'poppler', 'bin')\\n        if os.path.exists(local_poppler):\\n            return local_poppler\\n            \\n    return None # Default to system PATH\\n\\ndef extract_text_from_image(image_path_or_object) -> str:\\n    \\\"\\\"\\\"Extracts text from an image using pytesseract.\\\"\\\"\\\"\\n    try:\\n        if isinstance(image_path_or_object, str):\\n            img = Image.open(image_path_or_object)\\n        else:\\n            img = image_path_or_object\\n        return pytesseract.image_to_string(img)\\n    except Exception as e:\\n        # Check for common Tesseract \\\"not found\\\" errors\\n        if \\\"tesseract is not installed\\\" in str(e).lower() or \\\"not in your path\\\" in str(e).lower():\\n             logger.error(\\\"Tesseract not found! Please install it and check the path in utils/ocr_service.py\\\")\\n        else:\\n            logger.error(f\\\"Error during OCR text extraction: {e}\\\")\\n        return \\\"\\\"\\n\\ndef convert_page_to_image(pdf_path, page_number):\\n    \\\"\\\"\\\"Converts a specific page of a PDF into a PIL Image object using pdf2image.\\\"\\\"\\\"\\n    try:\\n        poppler_path = _get_poppler_path()\\n        \\n        # pdf2image uses 1-based indexing for first_page/last_page\\n        images = convert_from_path(\\n            pdf_path, \\n            first_page=page_number, \\n            last_page=page_number,\\n            poppler_path=poppler_path # Explicitly pass the path\\n        )\\n        if images:\\n            return images[0]\\n        return None\\n    except Exception as e:\\n        if \\\"poppler\\\" in str(e).lower():\\n            logger.error(f\\\"Poppler not found. Please update POPPLER_PATH in utils/ocr_service.py. Error: {e}\\\")\\n        else:\\n            logger.error(f\\\"Error converting PDF page {page_number} to image: {e}\\\")\\n        return None\",\n      \"size_bytes\": 2730,\n      \"labels\": {\n        \"ast_node_count\": 263,\n        \"function_count\": 3,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"b2e812c5072ce3f066127d36d9bc51f8\"\n      },\n      \"analysis\": null\n    }\n  ]\n}\n\"\"\"",
      "raw_content": "{\n  \"project\": \"RAG Ingestion Snapshot\",\n  \"chunk\": 1,\n  \"uid\": \"bce8782c\",\n  \"files\": [\n    {\n      \"path\": \".env\",\n      \"content_block\": \"\\\"\\\"\\\".env\\nMONGO_URI=mongodb://localhost:27017\\nLM_STUDIO_URL=http://localhost:1234/v1\\n# API Key is optional for local LM Studio, but typically required by the client code structure\\nOPENAI_API_KEY=lm-studio\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"MONGO_URI=mongodb://localhost:27017\\nLM_STUDIO_URL=http://localhost:1234/v1\\n# API Key is optional for local LM Studio, but typically required by the client code structure\\nOPENAI_API_KEY=lm-studio\",\n      \"size_bytes\": 206,\n      \"labels\": {\n        \"file_type\": \"config\",\n        \"file_extension\": \"\",\n        \"path_hash\": \"f579cccc964135c7d644c7b2d3b0d3ec\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"orchestrator.py\",\n      \"content_block\": \"\\\"\\\"\\\"orchestrator.py\\nimport argparse\\nimport sys\\nimport re\\nimport logging\\nfrom pathlib import Path\\n\\n# --- PATH CORRECTION ---\\n# Ensure project root is in sys.path so 'core' and 'utils' can be imported\\n# regardless of where the script is run from.\\nproject_root = Path(__file__).resolve().parent\\nif str(project_root) not in sys.path:\\n    sys.path.append(str(project_root))\\n\\nfrom core.ingest_manager import IngestManager\\nfrom core.retrieval_controller import RetrievalController\\n\\n# Configure logging if not already configured\\nif not logging.getLogger().handlers:\\n    logging.basicConfig(\\n        level=logging.INFO,\\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n        handlers=[\\n            logging.StreamHandler()\\n        ]\\n    )\\n\\ndef sanitize_input(text: str) -> str:\\n    \\\"\\\"\\\"\\n    Removes potentially problematic characters from query strings.\\n    \\\"\\\"\\\"\\n    if not text: return \\\"\\\"\\n    return re.sub(r'[^\\\\w\\\\s\\\\.\\\\-\\\\?\\\\!]', '', text).strip()\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Aletheia RAG CLI - Technical Enhancements Build\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n    parser.add_argument(\\n        \\\"mode\\\", \\n        choices=[\\\"ingest\\\", \\\"ask\\\"], \\n        help=\\\"System mode: 'ingest' to process documents, 'ask' to query the brain.\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--q\\\", \\n        help=\\\"The research question for Aletheia (required for 'ask' mode)\\\"\\n    )\\n    args = parser.parse_args()\\n\\n    if args.mode == \\\"ingest\\\":\\n        print(\\\"\\\\n[INIT] Starting Aletheia Ingestion Engine...\\\")\\n        print(\\\"[INFO] Scanning 'data/raw_landing' for new intelligence...\\\")\\n        try:\\n            manager = IngestManager()\\n            manager.process_all()\\n            print(\\\"\\\\n[SUCCESS] Ingestion cycle complete.\\\\n\\\")\\n        except Exception as e:\\n            # Catch fatal errors (config issues, missing folders)\\n            logging.error(f\\\"Ingestion failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] System failure during ingestion: {e}\\\")\\n            sys.exit(1)\\n    \\n    elif args.mode == \\\"ask\\\":\\n        if not args.q:\\n            print(\\\"\\\\n[ERROR] 'ask' mode requires a query. Use: --q 'your question'\\\")\\n            sys.exit(1)\\n            \\n        clean_q = sanitize_input(args.q)\\n        print(f\\\"\\\\n[QUERY] Researching: '{clean_q}'\\\")\\n        print(\\\"[INFO] Accessing semantic memory and canonical truth...\\\")\\n        \\n        try:\\n            controller = RetrievalController()\\n            answer = controller.query(clean_q)\\n            \\n            print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n            print(\\\" ALETHEIA EXPERT RESPONSE\\\")\\n            print(\\\"=\\\"*60)\\n            print(answer)\\n            print(\\\"=\\\"*60 + \\\"\\\\n\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Retrieval failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] Inference engine error: {e}\\\")\\n            sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    try:\\n        main()\\n    except KeyboardInterrupt:\\n        print(\\\"\\\\n[HALT] Shutdown signal received. Exiting gracefully.\\\")\\n        sys.exit(0)\\n    except Exception as e:\\n        print(f\\\"\\\\n[FATAL] Unhandled error: {e}\\\")\\n        sys.exit(1)\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import argparse\\nimport sys\\nimport re\\nimport logging\\nfrom pathlib import Path\\n\\n# --- PATH CORRECTION ---\\n# Ensure project root is in sys.path so 'core' and 'utils' can be imported\\n# regardless of where the script is run from.\\nproject_root = Path(__file__).resolve().parent\\nif str(project_root) not in sys.path:\\n    sys.path.append(str(project_root))\\n\\nfrom core.ingest_manager import IngestManager\\nfrom core.retrieval_controller import RetrievalController\\n\\n# Configure logging if not already configured\\nif not logging.getLogger().handlers:\\n    logging.basicConfig(\\n        level=logging.INFO,\\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n        handlers=[\\n            logging.StreamHandler()\\n        ]\\n    )\\n\\ndef sanitize_input(text: str) -> str:\\n    \\\"\\\"\\\"\\n    Removes potentially problematic characters from query strings.\\n    \\\"\\\"\\\"\\n    if not text: return \\\"\\\"\\n    return re.sub(r'[^\\\\w\\\\s\\\\.\\\\-\\\\?\\\\!]', '', text).strip()\\n\\ndef main():\\n    parser = argparse.ArgumentParser(\\n        description=\\\"Aletheia RAG CLI - Technical Enhancements Build\\\",\\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\\n    )\\n    parser.add_argument(\\n        \\\"mode\\\", \\n        choices=[\\\"ingest\\\", \\\"ask\\\"], \\n        help=\\\"System mode: 'ingest' to process documents, 'ask' to query the brain.\\\"\\n    )\\n    parser.add_argument(\\n        \\\"--q\\\", \\n        help=\\\"The research question for Aletheia (required for 'ask' mode)\\\"\\n    )\\n    args = parser.parse_args()\\n\\n    if args.mode == \\\"ingest\\\":\\n        print(\\\"\\\\n[INIT] Starting Aletheia Ingestion Engine...\\\")\\n        print(\\\"[INFO] Scanning 'data/raw_landing' for new intelligence...\\\")\\n        try:\\n            manager = IngestManager()\\n            manager.process_all()\\n            print(\\\"\\\\n[SUCCESS] Ingestion cycle complete.\\\\n\\\")\\n        except Exception as e:\\n            # Catch fatal errors (config issues, missing folders)\\n            logging.error(f\\\"Ingestion failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] System failure during ingestion: {e}\\\")\\n            sys.exit(1)\\n    \\n    elif args.mode == \\\"ask\\\":\\n        if not args.q:\\n            print(\\\"\\\\n[ERROR] 'ask' mode requires a query. Use: --q 'your question'\\\")\\n            sys.exit(1)\\n            \\n        clean_q = sanitize_input(args.q)\\n        print(f\\\"\\\\n[QUERY] Researching: '{clean_q}'\\\")\\n        print(\\\"[INFO] Accessing semantic memory and canonical truth...\\\")\\n        \\n        try:\\n            controller = RetrievalController()\\n            answer = controller.query(clean_q)\\n            \\n            print(\\\"\\\\n\\\" + \\\"=\\\"*60)\\n            print(\\\" ALETHEIA EXPERT RESPONSE\\\")\\n            print(\\\"=\\\"*60)\\n            print(answer)\\n            print(\\\"=\\\"*60 + \\\"\\\\n\\\")\\n        except Exception as e:\\n            logging.error(f\\\"Retrieval failed: {e}\\\")\\n            print(f\\\"\\\\n[CRITICAL] Inference engine error: {e}\\\")\\n            sys.exit(1)\\n\\nif __name__ == \\\"__main__\\\":\\n    try:\\n        main()\\n    except KeyboardInterrupt:\\n        print(\\\"\\\\n[HALT] Shutdown signal received. Exiting gracefully.\\\")\\n        sys.exit(0)\\n    except Exception as e:\\n        print(f\\\"\\\\n[FATAL] Unhandled error: {e}\\\")\\n        sys.exit(1)\",\n      \"size_bytes\": 3116,\n      \"labels\": {\n        \"ast_node_count\": 412,\n        \"function_count\": 2,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"2aa227cc2c894cc2b12ebfdf445352bd\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"RAG_System_Bundler.py\",\n      \"content_block\": \"\\\"\\\"\\\"RAG_System_Bundler.py\\nimport os\\nimport json\\nfrom pathlib import Path\\n\\ndef create_verification_snapshot(output_name=\\\"RAG_System_Deep_Snapshot.json\\\"):\\n    \\\"\\\"\\\"\\n    Scans all project files and their contents for code and telemetry verification.\\n    Includes logs and config files usually ignored in standard builds.\\n    \\\"\\\"\\\"\\n    snapshot = {\\n        \\\"project\\\": \\\"RAG Ingestion Pipeline (V4)\\\",\\n        \\\"purpose\\\": \\\"Code & Telemetry Verification\\\",\\n        \\\"directory_structure\\\": [],\\n        \\\"files\\\": []\\n    }\\n\\n    # Pruned ignore list: We now WANT to see logs and env files\\n    ignore_dirs = {'__pycache__', '.vs', '.git', '.idea', 'venv', 'env'}\\n    # Only skip actual heavy binaries that can't be read as text\\n    binary_extensions = {'.pyc', '.exe', '.dll', '.lib', '.pdf', '.zip', '.sqlite', '.h5'}\\n\\n    base_dir = Path(__file__).parent.resolve()\\n    print(f\\\"--- Initiating Deep Verification Scan ---\\\")\\n    print(f\\\"Scanning: {base_dir}\\\")\\n\\n    file_count = 0\\n    \\n    for root, dirs, files in os.walk(base_dir):\\n        # Prune basic system dirs\\n        dirs[:] = [d for d in dirs if d not in ignore_dirs]\\n        \\n        relative_root = Path(root).relative_to(base_dir)\\n        depth = len(relative_root.parts)\\n        indent = \\\"  \\\" * depth\\n        \\n        if root != str(base_dir):\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{indent}[DIR] {relative_root.as_posix()}\\\")\\n\\n        for file in files:\\n            path = Path(root) / file\\n            rel_path = path.relative_to(base_dir).as_posix()\\n            \\n            # Map the structure\\n            file_indent = \\\"  \\\" * (depth + 1)\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{file_indent}[FILE] {file}\\\")\\n\\n            # Skip binaries, but read everything else (logs, env, py, json)\\n            if path.suffix.lower() in binary_extensions or file == output_name:\\n                continue\\n                \\n            try:\\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\\n                    content = f.read()\\n                \\n                # Determine module or category\\n                parts = Path(rel_path).parts\\n                category = parts[0] if len(parts) > 1 else \\\"root\\\"\\n\\n                print(f\\\"Indexing for Verification: {rel_path}\\\")\\n\\n                snapshot[\\\"files\\\"].append({\\n                    \\\"path\\\": rel_path,\\n                    \\\"category\\\": category,\\n                    \\\"content\\\": content,\\n                    \\\"size_chars\\\": len(content)\\n                })\\n                file_count += 1\\n                \\n            except Exception as e:\\n                print(f\\\"Could not read {rel_path}: {e}\\\")\\n\\n    # Save the exhaustive snapshot\\n    try:\\n        output_path = base_dir / output_name\\n        with open(output_path, 'w', encoding='utf-8') as f:\\n            json.dump(snapshot, f, indent=2)\\n        print(f\\\"\\\\n--- Scan Complete ---\\\")\\n        print(f\\\"Verification file created: {output_path}\\\")\\n        print(f\\\"Total source/log files captured: {file_count}\\\")\\n    except Exception as e:\\n        print(f\\\"Critical error writing snapshot: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    create_verification_snapshot()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import os\\nimport json\\nfrom pathlib import Path\\n\\ndef create_verification_snapshot(output_name=\\\"RAG_System_Deep_Snapshot.json\\\"):\\n    \\\"\\\"\\\"\\n    Scans all project files and their contents for code and telemetry verification.\\n    Includes logs and config files usually ignored in standard builds.\\n    \\\"\\\"\\\"\\n    snapshot = {\\n        \\\"project\\\": \\\"RAG Ingestion Pipeline (V4)\\\",\\n        \\\"purpose\\\": \\\"Code & Telemetry Verification\\\",\\n        \\\"directory_structure\\\": [],\\n        \\\"files\\\": []\\n    }\\n\\n    # Pruned ignore list: We now WANT to see logs and env files\\n    ignore_dirs = {'__pycache__', '.vs', '.git', '.idea', 'venv', 'env'}\\n    # Only skip actual heavy binaries that can't be read as text\\n    binary_extensions = {'.pyc', '.exe', '.dll', '.lib', '.pdf', '.zip', '.sqlite', '.h5'}\\n\\n    base_dir = Path(__file__).parent.resolve()\\n    print(f\\\"--- Initiating Deep Verification Scan ---\\\")\\n    print(f\\\"Scanning: {base_dir}\\\")\\n\\n    file_count = 0\\n    \\n    for root, dirs, files in os.walk(base_dir):\\n        # Prune basic system dirs\\n        dirs[:] = [d for d in dirs if d not in ignore_dirs]\\n        \\n        relative_root = Path(root).relative_to(base_dir)\\n        depth = len(relative_root.parts)\\n        indent = \\\"  \\\" * depth\\n        \\n        if root != str(base_dir):\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{indent}[DIR] {relative_root.as_posix()}\\\")\\n\\n        for file in files:\\n            path = Path(root) / file\\n            rel_path = path.relative_to(base_dir).as_posix()\\n            \\n            # Map the structure\\n            file_indent = \\\"  \\\" * (depth + 1)\\n            snapshot[\\\"directory_structure\\\"].append(f\\\"{file_indent}[FILE] {file}\\\")\\n\\n            # Skip binaries, but read everything else (logs, env, py, json)\\n            if path.suffix.lower() in binary_extensions or file == output_name:\\n                continue\\n                \\n            try:\\n                with open(path, 'r', encoding='utf-8', errors='ignore') as f:\\n                    content = f.read()\\n                \\n                # Determine module or category\\n                parts = Path(rel_path).parts\\n                category = parts[0] if len(parts) > 1 else \\\"root\\\"\\n\\n                print(f\\\"Indexing for Verification: {rel_path}\\\")\\n\\n                snapshot[\\\"files\\\"].append({\\n                    \\\"path\\\": rel_path,\\n                    \\\"category\\\": category,\\n                    \\\"content\\\": content,\\n                    \\\"size_chars\\\": len(content)\\n                })\\n                file_count += 1\\n                \\n            except Exception as e:\\n                print(f\\\"Could not read {rel_path}: {e}\\\")\\n\\n    # Save the exhaustive snapshot\\n    try:\\n        output_path = base_dir / output_name\\n        with open(output_path, 'w', encoding='utf-8') as f:\\n            json.dump(snapshot, f, indent=2)\\n        print(f\\\"\\\\n--- Scan Complete ---\\\")\\n        print(f\\\"Verification file created: {output_path}\\\")\\n        print(f\\\"Total source/log files captured: {file_count}\\\")\\n    except Exception as e:\\n        print(f\\\"Critical error writing snapshot: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    create_verification_snapshot()\",\n      \"size_bytes\": 3129,\n      \"labels\": {\n        \"ast_node_count\": 443,\n        \"function_count\": 1,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"d9a556184a57524803e3b2769c5aa9d8\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"config/settings.py\",\n      \"content_block\": \"\\\"\\\"\\\"config/settings.py\\nimport os\\nimport logging\\nfrom pathlib import Path\\nfrom typing import Final, Optional\\nfrom dotenv import load_dotenv\\n\\n# Load environmental variables\\nload_dotenv()\\n\\n# Global Logging Configuration\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n    handlers=[\\n        logging.FileHandler('aletheia_system.log'),\\n        logging.StreamHandler()\\n    ]\\n)\\n\\nclass Settings:\\n    \\\"\\\"\\\"\\n    Centralized configuration engine for Aletheia RAG Infrastructure.\\n    \\\"\\\"\\\"\\n    # Section 1: Directory Management\\n    # Resolves to the parent of 'config', which is the root 'Ingest_pipeline_V2'\\n    BASE_DIR: Final[Path] = Path(__file__).resolve().parent.parent\\n    \\n    DATA_DIR: Final[Path] = BASE_DIR / \\\"data\\\"\\n    RAW_LANDING_DIR: Final[Path] = DATA_DIR / \\\"raw_landing\\\"\\n    PROCESSED_ARCHIVE_DIR: Final[Path] = DATA_DIR / \\\"processed_archive\\\"\\n    BACKUP_DIR: Final[Path] = PROCESSED_ARCHIVE_DIR / \\\"backups\\\"\\n    \\n    # Section 2: Storage Paths\\n    CHROMA_DB_PATH: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\"chroma_db\\\"\\n    EMBEDDING_CACHE_DIR: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\".embedding_cache\\\"\\n    USAGE_LOG_PATH: Final[Path] = BASE_DIR / \\\"logs\\\" / \\\"usage_stats.json\\\"\\n\\n    # Section 3: Database (MongoDB)\\n    MONGO_URI: Final[str] = os.getenv(\\\"MONGO_URI\\\", \\\"mongodb://localhost:27017\\\")\\n    DB_NAME: Final[str] = \\\"aletheia_memory\\\"\\n    COLLECTION_TRUTH: Final[str] = \\\"canonical_truth\\\"\\n    COLLECTION_TRACES: Final[str] = \\\"reasoning_traces\\\"\\n\\n    # Section 4: Inference (LM Studio)\\n    LM_STUDIO_BASE_URL: Final[str] = os.getenv(\\\"LM_STUDIO_URL\\\", \\\"http://localhost:1234/v1\\\")\\n    EMBEDDING_MODEL: Final[str] = \\\"nomic-ai/nomic-embed-text-v1.5-GGUF\\\"\\n    NOMIC_PREFIX: Final[str] = \\\"search_document: \\\" \\n\\n    # Section 5: RAG & OCR Logic\\n    CHUNK_SIZE: Final[int] = 1500 \\n    CHUNK_OVERLAP: Final[int] = 200\\n    OCR_TEXT_DENSITY_THRESHOLD: int = 50 # Characters per page below which OCR is triggered\\n    NUM_RETRIEVAL_RESULTS: int = 5\\n\\n    def validate_settings(self):\\n        \\\"\\\"\\\"Ensures directories exist and critical settings are present.\\\"\\\"\\\"\\n        paths = [\\n            self.DATA_DIR, self.RAW_LANDING_DIR, self.PROCESSED_ARCHIVE_DIR, \\n            self.BACKUP_DIR, self.CHROMA_DB_PATH, self.USAGE_LOG_PATH.parent,\\n            self.EMBEDDING_CACHE_DIR\\n        ]\\n        for p in paths:\\n            p.mkdir(parents=True, exist_ok=True)\\n        \\n        if not self.MONGO_URI:\\n            raise ValueError(\\\"MONGO_URI environment variable is missing.\\\")\\n\\nsettings = Settings()\\nsettings.validate_settings()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import os\\nimport logging\\nfrom pathlib import Path\\nfrom typing import Final, Optional\\nfrom dotenv import load_dotenv\\n\\n# Load environmental variables\\nload_dotenv()\\n\\n# Global Logging Configuration\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\\n    handlers=[\\n        logging.FileHandler('aletheia_system.log'),\\n        logging.StreamHandler()\\n    ]\\n)\\n\\nclass Settings:\\n    \\\"\\\"\\\"\\n    Centralized configuration engine for Aletheia RAG Infrastructure.\\n    \\\"\\\"\\\"\\n    # Section 1: Directory Management\\n    # Resolves to the parent of 'config', which is the root 'Ingest_pipeline_V2'\\n    BASE_DIR: Final[Path] = Path(__file__).resolve().parent.parent\\n    \\n    DATA_DIR: Final[Path] = BASE_DIR / \\\"data\\\"\\n    RAW_LANDING_DIR: Final[Path] = DATA_DIR / \\\"raw_landing\\\"\\n    PROCESSED_ARCHIVE_DIR: Final[Path] = DATA_DIR / \\\"processed_archive\\\"\\n    BACKUP_DIR: Final[Path] = PROCESSED_ARCHIVE_DIR / \\\"backups\\\"\\n    \\n    # Section 2: Storage Paths\\n    CHROMA_DB_PATH: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\"chroma_db\\\"\\n    EMBEDDING_CACHE_DIR: Final[Path] = BASE_DIR / \\\"memory\\\" / \\\".embedding_cache\\\"\\n    USAGE_LOG_PATH: Final[Path] = BASE_DIR / \\\"logs\\\" / \\\"usage_stats.json\\\"\\n\\n    # Section 3: Database (MongoDB)\\n    MONGO_URI: Final[str] = os.getenv(\\\"MONGO_URI\\\", \\\"mongodb://localhost:27017\\\")\\n    DB_NAME: Final[str] = \\\"aletheia_memory\\\"\\n    COLLECTION_TRUTH: Final[str] = \\\"canonical_truth\\\"\\n    COLLECTION_TRACES: Final[str] = \\\"reasoning_traces\\\"\\n\\n    # Section 4: Inference (LM Studio)\\n    LM_STUDIO_BASE_URL: Final[str] = os.getenv(\\\"LM_STUDIO_URL\\\", \\\"http://localhost:1234/v1\\\")\\n    EMBEDDING_MODEL: Final[str] = \\\"nomic-ai/nomic-embed-text-v1.5-GGUF\\\"\\n    NOMIC_PREFIX: Final[str] = \\\"search_document: \\\" \\n\\n    # Section 5: RAG & OCR Logic\\n    CHUNK_SIZE: Final[int] = 1500 \\n    CHUNK_OVERLAP: Final[int] = 200\\n    OCR_TEXT_DENSITY_THRESHOLD: int = 50 # Characters per page below which OCR is triggered\\n    NUM_RETRIEVAL_RESULTS: int = 5\\n\\n    def validate_settings(self):\\n        \\\"\\\"\\\"Ensures directories exist and critical settings are present.\\\"\\\"\\\"\\n        paths = [\\n            self.DATA_DIR, self.RAW_LANDING_DIR, self.PROCESSED_ARCHIVE_DIR, \\n            self.BACKUP_DIR, self.CHROMA_DB_PATH, self.USAGE_LOG_PATH.parent,\\n            self.EMBEDDING_CACHE_DIR\\n        ]\\n        for p in paths:\\n            p.mkdir(parents=True, exist_ok=True)\\n        \\n        if not self.MONGO_URI:\\n            raise ValueError(\\\"MONGO_URI environment variable is missing.\\\")\\n\\nsettings = Settings()\\nsettings.validate_settings()\",\n      \"size_bytes\": 2558,\n      \"labels\": {\n        \"ast_node_count\": 367,\n        \"function_count\": 1,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"1323dcc6d85cb5bceac7402cff7ddfa6\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/codebase_processor.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/codebase_processor.py\\nimport logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass CodebaseProcessor:\\n    \\\"\\\"\\\"\\n    Handles processing of text-based files (Python, JSON, Markdown, etc.).\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Reads text/code files directly and chunks them.\\\"\\\"\\\"\\n        documents = []\\n        try:\\n            # Use errors='ignore' to prevent crashing on non-UTF-8 binary artifacts\\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\\n                raw_text = f.read()\\n            \\n            if raw_text.strip():\\n                return self._chunk_text(raw_text, str(file_path), file_path.name)\\n        except Exception as e:\\n            logger.error(f\\\"Error processing text file {file_path.name}: {e}\\\")\\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Splits text into sliding window chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": 0, # Not applicable for flat text files\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"codebase\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass CodebaseProcessor:\\n    \\\"\\\"\\\"\\n    Handles processing of text-based files (Python, JSON, Markdown, etc.).\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Reads text/code files directly and chunks them.\\\"\\\"\\\"\\n        documents = []\\n        try:\\n            # Use errors='ignore' to prevent crashing on non-UTF-8 binary artifacts\\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\\n                raw_text = f.read()\\n            \\n            if raw_text.strip():\\n                return self._chunk_text(raw_text, str(file_path), file_path.name)\\n        except Exception as e:\\n            logger.error(f\\\"Error processing text file {file_path.name}: {e}\\\")\\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Splits text into sliding window chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": 0, # Not applicable for flat text files\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"codebase\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\",\n      \"size_bytes\": 1980,\n      \"labels\": {\n        \"ast_node_count\": 273,\n        \"function_count\": 3,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"78287a2d7d09009729b2d1c6e333de43\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/ingest_manager.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/ingest_manager.py\\nimport logging\\nfrom pathlib import Path\\nfrom pymongo import MongoClient\\nfrom pymongo.errors import BulkWriteError\\nimport chromadb\\nfrom datetime import datetime\\nfrom config.settings import settings\\n# FIX: Consistent imports\\nfrom core.pdf_processor import PDFProcessor\\nfrom core.codebase_processor import CodebaseProcessor  # Matches lowercase filename\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass IngestManager:\\n    \\\"\\\"\\\"\\n    Manages the complete ingestion pipeline for PDF and Text/Code documents.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        # Initialize Databases\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n        \\n        # Initialize ChromaDB\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # Initialize Core Engines\\n        self.pdf_processor = PDFProcessor()\\n        self.codebase_processor = CodebaseProcessor()\\n        self.embedder = EmbeddingClient()\\n        \\n    def process_file(self, file_path: Path) -> bool:\\n        \\\"\\\"\\\"\\n        Processes a single file through the ingestion pipeline.\\n        Routes to the appropriate processor based on file type.\\n        \\\"\\\"\\\"\\n        try:\\n            logger.info(f\\\"Processing: {file_path.name}\\\")\\n            \\n            # 1. Select Processor Strategy\\n            if file_path.suffix.lower() == '.pdf':\\n                chunks = list(self.pdf_processor.process_file(file_path))\\n            else:\\n                # Fallback to codebase processor for .py, .txt, .md, .json, etc.\\n                chunks = list(self.codebase_processor.process_file(file_path))\\n            \\n            if not chunks:\\n                logger.warning(f\\\"No usable content found in {file_path.name}\\\")\\n                return False\\n            \\n            # 2. Vectorization and Persistence\\n            chroma_ids = []\\n            chroma_embeddings = []\\n            chroma_metadatas = []\\n            mongo_docs = []\\n            \\n            for i, chunk in enumerate(chunks):\\n                content_text = chunk[\\\"content\\\"]\\n                chunk_meta = chunk[\\\"metadata\\\"]\\n                \\n                # Generate unique ID\\n                file_hash = chunk_meta.get('file_name', file_path.name)\\n                doc_id = f\\\"{file_hash}_{i}\\\"\\n                \\n                # Get Embedding\\n                vector = self.embedder.get_embedding(content_text)\\n                if not vector:\\n                    continue\\n                \\n                # Prepare Mongo Document\\n                mongo_docs.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"content\\\": content_text,\\n                    \\\"metadata\\\": chunk_meta,\\n                    \\\"ingested_at\\\": datetime.utcnow().isoformat()\\n                })\\n\\n                # Prepare Chroma Data\\n                chroma_ids.append(doc_id)\\n                chroma_embeddings.append(vector)\\n                chroma_metadatas.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"page\\\": chunk_meta.get('page_number', 0),\\n                    \\\"file_name\\\": chunk_meta.get('file_name', 'unknown')\\n                })\\n\\n            # Bulk Write to Mongo (Robust Duplicate Handling)\\n            if mongo_docs:\\n                try:\\n                    # ordered=False continues processing even if one insert fails (e.g. duplicate)\\n                    self.collection_truth.insert_many(mongo_docs, ordered=False)\\n                except BulkWriteError as bwe:\\n                    # Log duplicates as info, actual errors as warning\\n                    duplicates = [e for e in bwe.details['writeErrors'] if e['code'] == 11000]\\n                    if len(duplicates) == len(mongo_docs):\\n                        logger.info(f\\\"Skipping {file_path.name}: All chunks already exist in DB.\\\")\\n                        return True\\n                    elif duplicates:\\n                        logger.info(f\\\"Partial insert for {file_path.name}: {len(duplicates)} duplicates skipped.\\\")\\n                    else:\\n                        # Sanitize error message to prevent UnicodeEncodeError in Windows consoles\\n                        error_msg = str(bwe).encode('ascii', 'replace').decode('ascii')\\n                        logger.warning(f\\\"MongoDB Bulk Write Error: {error_msg}\\\")\\n\\n            # Bulk Write to Chroma\\n            if chroma_ids:\\n                try:\\n                    self.collection_index.add(\\n                        ids=chroma_ids,\\n                        embeddings=chroma_embeddings,\\n                        metadatas=chroma_metadatas,\\n                        documents=[d['content'] for d in mongo_docs]\\n                    )\\n                except Exception as e:\\n                    # Chroma might error on duplicates, but usually updates/upserts.\\n                    # If it fails, log and continue.\\n                    logger.warning(f\\\"ChromaDB Write Warning for {file_path.name}: {e}\\\")\\n                    \\n            logger.info(f\\\"Successfully processed: {file_path.name}\\\")\\n            return True\\n            \\n        except Exception as e:\\n            # Catch-all to ensure one bad file doesn't crash the whole batch\\n            # Sanitize error message to prevent UnicodeEncodeError\\n            safe_error = str(e).encode('ascii', 'replace').decode('ascii')\\n            logger.error(f\\\"Error processing file {file_path.name}: {safe_error}\\\")\\n            return False\\n    \\n    def process_all(self):\\n        \\\"\\\"\\\"Processes all supported files in the raw landing directory recursively.\\\"\\\"\\\"\\n        extensions = [\\\"*.pdf\\\", \\\"*.txt\\\", \\\"*.py\\\", \\\"*.md\\\", \\\"*.json\\\", \\\"*.sh\\\", \\\"*.ps1\\\"]\\n        all_files = []\\n        \\n        for ext in extensions:\\n            all_files.extend(list(settings.RAW_LANDING_DIR.rglob(ext)))\\n            \\n        if not all_files:\\n            logger.info(f\\\"No supported files found in {settings.RAW_LANDING_DIR}\\\")\\n            return\\n            \\n        logger.info(f\\\"Starting ingestion of {len(all_files)} files.\\\")\\n        processed_count = sum(1 for f in all_files if self.process_file(f))\\n        logger.info(f\\\"Ingestion completed. Processed {processed_count}/{len(all_files)}.\\\")\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import logging\\nfrom pathlib import Path\\nfrom pymongo import MongoClient\\nfrom pymongo.errors import BulkWriteError\\nimport chromadb\\nfrom datetime import datetime\\nfrom config.settings import settings\\n# FIX: Consistent imports\\nfrom core.pdf_processor import PDFProcessor\\nfrom core.codebase_processor import CodebaseProcessor  # Matches lowercase filename\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass IngestManager:\\n    \\\"\\\"\\\"\\n    Manages the complete ingestion pipeline for PDF and Text/Code documents.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        # Initialize Databases\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n        \\n        # Initialize ChromaDB\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # Initialize Core Engines\\n        self.pdf_processor = PDFProcessor()\\n        self.codebase_processor = CodebaseProcessor()\\n        self.embedder = EmbeddingClient()\\n        \\n    def process_file(self, file_path: Path) -> bool:\\n        \\\"\\\"\\\"\\n        Processes a single file through the ingestion pipeline.\\n        Routes to the appropriate processor based on file type.\\n        \\\"\\\"\\\"\\n        try:\\n            logger.info(f\\\"Processing: {file_path.name}\\\")\\n            \\n            # 1. Select Processor Strategy\\n            if file_path.suffix.lower() == '.pdf':\\n                chunks = list(self.pdf_processor.process_file(file_path))\\n            else:\\n                # Fallback to codebase processor for .py, .txt, .md, .json, etc.\\n                chunks = list(self.codebase_processor.process_file(file_path))\\n            \\n            if not chunks:\\n                logger.warning(f\\\"No usable content found in {file_path.name}\\\")\\n                return False\\n            \\n            # 2. Vectorization and Persistence\\n            chroma_ids = []\\n            chroma_embeddings = []\\n            chroma_metadatas = []\\n            mongo_docs = []\\n            \\n            for i, chunk in enumerate(chunks):\\n                content_text = chunk[\\\"content\\\"]\\n                chunk_meta = chunk[\\\"metadata\\\"]\\n                \\n                # Generate unique ID\\n                file_hash = chunk_meta.get('file_name', file_path.name)\\n                doc_id = f\\\"{file_hash}_{i}\\\"\\n                \\n                # Get Embedding\\n                vector = self.embedder.get_embedding(content_text)\\n                if not vector:\\n                    continue\\n                \\n                # Prepare Mongo Document\\n                mongo_docs.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"content\\\": content_text,\\n                    \\\"metadata\\\": chunk_meta,\\n                    \\\"ingested_at\\\": datetime.utcnow().isoformat()\\n                })\\n\\n                # Prepare Chroma Data\\n                chroma_ids.append(doc_id)\\n                chroma_embeddings.append(vector)\\n                chroma_metadatas.append({\\n                    \\\"file_hash\\\": file_hash,\\n                    \\\"chunk_index\\\": i,\\n                    \\\"page\\\": chunk_meta.get('page_number', 0),\\n                    \\\"file_name\\\": chunk_meta.get('file_name', 'unknown')\\n                })\\n\\n            # Bulk Write to Mongo (Robust Duplicate Handling)\\n            if mongo_docs:\\n                try:\\n                    # ordered=False continues processing even if one insert fails (e.g. duplicate)\\n                    self.collection_truth.insert_many(mongo_docs, ordered=False)\\n                except BulkWriteError as bwe:\\n                    # Log duplicates as info, actual errors as warning\\n                    duplicates = [e for e in bwe.details['writeErrors'] if e['code'] == 11000]\\n                    if len(duplicates) == len(mongo_docs):\\n                        logger.info(f\\\"Skipping {file_path.name}: All chunks already exist in DB.\\\")\\n                        return True\\n                    elif duplicates:\\n                        logger.info(f\\\"Partial insert for {file_path.name}: {len(duplicates)} duplicates skipped.\\\")\\n                    else:\\n                        # Sanitize error message to prevent UnicodeEncodeError in Windows consoles\\n                        error_msg = str(bwe).encode('ascii', 'replace').decode('ascii')\\n                        logger.warning(f\\\"MongoDB Bulk Write Error: {error_msg}\\\")\\n\\n            # Bulk Write to Chroma\\n            if chroma_ids:\\n                try:\\n                    self.collection_index.add(\\n                        ids=chroma_ids,\\n                        embeddings=chroma_embeddings,\\n                        metadatas=chroma_metadatas,\\n                        documents=[d['content'] for d in mongo_docs]\\n                    )\\n                except Exception as e:\\n                    # Chroma might error on duplicates, but usually updates/upserts.\\n                    # If it fails, log and continue.\\n                    logger.warning(f\\\"ChromaDB Write Warning for {file_path.name}: {e}\\\")\\n                    \\n            logger.info(f\\\"Successfully processed: {file_path.name}\\\")\\n            return True\\n            \\n        except Exception as e:\\n            # Catch-all to ensure one bad file doesn't crash the whole batch\\n            # Sanitize error message to prevent UnicodeEncodeError\\n            safe_error = str(e).encode('ascii', 'replace').decode('ascii')\\n            logger.error(f\\\"Error processing file {file_path.name}: {safe_error}\\\")\\n            return False\\n    \\n    def process_all(self):\\n        \\\"\\\"\\\"Processes all supported files in the raw landing directory recursively.\\\"\\\"\\\"\\n        extensions = [\\\"*.pdf\\\", \\\"*.txt\\\", \\\"*.py\\\", \\\"*.md\\\", \\\"*.json\\\", \\\"*.sh\\\", \\\"*.ps1\\\"]\\n        all_files = []\\n        \\n        for ext in extensions:\\n            all_files.extend(list(settings.RAW_LANDING_DIR.rglob(ext)))\\n            \\n        if not all_files:\\n            logger.info(f\\\"No supported files found in {settings.RAW_LANDING_DIR}\\\")\\n            return\\n            \\n        logger.info(f\\\"Starting ingestion of {len(all_files)} files.\\\")\\n        processed_count = sum(1 for f in all_files if self.process_file(f))\\n        logger.info(f\\\"Ingestion completed. Processed {processed_count}/{len(all_files)}.\\\")\",\n      \"size_bytes\": 6451,\n      \"labels\": {\n        \"ast_node_count\": 732,\n        \"function_count\": 3,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"c569701e1f7964941f7deeebb9ea7283\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/pdf_processor.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/pdf_processor.py\\nimport fitz # PyMuPDF\\nimport logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\nfrom utils import ocr_service\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass PDFProcessor:\\n    \\\"\\\"\\\"\\n    Specialized processor for PDF documents with OCR capabilities.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"\\n        Extracts text from PDF page-by-page, applying OCR if text density is low.\\n        \\\"\\\"\\\"\\n        documents = []\\n        try:\\n            doc = fitz.open(file_path)\\n            for page_num, page in enumerate(doc):\\n                raw_text = page.get_text()\\n\\n                # Decision Gate: Check for Scanned Pages\\n                if len(raw_text.strip()) < self.settings.OCR_TEXT_DENSITY_THRESHOLD:\\n                    logger.warning(f\\\"Low text density on page {page_num + 1} of {file_path.name}. Checking OCR...\\\")\\n                    try:\\n                        image = ocr_service.convert_page_to_image(str(file_path), page_num + 1)\\n                        if image:\\n                            ocr_text = ocr_service.extract_text_from_image(image)\\n                            # Only use OCR if it yielded more info than the raw extraction\\n                            if len(ocr_text.strip()) > len(raw_text.strip()):\\n                                raw_text = ocr_text\\n                                logger.info(f\\\"OCR improved text yield for page {page_num + 1}.\\\")\\n                    except Exception as ocr_e:\\n                        logger.error(f\\\"OCR failed for page {page_num + 1}: {ocr_e}\\\")\\n\\n                # Chunking\\n                if raw_text.strip():\\n                    page_docs = self._chunk_text(raw_text, str(file_path), file_path.name, page_num + 1)\\n                    documents.extend(page_docs)\\n            \\n            doc.close()\\n        except Exception as e:\\n            logger.error(f\\\"Error processing PDF {file_path}: {e}\\\")\\n            \\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str, page_num: int) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Helper to split text into chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": page_num,\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"pdf\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import fitz # PyMuPDF\\nimport logging\\nfrom pathlib import Path\\nfrom typing import List, Dict, Any\\nfrom config.settings import settings\\nfrom utils import ocr_service\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass PDFProcessor:\\n    \\\"\\\"\\\"\\n    Specialized processor for PDF documents with OCR capabilities.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.settings = settings\\n\\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"\\n        Extracts text from PDF page-by-page, applying OCR if text density is low.\\n        \\\"\\\"\\\"\\n        documents = []\\n        try:\\n            doc = fitz.open(file_path)\\n            for page_num, page in enumerate(doc):\\n                raw_text = page.get_text()\\n\\n                # Decision Gate: Check for Scanned Pages\\n                if len(raw_text.strip()) < self.settings.OCR_TEXT_DENSITY_THRESHOLD:\\n                    logger.warning(f\\\"Low text density on page {page_num + 1} of {file_path.name}. Checking OCR...\\\")\\n                    try:\\n                        image = ocr_service.convert_page_to_image(str(file_path), page_num + 1)\\n                        if image:\\n                            ocr_text = ocr_service.extract_text_from_image(image)\\n                            # Only use OCR if it yielded more info than the raw extraction\\n                            if len(ocr_text.strip()) > len(raw_text.strip()):\\n                                raw_text = ocr_text\\n                                logger.info(f\\\"OCR improved text yield for page {page_num + 1}.\\\")\\n                    except Exception as ocr_e:\\n                        logger.error(f\\\"OCR failed for page {page_num + 1}: {ocr_e}\\\")\\n\\n                # Chunking\\n                if raw_text.strip():\\n                    page_docs = self._chunk_text(raw_text, str(file_path), file_path.name, page_num + 1)\\n                    documents.extend(page_docs)\\n            \\n            doc.close()\\n        except Exception as e:\\n            logger.error(f\\\"Error processing PDF {file_path}: {e}\\\")\\n            \\n        return documents\\n\\n    def _chunk_text(self, text: str, file_path: str, file_name: str, page_num: int) -> List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Helper to split text into chunks.\\\"\\\"\\\"\\n        chunk_size = self.settings.CHUNK_SIZE\\n        overlap = self.settings.CHUNK_OVERLAP\\n        chunks = []\\n        \\n        text_len = len(text)\\n        start = 0\\n        chunk_idx = 0\\n        \\n        while start < text_len:\\n            end = min(start + chunk_size, text_len)\\n            chunk_content = text[start:end]\\n            \\n            chunks.append({\\n                \\\"content\\\": chunk_content,\\n                \\\"metadata\\\": {\\n                    \\\"file_path\\\": file_path,\\n                    \\\"file_name\\\": file_name,\\n                    \\\"page_number\\\": page_num,\\n                    \\\"chunk_index\\\": chunk_idx,\\n                    \\\"file_type\\\": \\\"pdf\\\"\\n                }\\n            })\\n            \\n            start += (chunk_size - overlap)\\n            chunk_idx += 1\\n            \\n        return chunks\",\n      \"size_bytes\": 3040,\n      \"labels\": {\n        \"ast_node_count\": 438,\n        \"function_count\": 3,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"9197055079c29b5f00fe2c764f13fe9a\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"core/retrieval_controller.py\",\n      \"content_block\": \"\\\"\\\"\\\"core/retrieval_controller.py\\nimport logging\\nimport chromadb\\nfrom pymongo import MongoClient\\nfrom config.settings import settings\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass RetrievalController:\\n    def __init__(self):\\n        self.embedding_client = EmbeddingClient()\\n        \\n        # ChromaDB (Index)\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # MongoDB (Canonical Truth)\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n\\n    def query(self, query: str) -> str:\\n        \\\"\\\"\\\"Retrieves context and generates a response.\\\"\\\"\\\"\\n        # 1. Embed Query\\n        query_embedding = self.embedding_client.get_embedding(query)\\n        if not query_embedding:\\n            return \\\"Error: Could not process query.\\\"\\n\\n        # 2. Retrieve from ChromaDB\\n        results = self.collection_index.query(\\n            query_embeddings=[query_embedding],\\n            n_results=settings.NUM_RETRIEVAL_RESULTS,\\n            include=['metadatas']\\n        )\\n\\n        # 3. Fetch Full Content from MongoDB (Canonical Truth)\\n        # We rely on the index to find *where* the data is, but fetch the *clean* data from Mongo.\\n        context_docs = []\\n        if results and results['metadatas'] and results['metadatas'][0]:\\n            for meta in results['metadatas'][0]:\\n                file_hash = meta.get('file_hash')\\n                chunk_index = meta.get('chunk_index')\\n                \\n                record = self.collection_truth.find_one({\\n                    \\\"file_hash\\\": file_hash, \\n                    \\\"chunk_index\\\": chunk_index\\n                })\\n                \\n                if record:\\n                    context_docs.append(record['content'])\\n        \\n        if not context_docs:\\n            return \\\"No relevant information found in the archives.\\\"\\n\\n        # 4. Construct Prompt\\n        context_text = \\\"\\\\n\\\\n---\\\\n\\\\n\\\".join(context_docs)\\n        return f\\\"Based on the following research:\\\\n\\\\n{context_text}\\\\n\\\\nAnswer: {query}\\\"\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import logging\\nimport chromadb\\nfrom pymongo import MongoClient\\nfrom config.settings import settings\\nfrom utils.embedding_client import EmbeddingClient\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass RetrievalController:\\n    def __init__(self):\\n        self.embedding_client = EmbeddingClient()\\n        \\n        # ChromaDB (Index)\\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\\\"aletheia_index\\\")\\n        \\n        # MongoDB (Canonical Truth)\\n        self.mongo_client = MongoClient(settings.MONGO_URI)\\n        self.db = self.mongo_client[settings.DB_NAME]\\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\\n\\n    def query(self, query: str) -> str:\\n        \\\"\\\"\\\"Retrieves context and generates a response.\\\"\\\"\\\"\\n        # 1. Embed Query\\n        query_embedding = self.embedding_client.get_embedding(query)\\n        if not query_embedding:\\n            return \\\"Error: Could not process query.\\\"\\n\\n        # 2. Retrieve from ChromaDB\\n        results = self.collection_index.query(\\n            query_embeddings=[query_embedding],\\n            n_results=settings.NUM_RETRIEVAL_RESULTS,\\n            include=['metadatas']\\n        )\\n\\n        # 3. Fetch Full Content from MongoDB (Canonical Truth)\\n        # We rely on the index to find *where* the data is, but fetch the *clean* data from Mongo.\\n        context_docs = []\\n        if results and results['metadatas'] and results['metadatas'][0]:\\n            for meta in results['metadatas'][0]:\\n                file_hash = meta.get('file_hash')\\n                chunk_index = meta.get('chunk_index')\\n                \\n                record = self.collection_truth.find_one({\\n                    \\\"file_hash\\\": file_hash, \\n                    \\\"chunk_index\\\": chunk_index\\n                })\\n                \\n                if record:\\n                    context_docs.append(record['content'])\\n        \\n        if not context_docs:\\n            return \\\"No relevant information found in the archives.\\\"\\n\\n        # 4. Construct Prompt\\n        context_text = \\\"\\\\n\\\\n---\\\\n\\\\n\\\".join(context_docs)\\n        return f\\\"Based on the following research:\\\\n\\\\n{context_text}\\\\n\\\\nAnswer: {query}\\\"\",\n      \"size_bytes\": 2270,\n      \"labels\": {\n        \"ast_node_count\": 269,\n        \"function_count\": 2,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"f29ac7b2821ef048e5b25928f8f2380d\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"settings/init.py\",\n      \"content_block\": \"\\\"\\\"\\\"settings/init.py\\nimport pymongo\\nimport sys\\nfrom pathlib import Path\\n\\n# Fix path to ensure imports work from top-level directory\\nsys.path.append(str(Path(__file__).resolve().parents[1]))\\n\\nfrom config.settings import settings\\n\\ndef init():\\n    try:\\n        client = pymongo.MongoClient(settings.MONGO_URI)\\n        db = client[settings.DB_NAME]\\n        \\n        colls = [settings.COLLECTION_TRUTH, settings.COLLECTION_TRACES]\\n        for c in colls:\\n            if c not in db.list_collection_names():\\n                db.create_collection(c)\\n                print(f\\\"Provisioned: {c}\\\")\\n                \\n        # Create unique index on file_hash and chunk_index pair for granular retrieval\\n        db[settings.COLLECTION_TRUTH].create_index(\\n            [(\\\"file_hash\\\", pymongo.ASCENDING), (\\\"chunk_index\\\", pymongo.ASCENDING)], \\n            unique=True\\n        )\\n        print(\\\"Aletheia Memory initialized successfully.\\\")\\n        \\n    except Exception as e:\\n        print(f\\\"Initialization failed: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    init()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import pymongo\\nimport sys\\nfrom pathlib import Path\\n\\n# Fix path to ensure imports work from top-level directory\\nsys.path.append(str(Path(__file__).resolve().parents[1]))\\n\\nfrom config.settings import settings\\n\\ndef init():\\n    try:\\n        client = pymongo.MongoClient(settings.MONGO_URI)\\n        db = client[settings.DB_NAME]\\n        \\n        colls = [settings.COLLECTION_TRUTH, settings.COLLECTION_TRACES]\\n        for c in colls:\\n            if c not in db.list_collection_names():\\n                db.create_collection(c)\\n                print(f\\\"Provisioned: {c}\\\")\\n                \\n        # Create unique index on file_hash and chunk_index pair for granular retrieval\\n        db[settings.COLLECTION_TRUTH].create_index(\\n            [(\\\"file_hash\\\", pymongo.ASCENDING), (\\\"chunk_index\\\", pymongo.ASCENDING)], \\n            unique=True\\n        )\\n        print(\\\"Aletheia Memory initialized successfully.\\\")\\n        \\n    except Exception as e:\\n        print(f\\\"Initialization failed: {e}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    init()\",\n      \"size_bytes\": 1041,\n      \"labels\": {\n        \"ast_node_count\": 161,\n        \"function_count\": 1,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"c08000706176c215599fd3275903e9ef\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"utils/embedding_client.py\",\n      \"content_block\": \"\\\"\\\"\\\"utils/embedding_client.py\\nimport requests\\nimport logging\\nimport time\\nfrom typing import List, Optional\\nfrom functools import lru_cache\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass EmbeddingClient:\\n    \\\"\\\"\\\"\\n    Interface for local LM Studio embeddings with caching and resource awareness.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.base_url = f\\\"{settings.LM_STUDIO_BASE_URL}/embeddings\\\"\\n        self.last_activity = time.time()\\n\\n    def _check_resource_status(self):\\n        \\\"\\\"\\\"\\n        Placeholder for checking system health or triggering model unloads.\\n        Could be extended to use LM Studio's /v1/models endpoint to check TTL.\\n        \\\"\\\"\\\"\\n        self.last_activity = time.time()\\n        # In a JIT strategy, we could ping a custom management script here\\n        pass\\n\\n    @lru_cache(maxsize=2048) # Increased cache size for better performance\\n    def get_embedding(self, text: str) -> Optional[List[float]]:\\n        \\\"\\\"\\\"\\n        Generates a vector with LRU caching.\\n        Note: Nomic models require the 'search_document: ' prefix.\\n        \\\"\\\"\\\"\\n        self._check_resource_status()\\n        \\n        prefixed_text = f\\\"{settings.NOMIC_PREFIX}{text}\\\"\\n        payload = {\\\"input\\\": prefixed_text, \\\"model\\\": settings.EMBEDDING_MODEL}\\n        \\n        # Implement internal retry logic\\n        for attempt in range(3):\\n            try:\\n                response = requests.post(self.base_url, json=payload, timeout=30)\\n                response.raise_for_status()\\n                return response.json()[\\\"data\\\"][0][\\\"embedding\\\"]\\n            except Exception as e:\\n                wait = (attempt + 1) * 2\\n                logger.warning(f\\\"Embedding failed (Attempt {attempt+1}): {e}. Retrying in {wait}s...\\\")\\n                time.sleep(wait)\\n        \\n        logger.error(f\\\"Failed to retrieve embedding after retries for text snippet.\\\")\\n        return None\\n\\n    def clear_cache(self):\\n        \\\"\\\"\\\"Clears the embedding cache.\\\"\\\"\\\"\\n        self.get_embedding.cache_clear()\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import requests\\nimport logging\\nimport time\\nfrom typing import List, Optional\\nfrom functools import lru_cache\\nfrom config.settings import settings\\n\\nlogger = logging.getLogger(__name__)\\n\\nclass EmbeddingClient:\\n    \\\"\\\"\\\"\\n    Interface for local LM Studio embeddings with caching and resource awareness.\\n    \\\"\\\"\\\"\\n    def __init__(self):\\n        self.base_url = f\\\"{settings.LM_STUDIO_BASE_URL}/embeddings\\\"\\n        self.last_activity = time.time()\\n\\n    def _check_resource_status(self):\\n        \\\"\\\"\\\"\\n        Placeholder for checking system health or triggering model unloads.\\n        Could be extended to use LM Studio's /v1/models endpoint to check TTL.\\n        \\\"\\\"\\\"\\n        self.last_activity = time.time()\\n        # In a JIT strategy, we could ping a custom management script here\\n        pass\\n\\n    @lru_cache(maxsize=2048) # Increased cache size for better performance\\n    def get_embedding(self, text: str) -> Optional[List[float]]:\\n        \\\"\\\"\\\"\\n        Generates a vector with LRU caching.\\n        Note: Nomic models require the 'search_document: ' prefix.\\n        \\\"\\\"\\\"\\n        self._check_resource_status()\\n        \\n        prefixed_text = f\\\"{settings.NOMIC_PREFIX}{text}\\\"\\n        payload = {\\\"input\\\": prefixed_text, \\\"model\\\": settings.EMBEDDING_MODEL}\\n        \\n        # Implement internal retry logic\\n        for attempt in range(3):\\n            try:\\n                response = requests.post(self.base_url, json=payload, timeout=30)\\n                response.raise_for_status()\\n                return response.json()[\\\"data\\\"][0][\\\"embedding\\\"]\\n            except Exception as e:\\n                wait = (attempt + 1) * 2\\n                logger.warning(f\\\"Embedding failed (Attempt {attempt+1}): {e}. Retrying in {wait}s...\\\")\\n                time.sleep(wait)\\n        \\n        logger.error(f\\\"Failed to retrieve embedding after retries for text snippet.\\\")\\n        return None\\n\\n    def clear_cache(self):\\n        \\\"\\\"\\\"Clears the embedding cache.\\\"\\\"\\\"\\n        self.get_embedding.cache_clear()\",\n      \"size_bytes\": 2004,\n      \"labels\": {\n        \"ast_node_count\": 235,\n        \"function_count\": 4,\n        \"class_count\": 1,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"838a4572ee81d2fba1369be708ac5bc1\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"utils/metadata_extractor.py\",\n      \"content_block\": \"\\\"\\\"\\\"utils/metadata_extractor.py\\nimport hashlib\\nimport logging\\nimport re\\nfrom pathlib import Path\\nfrom typing import Dict, Any, Optional\\nfrom datetime import datetime, timezone\\nimport PyPDF2\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef generate_file_hash(file_path: Path) -> str:\\n    \\\"\\\"\\\"Generates a SHA-256 hash of the file.\\\"\\\"\\\"\\n    sha256_hash = hashlib.sha256()\\n    try:\\n        with open(file_path, \\\"rb\\\") as f:\\n            for byte_block in iter(lambda: f.read(4096), b\\\"\\\"):\\n                sha256_hash.update(byte_block)\\n        return sha256_hash.hexdigest()\\n    except Exception as e:\\n        logger.error(f\\\"Hash generation failed for {file_path}: {e}\\\")\\n        return \\\"error_hash\\\"\\n\\ndef _parse_pdf_date(date_str: Optional[str]) -> Optional[str]:\\n    \\\"\\\"\\\"Converts PDF-style date strings into ISO format.\\\"\\\"\\\"\\n    if not date_str or not isinstance(date_str, str):\\n        return None\\n    \\n    clean_date = re.sub(r'[^0-9]', '', date_str)\\n    try:\\n        if len(clean_date) >= 8:\\n            return f\\\"{clean_date[0:4]}-{clean_date[4:6]}-{clean_date[6:8]}\\\"\\n    except Exception:\\n        pass\\n    return date_str\\n\\ndef _sanitize_string(text: Any) -> str:\\n    if not text or not isinstance(text, str):\\n        return \\\"Unknown\\\"\\n    clean_text = \\\"\\\".join(char for char in text if char.isprintable())\\n    return \\\" \\\".join(clean_text.split())\\n\\ndef extract_document_metadata(reader: PyPDF2.PdfReader, file_path: Path, content: str = \\\"\\\") -> Dict[str, Any]:\\n    \\\"\\\"\\\"Generates an enriched document profile.\\\"\\\"\\\"\\n    try:\\n        meta = reader.metadata\\n    except Exception:\\n        meta = None\\n\\n    word_count = len(content.split()) if content else 0\\n\\n    return {\\n        \\\"file_hash\\\": generate_file_hash(file_path),\\n        \\\"file_name\\\": file_path.name,\\n        \\\"internal_title\\\": _sanitize_string(meta.title if meta and meta.title else file_path.stem),\\n        \\\"author\\\": _sanitize_string(meta.author if meta and meta.author else \\\"Unknown Author\\\"),\\n        \\\"total_pages\\\": len(reader.pages),\\n        \\\"word_count\\\": word_count,\\n        \\\"creation_date\\\": _parse_pdf_date(meta.get('/CreationDate') if meta else None),\\n        \\\"ingested_at\\\": datetime.now(timezone.utc).isoformat(),\\n        \\\"version\\\": \\\"2.2\\\"\\n    }\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"import hashlib\\nimport logging\\nimport re\\nfrom pathlib import Path\\nfrom typing import Dict, Any, Optional\\nfrom datetime import datetime, timezone\\nimport PyPDF2\\n\\nlogger = logging.getLogger(__name__)\\n\\ndef generate_file_hash(file_path: Path) -> str:\\n    \\\"\\\"\\\"Generates a SHA-256 hash of the file.\\\"\\\"\\\"\\n    sha256_hash = hashlib.sha256()\\n    try:\\n        with open(file_path, \\\"rb\\\") as f:\\n            for byte_block in iter(lambda: f.read(4096), b\\\"\\\"):\\n                sha256_hash.update(byte_block)\\n        return sha256_hash.hexdigest()\\n    except Exception as e:\\n        logger.error(f\\\"Hash generation failed for {file_path}: {e}\\\")\\n        return \\\"error_hash\\\"\\n\\ndef _parse_pdf_date(date_str: Optional[str]) -> Optional[str]:\\n    \\\"\\\"\\\"Converts PDF-style date strings into ISO format.\\\"\\\"\\\"\\n    if not date_str or not isinstance(date_str, str):\\n        return None\\n    \\n    clean_date = re.sub(r'[^0-9]', '', date_str)\\n    try:\\n        if len(clean_date) >= 8:\\n            return f\\\"{clean_date[0:4]}-{clean_date[4:6]}-{clean_date[6:8]}\\\"\\n    except Exception:\\n        pass\\n    return date_str\\n\\ndef _sanitize_string(text: Any) -> str:\\n    if not text or not isinstance(text, str):\\n        return \\\"Unknown\\\"\\n    clean_text = \\\"\\\".join(char for char in text if char.isprintable())\\n    return \\\" \\\".join(clean_text.split())\\n\\ndef extract_document_metadata(reader: PyPDF2.PdfReader, file_path: Path, content: str = \\\"\\\") -> Dict[str, Any]:\\n    \\\"\\\"\\\"Generates an enriched document profile.\\\"\\\"\\\"\\n    try:\\n        meta = reader.metadata\\n    except Exception:\\n        meta = None\\n\\n    word_count = len(content.split()) if content else 0\\n\\n    return {\\n        \\\"file_hash\\\": generate_file_hash(file_path),\\n        \\\"file_name\\\": file_path.name,\\n        \\\"internal_title\\\": _sanitize_string(meta.title if meta and meta.title else file_path.stem),\\n        \\\"author\\\": _sanitize_string(meta.author if meta and meta.author else \\\"Unknown Author\\\"),\\n        \\\"total_pages\\\": len(reader.pages),\\n        \\\"word_count\\\": word_count,\\n        \\\"creation_date\\\": _parse_pdf_date(meta.get('/CreationDate') if meta else None),\\n        \\\"ingested_at\\\": datetime.now(timezone.utc).isoformat(),\\n        \\\"version\\\": \\\"2.2\\\"\\n    }\",\n      \"size_bytes\": 2186,\n      \"labels\": {\n        \"ast_node_count\": 400,\n        \"function_count\": 4,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"2232312cac37c9b553b7e7eb97aad369\"\n      },\n      \"analysis\": null\n    },\n    {\n      \"path\": \"utils/ocr_service.py\",\n      \"content_block\": \"\\\"\\\"\\\"utils/ocr_service.py\\nfrom PIL import Image\\nimport pytesseract\\nimport logging\\nfrom pdf2image import convert_from_path\\nimport os\\nimport sys\\n\\nlogger = logging.getLogger(__name__)\\n\\n# --- CONFIGURATION ---\\n# 1. POPPLER PATH (For PDF -> Image conversion)\\n# Updated to match your specific installation:\\nPOPPLER_PATH = r\\\"C:\\\\Users\\\\jakem\\\\Documents\\\\poppler\\\\poppler-25.12.0\\\\Library\\\\bin\\\"\\n\\n# 2. TESSERACT PATH (For Image -> Text OCR)\\n# CRITICAL FOR WINDOWS: Point this to your tesseract.exe\\n# If you haven't installed it, download from: https://github.com/UB-Mannheim/tesseract/wiki\\npytesseract.pytesseract.tesseract_cmd = r\\\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\\\"\\n\\ndef _get_poppler_path():\\n    \\\"\\\"\\\"\\n    Attempts to locate poppler path or returns None to let system PATH handle it.\\n    \\\"\\\"\\\"\\n    if os.name == 'nt': # Only for Windows\\n        if os.path.exists(POPPLER_PATH):\\n            return POPPLER_PATH\\n        \\n        # Check if user put it in the project folder for ease of use\\n        local_poppler = os.path.join(os.getcwd(), 'poppler', 'bin')\\n        if os.path.exists(local_poppler):\\n            return local_poppler\\n            \\n    return None # Default to system PATH\\n\\ndef extract_text_from_image(image_path_or_object) -> str:\\n    \\\"\\\"\\\"Extracts text from an image using pytesseract.\\\"\\\"\\\"\\n    try:\\n        if isinstance(image_path_or_object, str):\\n            img = Image.open(image_path_or_object)\\n        else:\\n            img = image_path_or_object\\n        return pytesseract.image_to_string(img)\\n    except Exception as e:\\n        # Check for common Tesseract \\\"not found\\\" errors\\n        if \\\"tesseract is not installed\\\" in str(e).lower() or \\\"not in your path\\\" in str(e).lower():\\n             logger.error(\\\"Tesseract not found! Please install it and check the path in utils/ocr_service.py\\\")\\n        else:\\n            logger.error(f\\\"Error during OCR text extraction: {e}\\\")\\n        return \\\"\\\"\\n\\ndef convert_page_to_image(pdf_path, page_number):\\n    \\\"\\\"\\\"Converts a specific page of a PDF into a PIL Image object using pdf2image.\\\"\\\"\\\"\\n    try:\\n        poppler_path = _get_poppler_path()\\n        \\n        # pdf2image uses 1-based indexing for first_page/last_page\\n        images = convert_from_path(\\n            pdf_path, \\n            first_page=page_number, \\n            last_page=page_number,\\n            poppler_path=poppler_path # Explicitly pass the path\\n        )\\n        if images:\\n            return images[0]\\n        return None\\n    except Exception as e:\\n        if \\\"poppler\\\" in str(e).lower():\\n            logger.error(f\\\"Poppler not found. Please update POPPLER_PATH in utils/ocr_service.py. Error: {e}\\\")\\n        else:\\n            logger.error(f\\\"Error converting PDF page {page_number} to image: {e}\\\")\\n        return None\\n\\\"\\\"\\\"\",\n      \"raw_content\": \"from PIL import Image\\nimport pytesseract\\nimport logging\\nfrom pdf2image import convert_from_path\\nimport os\\nimport sys\\n\\nlogger = logging.getLogger(__name__)\\n\\n# --- CONFIGURATION ---\\n# 1. POPPLER PATH (For PDF -> Image conversion)\\n# Updated to match your specific installation:\\nPOPPLER_PATH = r\\\"C:\\\\Users\\\\jakem\\\\Documents\\\\poppler\\\\poppler-25.12.0\\\\Library\\\\bin\\\"\\n\\n# 2. TESSERACT PATH (For Image -> Text OCR)\\n# CRITICAL FOR WINDOWS: Point this to your tesseract.exe\\n# If you haven't installed it, download from: https://github.com/UB-Mannheim/tesseract/wiki\\npytesseract.pytesseract.tesseract_cmd = r\\\"C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe\\\"\\n\\ndef _get_poppler_path():\\n    \\\"\\\"\\\"\\n    Attempts to locate poppler path or returns None to let system PATH handle it.\\n    \\\"\\\"\\\"\\n    if os.name == 'nt': # Only for Windows\\n        if os.path.exists(POPPLER_PATH):\\n            return POPPLER_PATH\\n        \\n        # Check if user put it in the project folder for ease of use\\n        local_poppler = os.path.join(os.getcwd(), 'poppler', 'bin')\\n        if os.path.exists(local_poppler):\\n            return local_poppler\\n            \\n    return None # Default to system PATH\\n\\ndef extract_text_from_image(image_path_or_object) -> str:\\n    \\\"\\\"\\\"Extracts text from an image using pytesseract.\\\"\\\"\\\"\\n    try:\\n        if isinstance(image_path_or_object, str):\\n            img = Image.open(image_path_or_object)\\n        else:\\n            img = image_path_or_object\\n        return pytesseract.image_to_string(img)\\n    except Exception as e:\\n        # Check for common Tesseract \\\"not found\\\" errors\\n        if \\\"tesseract is not installed\\\" in str(e).lower() or \\\"not in your path\\\" in str(e).lower():\\n             logger.error(\\\"Tesseract not found! Please install it and check the path in utils/ocr_service.py\\\")\\n        else:\\n            logger.error(f\\\"Error during OCR text extraction: {e}\\\")\\n        return \\\"\\\"\\n\\ndef convert_page_to_image(pdf_path, page_number):\\n    \\\"\\\"\\\"Converts a specific page of a PDF into a PIL Image object using pdf2image.\\\"\\\"\\\"\\n    try:\\n        poppler_path = _get_poppler_path()\\n        \\n        # pdf2image uses 1-based indexing for first_page/last_page\\n        images = convert_from_path(\\n            pdf_path, \\n            first_page=page_number, \\n            last_page=page_number,\\n            poppler_path=poppler_path # Explicitly pass the path\\n        )\\n        if images:\\n            return images[0]\\n        return None\\n    except Exception as e:\\n        if \\\"poppler\\\" in str(e).lower():\\n            logger.error(f\\\"Poppler not found. Please update POPPLER_PATH in utils/ocr_service.py. Error: {e}\\\")\\n        else:\\n            logger.error(f\\\"Error converting PDF page {page_number} to image: {e}\\\")\\n        return None\",\n      \"size_bytes\": 2730,\n      \"labels\": {\n        \"ast_node_count\": 263,\n        \"function_count\": 3,\n        \"class_count\": 0,\n        \"file_type\": \"code\",\n        \"file_extension\": \".py\",\n        \"path_hash\": \"b2e812c5072ce3f066127d36d9bc51f8\"\n      },\n      \"analysis\": null\n    }\n  ]\n}",
      "size_bytes": 68495,
      "labels": {
        "file_type": "config",
        "file_extension": ".json",
        "path_hash": "01ce443baeed27cf7271f76e24aa78c5"
      },
      "analysis": null
    },
    {
      "path": "config/settings.py",
      "content_block": "\"\"\"config/settings.py\nimport os\nimport logging\nfrom pathlib import Path\nfrom typing import Final, Optional\nfrom dotenv import load_dotenv\n\n# Load environmental variables\nload_dotenv()\n\n# Global Logging Configuration\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler('aletheia_system.log'),\n        logging.StreamHandler()\n    ]\n)\n\nclass Settings:\n    \"\"\"\n    Centralized configuration engine for Aletheia RAG Infrastructure.\n    \"\"\"\n    # Section 1: Directory Management\n    # Resolves to the parent of 'config', which is the root 'Ingest_pipeline_V2'\n    BASE_DIR: Final[Path] = Path(__file__).resolve().parent.parent\n    \n    DATA_DIR: Final[Path] = BASE_DIR / \"data\"\n    RAW_LANDING_DIR: Final[Path] = DATA_DIR / \"raw_landing\"\n    PROCESSED_ARCHIVE_DIR: Final[Path] = DATA_DIR / \"processed_archive\"\n    BACKUP_DIR: Final[Path] = PROCESSED_ARCHIVE_DIR / \"backups\"\n    \n    # Section 2: Storage Paths\n    CHROMA_DB_PATH: Final[Path] = BASE_DIR / \"memory\" / \"chroma_db\"\n    EMBEDDING_CACHE_DIR: Final[Path] = BASE_DIR / \"memory\" / \".embedding_cache\"\n    USAGE_LOG_PATH: Final[Path] = BASE_DIR / \"logs\" / \"usage_stats.json\"\n\n    # Section 3: Database (MongoDB)\n    MONGO_URI: Final[str] = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n    DB_NAME: Final[str] = \"aletheia_memory\"\n    COLLECTION_TRUTH: Final[str] = \"canonical_truth\"\n    COLLECTION_TRACES: Final[str] = \"reasoning_traces\"\n\n    # Section 4: Inference (LM Studio)\n    LM_STUDIO_BASE_URL: Final[str] = os.getenv(\"LM_STUDIO_URL\", \"http://localhost:1234/v1\")\n    EMBEDDING_MODEL: Final[str] = \"nomic-ai/nomic-embed-text-v1.5-GGUF\"\n    NOMIC_PREFIX: Final[str] = \"search_document: \" \n\n    # Section 5: RAG & OCR Logic\n    CHUNK_SIZE: Final[int] = 1500 \n    CHUNK_OVERLAP: Final[int] = 200\n    OCR_TEXT_DENSITY_THRESHOLD: int = 50 # Characters per page below which OCR is triggered\n    NUM_RETRIEVAL_RESULTS: int = 5\n\n    def validate_settings(self):\n        \"\"\"Ensures directories exist and critical settings are present.\"\"\"\n        paths = [\n            self.DATA_DIR, self.RAW_LANDING_DIR, self.PROCESSED_ARCHIVE_DIR, \n            self.BACKUP_DIR, self.CHROMA_DB_PATH, self.USAGE_LOG_PATH.parent,\n            self.EMBEDDING_CACHE_DIR\n        ]\n        for p in paths:\n            p.mkdir(parents=True, exist_ok=True)\n        \n        if not self.MONGO_URI:\n            raise ValueError(\"MONGO_URI environment variable is missing.\")\n\nsettings = Settings()\nsettings.validate_settings()\n\"\"\"",
      "size_bytes": 2558,
      "labels": {
        "ast_node_count": 367,
        "function_count": 1,
        "class_count": 1,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "1323dcc6d85cb5bceac7402cff7ddfa6"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 367,
        "function_count": 1,
        "class_count": 1,
        "imports": [
          "dotenv",
          "logging",
          "os",
          "pathlib",
          "typing"
        ],
        "dangerous_calls": [],
        "io_functions": [],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "core/codebase_processor.py",
      "content_block": "\"\"\"core/codebase_processor.py\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nfrom config.settings import settings\n\nlogger = logging.getLogger(__name__)\n\nclass CodebaseProcessor:\n    \"\"\"\n    Handles processing of text-based files (Python, JSON, Markdown, etc.).\n    \"\"\"\n    def __init__(self):\n        self.settings = settings\n\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"Reads text/code files directly and chunks them.\"\"\"\n        documents = []\n        try:\n            # Use errors='ignore' to prevent crashing on non-UTF-8 binary artifacts\n            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n                raw_text = f.read()\n            \n            if raw_text.strip():\n                return self._chunk_text(raw_text, str(file_path), file_path.name)\n        except Exception as e:\n            logger.error(f\"Error processing text file {file_path.name}: {e}\")\n        return documents\n\n    def _chunk_text(self, text: str, file_path: str, file_name: str) -> List[Dict[str, Any]]:\n        \"\"\"Splits text into sliding window chunks.\"\"\"\n        chunk_size = self.settings.CHUNK_SIZE\n        overlap = self.settings.CHUNK_OVERLAP\n        chunks = []\n        \n        text_len = len(text)\n        start = 0\n        chunk_idx = 0\n        \n        while start < text_len:\n            end = min(start + chunk_size, text_len)\n            chunk_content = text[start:end]\n            \n            chunks.append({\n                \"content\": chunk_content,\n                \"metadata\": {\n                    \"file_path\": file_path,\n                    \"file_name\": file_name,\n                    \"page_number\": 0, # Not applicable for flat text files\n                    \"chunk_index\": chunk_idx,\n                    \"file_type\": \"codebase\"\n                }\n            })\n            \n            start += (chunk_size - overlap)\n            chunk_idx += 1\n            \n        return chunks\n\"\"\"",
      "size_bytes": 1980,
      "labels": {
        "ast_node_count": 273,
        "function_count": 3,
        "class_count": 1,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "78287a2d7d09009729b2d1c6e333de43"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 273,
        "function_count": 3,
        "class_count": 1,
        "imports": [
          "config",
          "logging",
          "pathlib",
          "typing"
        ],
        "dangerous_calls": [],
        "io_functions": [
          "open"
        ],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "core/ingest_manager.py",
      "content_block": "\"\"\"core/ingest_manager.py\nimport logging\nfrom pathlib import Path\nfrom pymongo import MongoClient\nfrom pymongo.errors import BulkWriteError\nimport chromadb\nfrom datetime import datetime\nfrom config.settings import settings\n# FIX: Consistent imports\nfrom core.pdf_processor import PDFProcessor\nfrom core.codebase_processor import CodebaseProcessor  # Matches lowercase filename\nfrom utils.embedding_client import EmbeddingClient\n\nlogger = logging.getLogger(__name__)\n\nclass IngestManager:\n    \"\"\"\n    Manages the complete ingestion pipeline for PDF and Text/Code documents.\n    \"\"\"\n    def __init__(self):\n        # Initialize Databases\n        self.mongo_client = MongoClient(settings.MONGO_URI)\n        self.db = self.mongo_client[settings.DB_NAME]\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\n        \n        # Initialize ChromaDB\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\"aletheia_index\")\n        \n        # Initialize Core Engines\n        self.pdf_processor = PDFProcessor()\n        self.codebase_processor = CodebaseProcessor()\n        self.embedder = EmbeddingClient()\n        \n    def process_file(self, file_path: Path) -> bool:\n        \"\"\"\n        Processes a single file through the ingestion pipeline.\n        Routes to the appropriate processor based on file type.\n        \"\"\"\n        try:\n            logger.info(f\"Processing: {file_path.name}\")\n            \n            # 1. Select Processor Strategy\n            if file_path.suffix.lower() == '.pdf':\n                chunks = list(self.pdf_processor.process_file(file_path))\n            else:\n                # Fallback to codebase processor for .py, .txt, .md, .json, etc.\n                chunks = list(self.codebase_processor.process_file(file_path))\n            \n            if not chunks:\n                logger.warning(f\"No usable content found in {file_path.name}\")\n                return False\n            \n            # 2. Vectorization and Persistence\n            chroma_ids = []\n            chroma_embeddings = []\n            chroma_metadatas = []\n            mongo_docs = []\n            \n            for i, chunk in enumerate(chunks):\n                content_text = chunk[\"content\"]\n                chunk_meta = chunk[\"metadata\"]\n                \n                # Generate unique ID\n                file_hash = chunk_meta.get('file_name', file_path.name)\n                doc_id = f\"{file_hash}_{i}\"\n                \n                # Get Embedding\n                vector = self.embedder.get_embedding(content_text)\n                if not vector:\n                    continue\n                \n                # Prepare Mongo Document\n                mongo_docs.append({\n                    \"file_hash\": file_hash,\n                    \"chunk_index\": i,\n                    \"content\": content_text,\n                    \"metadata\": chunk_meta,\n                    \"ingested_at\": datetime.utcnow().isoformat()\n                })\n\n                # Prepare Chroma Data\n                chroma_ids.append(doc_id)\n                chroma_embeddings.append(vector)\n                chroma_metadatas.append({\n                    \"file_hash\": file_hash,\n                    \"chunk_index\": i,\n                    \"page\": chunk_meta.get('page_number', 0),\n                    \"file_name\": chunk_meta.get('file_name', 'unknown')\n                })\n\n            # Bulk Write to Mongo (Robust Duplicate Handling)\n            if mongo_docs:\n                try:\n                    # ordered=False continues processing even if one insert fails (e.g. duplicate)\n                    self.collection_truth.insert_many(mongo_docs, ordered=False)\n                except BulkWriteError as bwe:\n                    # Log duplicates as info, actual errors as warning\n                    duplicates = [e for e in bwe.details['writeErrors'] if e['code'] == 11000]\n                    if len(duplicates) == len(mongo_docs):\n                        logger.info(f\"Skipping {file_path.name}: All chunks already exist in DB.\")\n                        return True\n                    elif duplicates:\n                        logger.info(f\"Partial insert for {file_path.name}: {len(duplicates)} duplicates skipped.\")\n                    else:\n                        # Sanitize error message to prevent UnicodeEncodeError in Windows consoles\n                        error_msg = str(bwe).encode('ascii', 'replace').decode('ascii')\n                        logger.warning(f\"MongoDB Bulk Write Error: {error_msg}\")\n\n            # Bulk Write to Chroma\n            if chroma_ids:\n                try:\n                    self.collection_index.add(\n                        ids=chroma_ids,\n                        embeddings=chroma_embeddings,\n                        metadatas=chroma_metadatas,\n                        documents=[d['content'] for d in mongo_docs]\n                    )\n                except Exception as e:\n                    # Chroma might error on duplicates, but usually updates/upserts.\n                    # If it fails, log and continue.\n                    logger.warning(f\"ChromaDB Write Warning for {file_path.name}: {e}\")\n                    \n            logger.info(f\"Successfully processed: {file_path.name}\")\n            return True\n            \n        except Exception as e:\n            # Catch-all to ensure one bad file doesn't crash the whole batch\n            # Sanitize error message to prevent UnicodeEncodeError\n            safe_error = str(e).encode('ascii', 'replace').decode('ascii')\n            logger.error(f\"Error processing file {file_path.name}: {safe_error}\")\n            return False\n    \n    def process_all(self):\n        \"\"\"Processes all supported files in the raw landing directory recursively.\"\"\"\n        extensions = [\"*.pdf\", \"*.txt\", \"*.py\", \"*.md\", \"*.json\", \"*.sh\", \"*.ps1\"]\n        all_files = []\n        \n        for ext in extensions:\n            all_files.extend(list(settings.RAW_LANDING_DIR.rglob(ext)))\n            \n        if not all_files:\n            logger.info(f\"No supported files found in {settings.RAW_LANDING_DIR}\")\n            return\n            \n        logger.info(f\"Starting ingestion of {len(all_files)} files.\")\n        processed_count = sum(1 for f in all_files if self.process_file(f))\n        logger.info(f\"Ingestion completed. Processed {processed_count}/{len(all_files)}.\")\n\"\"\"",
      "size_bytes": 6451,
      "labels": {
        "ast_node_count": 732,
        "function_count": 3,
        "class_count": 1,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "c569701e1f7964941f7deeebb9ea7283"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 732,
        "function_count": 3,
        "class_count": 1,
        "imports": [
          "chromadb",
          "config",
          "core",
          "datetime",
          "logging",
          "pathlib",
          "pymongo",
          "utils"
        ],
        "dangerous_calls": [],
        "io_functions": [],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "core/pdf_processor.py",
      "content_block": "\"\"\"core/pdf_processor.py\nimport fitz # PyMuPDF\nimport logging\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nfrom config.settings import settings\nfrom utils import ocr_service\n\nlogger = logging.getLogger(__name__)\n\nclass PDFProcessor:\n    \"\"\"\n    Specialized processor for PDF documents with OCR capabilities.\n    \"\"\"\n    def __init__(self):\n        self.settings = settings\n\n    def process_file(self, file_path: Path) -> List[Dict[str, Any]]:\n        \"\"\"\n        Extracts text from PDF page-by-page, applying OCR if text density is low.\n        \"\"\"\n        documents = []\n        try:\n            doc = fitz.open(file_path)\n            for page_num, page in enumerate(doc):\n                raw_text = page.get_text()\n\n                # Decision Gate: Check for Scanned Pages\n                if len(raw_text.strip()) < self.settings.OCR_TEXT_DENSITY_THRESHOLD:\n                    logger.warning(f\"Low text density on page {page_num + 1} of {file_path.name}. Checking OCR...\")\n                    try:\n                        image = ocr_service.convert_page_to_image(str(file_path), page_num + 1)\n                        if image:\n                            ocr_text = ocr_service.extract_text_from_image(image)\n                            # Only use OCR if it yielded more info than the raw extraction\n                            if len(ocr_text.strip()) > len(raw_text.strip()):\n                                raw_text = ocr_text\n                                logger.info(f\"OCR improved text yield for page {page_num + 1}.\")\n                    except Exception as ocr_e:\n                        logger.error(f\"OCR failed for page {page_num + 1}: {ocr_e}\")\n\n                # Chunking\n                if raw_text.strip():\n                    page_docs = self._chunk_text(raw_text, str(file_path), file_path.name, page_num + 1)\n                    documents.extend(page_docs)\n            \n            doc.close()\n        except Exception as e:\n            logger.error(f\"Error processing PDF {file_path}: {e}\")\n            \n        return documents\n\n    def _chunk_text(self, text: str, file_path: str, file_name: str, page_num: int) -> List[Dict[str, Any]]:\n        \"\"\"Helper to split text into chunks.\"\"\"\n        chunk_size = self.settings.CHUNK_SIZE\n        overlap = self.settings.CHUNK_OVERLAP\n        chunks = []\n        \n        text_len = len(text)\n        start = 0\n        chunk_idx = 0\n        \n        while start < text_len:\n            end = min(start + chunk_size, text_len)\n            chunk_content = text[start:end]\n            \n            chunks.append({\n                \"content\": chunk_content,\n                \"metadata\": {\n                    \"file_path\": file_path,\n                    \"file_name\": file_name,\n                    \"page_number\": page_num,\n                    \"chunk_index\": chunk_idx,\n                    \"file_type\": \"pdf\"\n                }\n            })\n            \n            start += (chunk_size - overlap)\n            chunk_idx += 1\n            \n        return chunks\n\"\"\"",
      "size_bytes": 3040,
      "labels": {
        "ast_node_count": 438,
        "function_count": 3,
        "class_count": 1,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "9197055079c29b5f00fe2c764f13fe9a"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 438,
        "function_count": 3,
        "class_count": 1,
        "imports": [
          "config",
          "fitz",
          "logging",
          "pathlib",
          "typing",
          "utils"
        ],
        "dangerous_calls": [],
        "io_functions": [],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "core/retrieval_controller.py",
      "content_block": "\"\"\"core/retrieval_controller.py\nimport logging\nimport chromadb\nfrom pymongo import MongoClient\nfrom config.settings import settings\nfrom utils.embedding_client import EmbeddingClient\n\nlogger = logging.getLogger(__name__)\n\nclass RetrievalController:\n    def __init__(self):\n        self.embedding_client = EmbeddingClient()\n        \n        # ChromaDB (Index)\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\"aletheia_index\")\n        \n        # MongoDB (Canonical Truth)\n        self.mongo_client = MongoClient(settings.MONGO_URI)\n        self.db = self.mongo_client[settings.DB_NAME]\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\n\n    def query(self, query: str) -> str:\n        \"\"\"Retrieves context and generates a response.\"\"\"\n        # 1. Embed Query\n        query_embedding = self.embedding_client.get_embedding(query)\n        if not query_embedding:\n            return \"Error: Could not process query.\"\n\n        # 2. Retrieve from ChromaDB\n        results = self.collection_index.query(\n            query_embeddings=[query_embedding],\n            n_results=settings.NUM_RETRIEVAL_RESULTS,\n            include=['metadatas']\n        )\n\n        # 3. Fetch Full Content from MongoDB (Canonical Truth)\n        # We rely on the index to find *where* the data is, but fetch the *clean* data from Mongo.\n        context_docs = []\n        if results and results['metadatas'] and results['metadatas'][0]:\n            for meta in results['metadatas'][0]:\n                file_hash = meta.get('file_hash')\n                chunk_index = meta.get('chunk_index')\n                \n                record = self.collection_truth.find_one({\n                    \"file_hash\": file_hash, \n                    \"chunk_index\": chunk_index\n                })\n                \n                if record:\n                    context_docs.append(record['content'])\n        \n        if not context_docs:\n            return \"No relevant information found in the archives.\"\n\n        # 4. Construct Prompt\n        context_text = \"\\n\\n---\\n\\n\".join(context_docs)\n        return f\"Based on the following research:\\n\\n{context_text}\\n\\nAnswer: {query}\"\n\"\"\"",
      "size_bytes": 2270,
      "labels": {
        "ast_node_count": 269,
        "function_count": 2,
        "class_count": 1,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "f29ac7b2821ef048e5b25928f8f2380d"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 269,
        "function_count": 2,
        "class_count": 1,
        "imports": [
          "chromadb",
          "config",
          "logging",
          "pymongo",
          "utils"
        ],
        "dangerous_calls": [],
        "io_functions": [],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "settings/init.py",
      "content_block": "\"\"\"settings/init.py\nimport pymongo\nimport sys\nfrom pathlib import Path\n\n# Fix path to ensure imports work from top-level directory\nsys.path.append(str(Path(__file__).resolve().parents[1]))\n\nfrom config.settings import settings\n\ndef init():\n    try:\n        client = pymongo.MongoClient(settings.MONGO_URI)\n        db = client[settings.DB_NAME]\n        \n        colls = [settings.COLLECTION_TRUTH, settings.COLLECTION_TRACES]\n        for c in colls:\n            if c not in db.list_collection_names():\n                db.create_collection(c)\n                print(f\"Provisioned: {c}\")\n                \n        # Create unique index on file_hash and chunk_index pair for granular retrieval\n        db[settings.COLLECTION_TRUTH].create_index(\n            [(\"file_hash\", pymongo.ASCENDING), (\"chunk_index\", pymongo.ASCENDING)], \n            unique=True\n        )\n        print(\"Aletheia Memory initialized successfully.\")\n        \n    except Exception as e:\n        print(f\"Initialization failed: {e}\")\n\nif __name__ == \"__main__\":\n    init()\n\"\"\"",
      "size_bytes": 1041,
      "labels": {
        "ast_node_count": 161,
        "function_count": 1,
        "class_count": 0,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "c08000706176c215599fd3275903e9ef"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 161,
        "function_count": 1,
        "class_count": 0,
        "imports": [
          "config",
          "pathlib",
          "pymongo",
          "sys"
        ],
        "dangerous_calls": [],
        "io_functions": [
          "print"
        ],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "utils/embedding_client.py",
      "content_block": "\"\"\"utils/embedding_client.py\nimport requests\nimport logging\nimport time\nfrom typing import List, Optional\nfrom functools import lru_cache\nfrom config.settings import settings\n\nlogger = logging.getLogger(__name__)\n\nclass EmbeddingClient:\n    \"\"\"\n    Interface for local LM Studio embeddings with caching and resource awareness.\n    \"\"\"\n    def __init__(self):\n        self.base_url = f\"{settings.LM_STUDIO_BASE_URL}/embeddings\"\n        self.last_activity = time.time()\n\n    def _check_resource_status(self):\n        \"\"\"\n        Placeholder for checking system health or triggering model unloads.\n        Could be extended to use LM Studio's /v1/models endpoint to check TTL.\n        \"\"\"\n        self.last_activity = time.time()\n        # In a JIT strategy, we could ping a custom management script here\n        pass\n\n    @lru_cache(maxsize=2048) # Increased cache size for better performance\n    def get_embedding(self, text: str) -> Optional[List[float]]:\n        \"\"\"\n        Generates a vector with LRU caching.\n        Note: Nomic models require the 'search_document: ' prefix.\n        \"\"\"\n        self._check_resource_status()\n        \n        prefixed_text = f\"{settings.NOMIC_PREFIX}{text}\"\n        payload = {\"input\": prefixed_text, \"model\": settings.EMBEDDING_MODEL}\n        \n        # Implement internal retry logic\n        for attempt in range(3):\n            try:\n                response = requests.post(self.base_url, json=payload, timeout=30)\n                response.raise_for_status()\n                return response.json()[\"data\"][0][\"embedding\"]\n            except Exception as e:\n                wait = (attempt + 1) * 2\n                logger.warning(f\"Embedding failed (Attempt {attempt+1}): {e}. Retrying in {wait}s...\")\n                time.sleep(wait)\n        \n        logger.error(f\"Failed to retrieve embedding after retries for text snippet.\")\n        return None\n\n    def clear_cache(self):\n        \"\"\"Clears the embedding cache.\"\"\"\n        self.get_embedding.cache_clear()\n\"\"\"",
      "size_bytes": 2004,
      "labels": {
        "ast_node_count": 235,
        "function_count": 4,
        "class_count": 1,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "838a4572ee81d2fba1369be708ac5bc1"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 235,
        "function_count": 4,
        "class_count": 1,
        "imports": [
          "config",
          "functools",
          "logging",
          "requests",
          "time",
          "typing"
        ],
        "dangerous_calls": [],
        "io_functions": [],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "utils/metadata_extractor.py",
      "content_block": "\"\"\"utils/metadata_extractor.py\nimport hashlib\nimport logging\nimport re\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom datetime import datetime, timezone\nimport PyPDF2\n\nlogger = logging.getLogger(__name__)\n\ndef generate_file_hash(file_path: Path) -> str:\n    \"\"\"Generates a SHA-256 hash of the file.\"\"\"\n    sha256_hash = hashlib.sha256()\n    try:\n        with open(file_path, \"rb\") as f:\n            for byte_block in iter(lambda: f.read(4096), b\"\"):\n                sha256_hash.update(byte_block)\n        return sha256_hash.hexdigest()\n    except Exception as e:\n        logger.error(f\"Hash generation failed for {file_path}: {e}\")\n        return \"error_hash\"\n\ndef _parse_pdf_date(date_str: Optional[str]) -> Optional[str]:\n    \"\"\"Converts PDF-style date strings into ISO format.\"\"\"\n    if not date_str or not isinstance(date_str, str):\n        return None\n    \n    clean_date = re.sub(r'[^0-9]', '', date_str)\n    try:\n        if len(clean_date) >= 8:\n            return f\"{clean_date[0:4]}-{clean_date[4:6]}-{clean_date[6:8]}\"\n    except Exception:\n        pass\n    return date_str\n\ndef _sanitize_string(text: Any) -> str:\n    if not text or not isinstance(text, str):\n        return \"Unknown\"\n    clean_text = \"\".join(char for char in text if char.isprintable())\n    return \" \".join(clean_text.split())\n\ndef extract_document_metadata(reader: PyPDF2.PdfReader, file_path: Path, content: str = \"\") -> Dict[str, Any]:\n    \"\"\"Generates an enriched document profile.\"\"\"\n    try:\n        meta = reader.metadata\n    except Exception:\n        meta = None\n\n    word_count = len(content.split()) if content else 0\n\n    return {\n        \"file_hash\": generate_file_hash(file_path),\n        \"file_name\": file_path.name,\n        \"internal_title\": _sanitize_string(meta.title if meta and meta.title else file_path.stem),\n        \"author\": _sanitize_string(meta.author if meta and meta.author else \"Unknown Author\"),\n        \"total_pages\": len(reader.pages),\n        \"word_count\": word_count,\n        \"creation_date\": _parse_pdf_date(meta.get('/CreationDate') if meta else None),\n        \"ingested_at\": datetime.now(timezone.utc).isoformat(),\n        \"version\": \"2.2\"\n    }\n\"\"\"",
      "size_bytes": 2186,
      "labels": {
        "ast_node_count": 400,
        "function_count": 4,
        "class_count": 0,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "2232312cac37c9b553b7e7eb97aad369"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 400,
        "function_count": 4,
        "class_count": 0,
        "imports": [
          "PyPDF2",
          "datetime",
          "hashlib",
          "logging",
          "pathlib",
          "re",
          "typing"
        ],
        "dangerous_calls": [],
        "io_functions": [
          "open"
        ],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    },
    {
      "path": "utils/ocr_service.py",
      "content_block": "\"\"\"utils/ocr_service.py\nfrom PIL import Image\nimport pytesseract\nimport logging\nfrom pdf2image import convert_from_path\nimport os\nimport sys\n\nlogger = logging.getLogger(__name__)\n\n# --- CONFIGURATION ---\n# 1. POPPLER PATH (For PDF -> Image conversion)\n# Updated to match your specific installation:\nPOPPLER_PATH = r\"C:\\Users\\jakem\\Documents\\poppler\\poppler-25.12.0\\Library\\bin\"\n\n# 2. TESSERACT PATH (For Image -> Text OCR)\n# CRITICAL FOR WINDOWS: Point this to your tesseract.exe\n# If you haven't installed it, download from: https://github.com/UB-Mannheim/tesseract/wiki\npytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n\ndef _get_poppler_path():\n    \"\"\"\n    Attempts to locate poppler path or returns None to let system PATH handle it.\n    \"\"\"\n    if os.name == 'nt': # Only for Windows\n        if os.path.exists(POPPLER_PATH):\n            return POPPLER_PATH\n        \n        # Check if user put it in the project folder for ease of use\n        local_poppler = os.path.join(os.getcwd(), 'poppler', 'bin')\n        if os.path.exists(local_poppler):\n            return local_poppler\n            \n    return None # Default to system PATH\n\ndef extract_text_from_image(image_path_or_object) -> str:\n    \"\"\"Extracts text from an image using pytesseract.\"\"\"\n    try:\n        if isinstance(image_path_or_object, str):\n            img = Image.open(image_path_or_object)\n        else:\n            img = image_path_or_object\n        return pytesseract.image_to_string(img)\n    except Exception as e:\n        # Check for common Tesseract \"not found\" errors\n        if \"tesseract is not installed\" in str(e).lower() or \"not in your path\" in str(e).lower():\n             logger.error(\"Tesseract not found! Please install it and check the path in utils/ocr_service.py\")\n        else:\n            logger.error(f\"Error during OCR text extraction: {e}\")\n        return \"\"\n\ndef convert_page_to_image(pdf_path, page_number):\n    \"\"\"Converts a specific page of a PDF into a PIL Image object using pdf2image.\"\"\"\n    try:\n        poppler_path = _get_poppler_path()\n        \n        # pdf2image uses 1-based indexing for first_page/last_page\n        images = convert_from_path(\n            pdf_path, \n            first_page=page_number, \n            last_page=page_number,\n            poppler_path=poppler_path # Explicitly pass the path\n        )\n        if images:\n            return images[0]\n        return None\n    except Exception as e:\n        if \"poppler\" in str(e).lower():\n            logger.error(f\"Poppler not found. Please update POPPLER_PATH in utils/ocr_service.py. Error: {e}\")\n        else:\n            logger.error(f\"Error converting PDF page {page_number} to image: {e}\")\n        return None\n\"\"\"",
      "size_bytes": 2730,
      "labels": {
        "ast_node_count": 263,
        "function_count": 3,
        "class_count": 0,
        "file_type": "code",
        "file_extension": ".py",
        "path_hash": "b2e812c5072ce3f066127d36d9bc51f8"
      },
      "analysis": {
        "syntax_ok": true,
        "node_count": 263,
        "function_count": 3,
        "class_count": 0,
        "imports": [
          "PIL",
          "logging",
          "os",
          "pdf2image",
          "pytesseract",
          "sys"
        ],
        "dangerous_calls": [],
        "io_functions": [],
        "has_async": false,
        "error": "module 'ast' has no attribute 'Decorator'"
      }
    }
  ]
}