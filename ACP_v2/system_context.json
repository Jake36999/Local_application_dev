{
  "status": "initialized",
  "system_context": "You are Aletheia, the internal system architect. Below is the current state of your source code and configuration. Use this context to answer technical questions about the platform structure, identify bugs, and suggest architectural improvements.\n\n--- ALETHEIA SYSTEM SNAPSHOT ---\nProject Root: ACP_v2\nACP_v2/\n    .mypy.ini\n    ACP_v2.code-workspace\n    aletheia_mcp.py\n    backend_startup.py\n    fix_config.py\n    lm_studio_config_export.json\n    migration_import_map.json\n    orchestrator.py\n    orchestrator.status.json\n    package.json\n    README.md\n    setup_config.py\n    system_context.json\n    system_manifest.py\n    __init__.py\n    .venv/\n        .gitignore\n        pyvenv.cfg\n        etc/\n            jupyter/\n                nbconfig/\n                    notebook.d/\n                        pydeck.json\n        Include/\n        Lib/\n        Scripts/\n            activate\n            activate.bat\n            Activate.ps1\n            deactivate.bat\n            dotenv.exe\n            f2py.exe\n            fastapi.exe\n            jsonschema.exe\n            normalizer.exe\n            numpy-config.exe\n            pip.exe\n            pip3.11.exe\n            pip3.exe\n            python.exe\n            pythonw.exe\n            pythonw_d.exe\n            python_d.exe\n            sqlite-utils.exe\n            streamlit.exe\n            tabulate.exe\n            uvicorn.exe\n            watchfiles.exe\n            watchmedo.exe\n            websockets.exe\n        share/\n            jupyter/\n                nbextensions/\n                    pydeck/\n                        extensionRequires.js\n                        index.js\n                        index.js.map\n    ACP_v2_bundle_20260206_213825/\n        ACP_v2_bundle_20260206_213825.txt\n        ACP_v2_bundle_20260206_213825.yaml\n    ACP_v2_bundle_20260206_232817/\n        ACP_v2_bundle_20260206_232817.txt\n        ACP_v2_bundle_20260206_232817.yaml\n    ACP_v2_bundle_20260207_011453/\n        ACP_v2_bundle_20260207_011453.yaml\n    ACP_v2_bundle_20260207_135944/\n        ACP_v2_bundle_20260207_135944.yaml\n    ACP_v2_bundle_20260208_001455/\n        ACP_v2_bundle_20260208_001455.yaml\n    config/\n        .keep\n        intent_specs.yaml\n        lms_config.json\n        methodology_library.json\n        orchestrator_config.json\n        py.typed\n        settings.py\n        settings.pyi\n        tsconfig.json\n        tsconfig.node.json\n        __init__.py\n    core/\n        bootstrap.py\n        py.typed\n        schemas.py\n        sentinel.py\n        brain/\n            stack_creator.py\n            workflow_analyzer.py\n            __init__.py\n        bridge/\n            context_manager.py\n            __init__.py\n        bus/\n            message_bus.py\n            settings_db.py\n            __init__.py\n        canon/\n            canonical_code_platform_port/\n                api.py\n                finish_cleanup.py\n                governance_report.json\n                ui_app_backup.py\n                workflow_ingest_enhanced.py\n                _tmp_replace.py\n                __init__.py\n                analysis/\n                    call_graph_normalizer.py\n                    cut_analysis.py\n                    drift_detector.py\n                    governance_report.py\n                    rule_engine.py\n                    symbol_resolver.py\n                    __init__.py\n                bus/\n                    message_bus.py\n                    settings_db.py\n                    __init__.py\n                orchestrator/\n                    rag_orchestrator.py\n                    __init__.py\n                staging/\n                    metadata.json\n                test_suite/\n                    tests.py\n                    __init__.py\n                tools/\n                    debug_rebuild.py\n                    show_status.py\n                    view_results.py\n                    __init__.py\n                    ci/\n                        run_system_tests.py\n                        __init__.py\n                    setup/\n                        init_rag.py\n                        __init__.py\n                ui/\n                    llm_workflow_ui.py\n                    __init__.py\n                    react-flow-app/\n                        index.html\n                        vite.config.ts\n                workflows/\n                    ingest_workflow.py\n                    workflow_builder.py\n                    workflow_polyglot.py\n                    workflow_verify.py\n                    __init__.py\n        control/\n            adaptive_protection.py\n            damping_controller.py\n            pmu_filter.py\n            stability_math.py\n            telemetry_norm.py\n            __init__.py\n        integration/\n            contract_registry.py\n            io_linker.py\n            __init__.py\n        rebuild/\n            semantic_rebuilder.py\n            semantic_rebuilder_duplicate.py\n            __init__.py\n    data/\n        provenance.py\n        __init__.py\n    discovered_assets/\n        governance_report.json\n    docs/\n        guides/\n            .keep\n            AI_ANALYSIS_VERIFICATION.md\n            AI_ANALYSIS_VERIFICATION_duplicate.md\n            ARCHITECTURE.md\n            BREAKTHROUGH_SUMMARY.md\n            BREAKTHROUGH_SUMMARY_duplicate.md\n            CLI_ARGUMENT_FIX.md\n            CLI_ARGUMENT_FIX_duplicate.md\n            ENHANCEMENT_SUMMARY.md\n            ENHANCEMENT_SUMMARY_duplicate.md\n            GETTING_STARTED.md\n            GETTING_STARTED_duplicate.md\n            intergration_guide.md\n            knowledge_bundle.md\n            knowledge_bundle_duplicate.md\n            LLM_ARCHITECTURE_DIAGRAM.md\n            LLM_COMPLETE_IMPLEMENTATION.md\n            LLM_IMPLEMENTATION_SUMMARY.md\n            LLM_QUICK_START.md\n            LLM_WORKFLOW_BUILDER_GUIDE.md\n            MIGRATION_GUIDE.md\n            QUICKSTART.md\n            QUICK_REFERENCE.md\n            RAG_GUIDE.md\n            README_LLM_WORKFLOW.md\n            read_me.md\n            read_me_duplicate.md\n            SCAN_ASSESSMENT.md\n            SCAN_ASSESSMENT_duplicate.md\n            SETUP_GUIDE.md\n            SETUP_GUIDE_duplicate.md\n            WEB_INTERFACE_README.md\n            WEB_INTERFACE_README_duplicate.md\n            WORKFLOWS.md\n    input_configs/\n    legacy/\n        .keep\n        CLEANUP_SUMMARY.md\n        codebase_processor.py\n        create_test_directives.py\n        DOCUMENTATION_CONSOLIDATION.md\n        ingest_manager.py\n        init.py\n        migrate_legacy.py\n        MIGRATION_GUIDE_PART6.md\n        MIGRATION_LOG.json\n        PART6_COMPLETION_SUMMARY.md\n        PART6_FINAL_OVERVIEW.md\n        PART6_INTEGRATION_CHECKLIST.md\n        pdf_processor.py\n        PHASE5_VERIFICATION.md\n        PHASE6_SUMMARY.md\n        PHASE7_COMPLETE.md\n        report_detailed_74bc588f.json\n        report_summary_74bc588f.json\n        report_summary_bce8782c.json\n        retrieval_controller.py\n        scan_DIRECTORY_MAP_74bc588f.json\n        scan_DIRECTORY_MAP_bce8782c.json\n        scan_part1_74bc588f.json\n        scan_part1_bce8782c.json\n        settings.py\n        SYSTEM_COMPLETE.md\n        VERIFICATION_PLAN.md\n        __init__.py\n    logs/\n        audit_logger.py\n        bootloader.log\n        __init__.py\n    memory/\n        components/\n            .keep\n        documents/\n            .keep\n        project/\n            .keep\n        schemas/\n            .keep\n        sql/\n            project_meta.db\n        task/\n            .keep\n    ops/\n        find_tesseract.py\n        init.py\n        requirements.txt\n        suggested_requirements.txt\n        __init__.py\n        audit/\n            discrepancy_check.py\n            __init__.py\n        bootloader/\n            boot_api.py\n            boot_main.py\n            dependency_manager.py\n            dep_manager.py\n            initializer.py\n            __init__.py\n        diagnostics/\n            ai_verification.md\n            scan_assessment.md\n        launchers/\n            boot.bat\n            deploy.ps1\n            deploy_lifecycle.sh\n            Directory_Bundler_Launcher.bat\n            Directory_Bundler_Launcher_duplicate.bat\n            Install_Dependencies.bat\n            Install_Dependencies_duplicate.bat\n            install_requirements.bat\n            migrate_to_acp.ps1\n            package_workspace.bat\n            reference_calls.ps1\n            reference_calls_duplicate.ps1\n            run_boot.bat\n            run_boot_duplicate.bat\n            Run_Directory_Bundler.bat\n            Run_Directory_Bundler_duplicate.bat\n            run_frontend.bat\n            Run_Tests.bat\n            Run_Tests_duplicate.bat\n            Scan_Project_Targets.bat\n            setup_frontend.bat\n            start.bat\n            start_backend.bat\n            start_orchestrator.bat\n            start_platform.bat\n            verify_lmstudio_contract.ps1\n            verify_lmstudio_contract_duplicate.ps1\n        safe_ops/\n            isolation_layer.py\n            __init__.py\n        scaling/\n            micro_data_center.py\n            tiga_commit.py\n            __init__.py\n    output/\n        manifest.py\n        __init__.py\n    provenance_reports/\n    reference/\n        .keep\n        legacy_directory_structure.md\n    services/\n        add_numbers/\n            api.py\n            deployment.yaml\n            interface.py\n            __init__.py\n        calculate/\n            api.py\n            deployment.yaml\n            interface.py\n            __init__.py\n        calculate_average/\n            api.py\n            deployment.yaml\n            interface.py\n            __init__.py\n        compute_sum/\n            api.py\n            deployment.yaml\n            interface.py\n            __init__.py\n        multiply/\n            api.py\n            deployment.yaml\n            interface.py\n            __init__.py\n        process_data/\n            api.py\n            deployment.yaml\n            interface.py\n            __init__.py\n    simulation_data/\n    staging/\n        failed/\n        incoming/\n            A Neural Scaling Law from.pdf\n            chinchilla's wild implications.pdf\n            Efficient Large-Scale Language Model Training on GPU Clusters.pdf\n            Eight Things to Know about Large Language Models.pdf\n            Explaining Neural Scaling Laws.pdf\n            GPT-4 Technical Report.pdf\n            Language Models are Few-Shot Learners.pdf\n            OpenAI's GPT-3 Language.pdf\n            OpenAI\u2019s CEO Saysthe Ageof Giant AI ModelsIs Already.pdf\n            Scaling Laws for Autoregressive Generative Modeling.pdf\n            Scaling Laws for Neural Language Models.pdf\n            Theoretical models that predict scaling laws.pdf\n            Training Compute-Optimal Large Language Models.pdf\n        processed/\n    tests/\n        conftest.py\n        test_bundler.py\n        test_deep_scan.py\n        test_drift_v1.py\n        test_drift_v2.py\n        test_infrastructure.py\n        test_orchestration.py\n        test_pdf_processor.py\n        test_phase7_rules.py\n        test_proxy.py\n        test_proxy_duplicate.py\n        test_runner.py\n        test_runner_duplicate.py\n        test_sample.py\n        test_shadow_observer.py\n        test_system.py\n        test_workflow.py\n        __init__.py\n    tools/\n        py.typed\n        analysis/\n            aletheia_mcp.py\n            analysis.rule_engine.py\n            bundler_constants.py\n            call_graph_normalizer.py\n            check_ai_analysis.py\n            codebase_scanner.py\n            cut_analysis.py\n            data_parser.py\n            drift_detector.py\n            fi_migration.py\n            governance_report.py\n            graph_transformer.pyi\n            hydrate_memory.py\n            lens_manager.pyi\n            main_platform.py\n            orchestrator.py\n            orchestrator_duplicate.py\n            py.typed\n            rule_engine.py\n            semantic_rebuilder.py\n            smoke_test.py\n            symbol_resolver.py\n            update_mcp_setup.py\n            verify_setup.py\n            __init__.py\n        api/\n            ws_runner.py\n            __init__.py\n        bundler/\n            Directory_bundler.py\n            workspace_packager.py\n            __init__.py\n        common/\n            .keep\n            canon_scanner.py\n            canon_scanner_duplicate.py\n            codebase_processor.py\n            embedding_client.py\n            embedding_client_duplicate.py\n            init_db.py\n            library_builder.py\n            main.py\n            metadata_extractor.py\n            metadata_extractor_duplicate.py\n            ocr_service.py\n            ocr_service_duplicate.py\n            pdf_processor.py\n            pdf_processor_duplicate.py\n            pdf_scanner.py\n            pdf_scanner_duplicate.py\n            profiler.py\n            profiler_duplicate.py\n            py.typed\n            retrieval_controller.py\n            rules_stub.py\n            rules_stub_duplicate.py\n            security.py\n            security_duplicate.py\n            security_utils.py\n            security_utils_duplicate.py\n            settings.py\n            settings_duplicate.py\n            workspace_packager_extension.py\n            workspace_packager_v2.3.py\n            __init__.py\n            __init___duplicate.py\n        core/\n            canon_db.py\n            canon_extractor.py\n            graph_transformer.py\n            ingest.py\n            lens_manager.py\n            py.typed\n            rebuild_verifier.py\n            shared_utils.py\n            __init__.py\n        debug/\n            debug_db.py\n            debug_queries.py\n            __init__.py\n        ingest/\n            ingest_manager.py\n            rag_bundler.py\n            rag_orchestrator.py\n            view_knowledge.py\n            __init__.py\n        physics_engine/\n            IRER_Validation_suite/\n                app.py\n                aste_hunter.py\n                core_engine.py\n                settings.py\n                solver_sdg.py\n                status.json\n                validation_pipeline.py\n                worker_sncgl_sdg.py\n                __init__.py\n                logs/\n                    agent_3_validation_pipeline_update/\n                        settings.py\n                        validation_pipeline.py\n                        worker_sncgl_sdg.py\n                        __init__.py\n                    agent_4/\n                        settings.py\n                        validation_pipeline.py\n                        worker_sncgl_sdg.py\n                        __init__.py\n                modules/\n                    analysis_&_Validation/\n                        certification_runner.py\n                        spectral_analysis.py\n                        spectral_validation.py\n                        tda_analyzer.py\n                        tda_taxonomy_validator.py\n                        __init__.py\n                    core_numerics_physics/\n                        fmia_dynamics_solver.py\n                        fmia_rhs.py\n                        numerics.py\n                        __init__.py\n                    I_O_&_Geometry/\n                        bssn_solver.py\n                        bssn_source_hook.py\n                        emergent_gravity_core.py\n                        geometry_metric.py\n                        __init__.py\n                    LOM/\n                        lom_translator.py\n                        __init__.py\n                templates/\n                    index.html\n        rebuild/\n            manual_rebuild.py\n            __init__.py\n        scanner/\n            heuristics.py\n            traversal.py\n            __init__.py\n        viz/\n            legacy_app.py\n            __init__.py\n    ui/\n        package.json\n        frontend/\n            index.css\n            index.html\n            index.tsx\n            metadata.json\n            package.json\n            postcss.config.js\n            tailwind.config.js\n            tsconfig.json\n            vite.config.ts\n            components/\n                .keep\n                AdaptiveMessage.tsx\n                ChatPage.tsx\n                CodeEditorPage.tsx\n                CodeReviewPage.tsx\n                CognitiveToolbar.tsx\n                DashboardPage.tsx\n                DevToolsPage.tsx\n                DialecticalView.tsx\n                DocReviewPage.tsx\n                DocumentEditorPage.tsx\n                HomePage.tsx\n                LMSetupPage.tsx\n                StackBuilderPage.tsx\n                TerminalBlock.tsx\n                ui.tsx\n            frontend_bundle_20260208_012859/\n                frontend_bundle_20260208_012859.yaml\n            hooks/\n                .keep\n            services/\n                .keep\n        static/\n            .keep\n    utils/\n        ocr_service.py\n        py.typed\n    workflows/\n        workflow_analyze.py\n        workflow_build_stack.py\n        workflow_extract.py\n        workflow_extract_duplicate.py\n        workflow_ingest.py\n        workflow_ingest_duplicate.py\n        workflow_validate_schema.py\n        __init__.py\n\n\n--- CRITICAL SYSTEM FILES ---\n\nFile: config/settings.py\n----------------------------------------\nimport os\nfrom pathlib import Path\nfrom config import settings\n\n\n\nclass Settings:\n    def __init__(self):\n        self.COLLECTION_TRUTH: str = os.environ.get('COLLECTION_TRUTH', 'truth_collection')\n        self.COLLECTION_TRACES: str = os.environ.get('COLLECTION_TRACES', 'traces')\n        self.NUM_RETRIEVAL_RESULTS: int = int(os.environ.get('NUM_RETRIEVAL_RESULTS', 5))\n        self.OCR_TEXT_DENSITY_THRESHOLD: int = int(os.environ.get('OCR_TEXT_DENSITY_THRESHOLD', 100))\n        self.CHUNK_SIZE: int = int(os.environ.get('CHUNK_SIZE', 500))\n        self.CHUNK_OVERLAP: int = int(os.environ.get('CHUNK_OVERLAP', 50))\n        self.MONGO_URI: str = os.environ.get('MONGO_URI', 'mongodb://localhost:27017')\n        self.DB_NAME: str = os.environ.get('DB_NAME', 'aletheia_db')\n        self.CHROMA_DB_PATH: Path = Path(os.environ.get('CHROMA_DB_PATH', './chroma_db'))\n        self.RAW_LANDING_DIR: Path = Path(os.environ.get('RAW_LANDING_DIR', './staging/incoming'))\n        self.LM_STUDIO_BASE_URL: str = os.environ.get('LM_STUDIO_BASE_URL', 'http://localhost:1234')\n        self.EMBEDDING_MODEL: str = os.environ.get('EMBEDDING_MODEL', 'nomic-embed-text-v1.5')\n        self.NOMIC_PREFIX: str = os.environ.get('NOMIC_PREFIX', 'search_document: ')\n        self.SSE_METRIC_KEY: str = os.environ.get('SSE_METRIC_KEY', 'sse_metric')\n        self.STABILITY_METRIC_KEY: str = os.environ.get('STABILITY_METRIC_KEY', 'stability_metric')\n        self.HASH_KEY: str = os.environ.get('HASH_KEY', 'hash_key')\n\n# Provide a module-level settings instance for import\nsettings = Settings()\n----------------------------------------\n\nFile: tools/analysis/main_platform.py\n----------------------------------------\nimport sys\nimport os\nimport json\nfrom pathlib import Path\nfrom typing import Optional, Any, Type\nfrom fastapi import FastAPI, Header, HTTPException, APIRouter\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom pydantic import BaseModel\n\n# --- 1. Path Setup (Must be first) ---\nfile_path = Path(__file__).resolve()\nPROJECT_ROOT = file_path.parents[2]  # ACP_v2 root\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.append(str(PROJECT_ROOT))\n\n# --- 2. Imports ---\nScannerClass: Any = None \ntry:\n    from tools.analysis.codebase_scanner import CodebaseScanner\n    ScannerClass = CodebaseScanner\nexcept ImportError:\n    print(\"[WARN] CodebaseScanner not found. /dev/initialize-context will fail.\")\n\n# Import DB logic\ntry:\n    from tools.core.canon_db import init_db\nexcept ImportError:\n    def init_db(db_path: Any = ...) -> Any:\n        pass\n\n# Import Routers (Safe Import block)\nws_router = None\nlens_router = None\ngraph_router = None\n\ntry:\n    from tools.api.ws_runner import router as ws_router_module\n    ws_router = ws_router_module\nexcept ImportError:\n    pass\n\ntry:\n    from tools.core.lens_manager import router as lens_router_module\n    lens_router = lens_router_module\nexcept ImportError:\n    pass\n    \ntry:\n    from tools.core.graph_transformer import router as graph_router_module\n    graph_router = graph_router_module\nexcept ImportError:\n    pass\n\n# --- 3. App Initialization ---\nCANON_DB_PATH = PROJECT_ROOT / \"memory/sql/project_meta.db\"\nif not CANON_DB_PATH.parent.exists():\n    CANON_DB_PATH.parent.mkdir(parents=True, exist_ok=True)\n\ntry: \n    init_db(str(CANON_DB_PATH))\nexcept: \n    pass\n\napp = FastAPI(title=\"Aletheia IDE\", version=\"2.0.0\")\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# --- 4. Helper Functions ---\nasync def verify_admin(x_api_key: Optional[str] = Header(None)):\n    allowed_keys = [\"dev_key\", \"your-admin-api-key\"]\n    if not x_api_key or x_api_key not in allowed_keys:\n        raise HTTPException(status_code=401, detail=\"Invalid or missing API key.\")\n\n# --- 5. Register Routers ---\nif ws_router: app.include_router(ws_router)\nif lens_router: app.include_router(lens_router)\nif graph_router: app.include_router(graph_router)\n\n# --- 6. Endpoints ---\n@app.get(\"/health\")\ndef health():\n    return {\"status\": \"ok\", \"system\": \"ACP_v2\"}\n\n@app.post(\"/dev/initialize-context\")\nasync def initialize_dev_context(x_api_key: str = Header(...)):\n    \"\"\"\n    Generates a full system snapshot to prime the LLM for internal audits.\n    \"\"\"\n    await verify_admin(x_api_key)\n    \n    if ScannerClass is None:\n        return {\"status\": \"error\", \"detail\": \"CodebaseScanner module not found.\"}\n\n    # Initialize the scanner relative to PROJECT_ROOT (Robust)\n    scanner = ScannerClass(str(PROJECT_ROOT))\n    snapshot = scanner.generate_system_snapshot()\n    \n    system_prompt = (\n        \"You are Aletheia, the internal system architect. \"\n        \"Below is the current state of your source code and configuration. \"\n        \"Use this context to answer technical questions about the platform structure, \"\n        \"identify bugs, and suggest architectural improvements.\\n\\n\"\n        f\"{snapshot}\"\n    )\n    \n    # Save to file for easy copy-pasting\n    output_path = PROJECT_ROOT / \"system_context.json\"\n    try:\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump({\n                \"status\": \"initialized\",\n                \"system_context\": system_prompt,\n                \"token_estimate\": len(system_prompt) / 4\n            }, f, indent=2)\n        print(f\"[INFO] System context saved to {output_path}\")\n    except Exception as e:\n        print(f\"[ERROR] Failed to save context file: {e}\")\n\n    return {\n        \"status\": \"initialized\",\n        \"system_context\": system_prompt,\n        \"token_estimate\": len(system_prompt) / 4\n    }\n\n# --- 7. Main Execution ---\nif __name__ == \"__main__\":\n    import uvicorn\n    port = int(os.environ.get(\"PORT\", 8090))\n    print(f\"Starting Aletheia Backend on port {port}... Root: {PROJECT_ROOT}\")\n    uvicorn.run(app, host=\"0.0.0.0\", port=port)\n----------------------------------------\n\nFile: aletheia_mcp.py\n----------------------------------------\nfrom mcp.server.fastmcp import FastMCP\nimport sqlite3\nimport sys\nimport os\nimport requests\nfrom pathlib import Path\nimport logging\nfrom typing import Any, Optional, List, Dict\n\n# --- 1. V2 PATH SETUP ---\n# We anchor everything to the ACP_v2 directory\nROOT_DIR = os.path.dirname(os.path.abspath(__file__))\nif ROOT_DIR not in sys.path:\n    sys.path.append(ROOT_DIR)\nDirectoryBundlerClass: Any = None\napp_settings: Any = None\ntry:\n    from tools.bundler.directory_bundler import DirectoryBundler as DB  #  type: ignore\n    from config.settings import settings as imported_settings  # type: ignore\n    \n    DirectoryBundlerClass = DB\n    app_settings = imported_settings\nexcept ImportError as e:\n    logging.error(f\"Failed to import V2 tools: {e}\")\n# Initialize V2 Bundler for logic reuse\n# We explicitly type 'bundler' as Any because it might be the class instance or None\nbundler: Any = DirectoryBundlerClass() if DirectoryBundlerClass else None\n# Initialize MCP\nmcp = FastMCP(\"ACP_v2 Factory Manager\")\nBOOT_API = \"http://localhost:8000\"\n# --- 3. HELPER: Self-Healing Logic (Adapted for V2) ---\ndef self_healing_tool(func: Any) -> Any:\n    \"\"\"Decorator: If tool fails, check V2 Bootloader health.\"\"\"\n    def wrapper(*args: Any, **kwargs: Any) -> Any:\n        try:\n            return func(*args, **kwargs)\n        except Exception as tool_error:\n            try:\n                # Check V2 Health Endpoint\n                response = requests.get(f\"{BOOT_API}/system/health\", timeout=2)\n                health = response.json()\n                \n                if health.get(\"status\") != \"HEALTHY\":\n                    return f\"\u26a0\ufe0f System Degraded (DB: {health.get('database')}). Please run 'Repair Stack' workflow.\"\n                \n                # If health is fine but tool failed, it's a code error\n                return f\"Tool Execution Error: {tool_error}\"\n            except Exception:\n                return f\"Critical Failure: {tool_error} (And Bootloader is unreachable at {BOOT_API})\"\n    return wrapper\n\ndef get_db() -> sqlite3.Connection:\n    # Points to V2 Canon DB\n    db_path = os.path.join(ROOT_DIR, \"canon.db\")\n    return sqlite3.connect(db_path, check_same_thread=False)\n# --- 4. V2 TOOLS ---\n@mcp.tool()\ndef read_system_state() -> str:\n    \"\"\"Reads the V2 Staging Area (Watchdog targets).\"\"\"\n    # In V2, we watch 'staging/incoming'\n    STAGING_ROOT = Path(ROOT_DIR) / \"staging\" / \"incoming\"\n    \n    if not STAGING_ROOT.exists():\n        return \"Error: V2 Staging directory not found.\"\n    \n    files = list(STAGING_ROOT.rglob(\"*\"))\n    summary = [f\"- {f.name} ({f.stat().st_size} bytes)\" for f in files if f.is_file()]\n    \n    return f\"Active V2 Staging Context ({len(summary)} files):\\n\" + \"\\n\".join(summary)\n@mcp.tool()\n@self_healing_tool\ndef scan_project_structure(path: str = \".\") -> str:\n    \"\"\"Uses DirectoryBundler to map the project.\"\"\"\n    if not bundler:\n        return \"Error: DirectoryBundler not loaded.\"\n    \n    # Use the method we specifically fixed types for\n    target_path = os.path.join(ROOT_DIR, path) if path == \".\" else path\n    \n    # We cast to str because mypy doesn't know the return type of the dynamic bundler\n    files = str(bundler.scan_directory(target_path)) \n    return f\"Scanned {len(files)} files in V2 structure.\"\n@mcp.tool()\ndef query_canon_db(query: str) -> str:\n    \"\"\"Read-only access to the V2 Canonical Database.\"\"\"\n    if \"DROP\" in query.upper() or \"DELETE\" in query.upper():\n        return \"Error: Read-only access permitted.\"\n    \n    conn = get_db()\n    try:\n        cursor = conn.cursor()\n        cursor.execute(query)\n        rows = cursor.fetchall()\n        return str(rows[:20]) + (\"...\" if len(rows) > 20 else \"\")\n    except Exception as e:\n        return f\"Database Error: {e}\"\n    finally:\n        conn.close()\n\nif __name__ == \"__main__\":\n    mcp.run()\n----------------------------------------\n\nFile: system_manifest.py\n----------------------------------------\nimport os\nimport sqlite3\nfrom pymongo import MongoClient\nfrom chromadb.client import Client as ChromaClient  # type: ignore\n\n# === Customization Variables ===\nSQLITE_DB_PATH = os.getenv(\"SQLITE_DB_PATH\", \"./data/app.db\")\nMONGODB_URI = os.getenv(\"MONGODB_URI\", \"mongodb://localhost:27017/\")\nMONGODB_DB = os.getenv(\"MONGODB_DB\", \"aletheia_db\")\nCHROMADB_PATH = os.getenv(\"CHROMADB_PATH\", \"./data/chromadb\")\n\n# === Health Check Logic ===\n\ndef check_sqlite_health():\n    try:\n        conn = sqlite3.connect(SQLITE_DB_PATH)\n        conn.execute(\"SELECT 1\")\n        conn.close()\n        return True, \"SQLite healthy\"\n    except Exception as e:\n        return False, f\"SQLite error: {e}\"\n\ndef check_mongodb_health():\n    try:\n        client: MongoClient = MongoClient(MONGODB_URI, serverSelectionTimeoutMS=2000)\n        client.server_info()  # Will throw if cannot connect\n        db = client[MONGODB_DB]\n        db.list_collection_names()\n        return True, \"MongoDB healthy\"\n    except Exception as e:\n        return False, f\"MongoDB error: {e}\"\n\ndef check_chromadb_health():\n    try:\n        client = ChromaClient(path=CHROMADB_PATH)\n        _ = client.list_collections()\n        return True, \"ChromaDB healthy\"\n    except Exception as e:\n        return False, f\"ChromaDB error: {e}\"\n\ndef system_health():\n    health = {}\n    sqlite_ok, sqlite_msg = check_sqlite_health()\n    mongo_ok, mongo_msg = check_mongodb_health()\n    chroma_ok, chroma_msg = check_chromadb_health()\n    health[\"sqlite\"] = {\"ok\": sqlite_ok, \"msg\": sqlite_msg}\n    health[\"mongodb\"] = {\"ok\": mongo_ok, \"msg\": mongo_msg}\n    health[\"chromadb\"] = {\"ok\": chroma_ok, \"msg\": chroma_msg}\n    health[\"all_ok\"] = sqlite_ok and mongo_ok and chroma_ok\n    return health\n\n----------------------------------------\n\nFile: tools/ingest/ingest_manager.py\n----------------------------------------\nimport os\nimport sys\nfrom pathlib import Path\n\n# Add project root to sys.path for robust import resolution\nPROJECT_ROOT = Path(__file__).resolve().parents[2]\nif str(PROJECT_ROOT) not in sys.path:\n    sys.path.append(str(PROJECT_ROOT))\nimport logging\nfrom typing import cast, Sequence, List, Dict, Any, Mapping\nfrom pathlib import Path\nfrom pymongo import MongoClient\nfrom pymongo.errors import BulkWriteError\nimport chromadb\nfrom datetime import datetime\nfrom config.settings import settings\n# FIX: Consistent imports\nfrom tools.common.pdf_processor import PDFProcessor\nfrom tools.common.codebase_processor import CodebaseProcessor\n\nfrom tools.common.embedding_client import EmbeddingClient\nfrom tools.common.metadata_extractor import extract_document_metadata\n\nlogger = logging.getLogger(__name__)\n\nclass IngestManager:\n    \"\"\"\n    Manages the complete ingestion pipeline for PDF and Text/Code documents.\n    \"\"\"\n    def __init__(self):\n        # Initialize Databases\n        self.mongo_client: Any = MongoClient(settings.MONGO_URI)\n        self.db = self.mongo_client[settings.DB_NAME]\n        self.collection_truth = self.db[settings.COLLECTION_TRUTH]\n        \n        # Initialize ChromaDB\n        self.chroma_client = chromadb.PersistentClient(path=str(settings.CHROMA_DB_PATH))\n        self.collection_index = self.chroma_client.get_or_create_collection(name=\"aletheia_index\")\n        \n        # Initialize Core Engines\n        self.pdf_processor = PDFProcessor()\n        self.codebase_processor = CodebaseProcessor()\n        self.embedder = EmbeddingClient()\n\n        # Ensure processed/failed directories exist\n        self.processed_dir = settings.RAW_LANDING_DIR.parent / \"processed\"\n        self.failed_dir = settings.RAW_LANDING_DIR.parent / \"failed\"\n        self.processed_dir.mkdir(parents=True, exist_ok=True)\n        self.failed_dir.mkdir(parents=True, exist_ok=True)\n        \n    def process_file(self, file_path: Path) -> bool:\n        \"\"\"\n        Processes a single file through the ingestion pipeline.\n        Routes to the appropriate processor based on file type.\n        \"\"\"\n        try:\n            logger.info(f\"Processing: {file_path.name}\")\n            # 1. Select Processor Strategy\n            if file_path.suffix.lower() == '.pdf':\n                chunks = list(self.pdf_processor.process_file(file_path))\n            else:\n                chunks = list(self.codebase_processor.process_file(file_path))\n            if not chunks:\n                logger.warning(f\"No usable content found in {file_path.name}\")\n                raise ValueError(\"No text extracted\")\n            # 2. Vectorization and Persistence\n            chroma_ids = []\n            chroma_embeddings = []\n            chroma_metadatas = []\n            mongo_docs = []\n            for i, chunk in enumerate(chunks):\n                content_text = chunk[\"content\"]\n                chunk_meta = chunk[\"metadata\"]\n                file_hash = chunk_meta.get('file_name', file_path.name)\n                doc_id = f\"{file_hash}_{i}\"\n                vector = self.embedder.get_embedding(content_text)\n                if not vector:\n                    continue\n                mongo_docs.append({\n                    \"file_hash\": file_hash,\n                    \"chunk_index\": i,\n                    \"content\": content_text,\n                    \"metadata\": chunk_meta,\n                    \"ingested_at\": datetime.utcnow().isoformat()\n                })\n                chroma_ids.append(doc_id)\n                chroma_embeddings.append(vector)\n                chroma_metadatas.append({\n                    \"file_hash\": file_hash,\n                    \"chunk_index\": i,\n                    \"page\": chunk_meta.get('page_number', 0),\n                    \"file_name\": chunk_meta.get('file_name', 'unknown')\n                })\n            if mongo_docs:\n                try:\n                    self.collection_truth.insert_many(mongo_docs, ordered=False)\n                except BulkWriteError as bwe:\n                    duplicates = [e for e in bwe.details['writeErrors'] if e['code'] == 11000]\n                    if len(duplicates) == len(mongo_docs):\n                        logger.info(f\"Skipping {file_path.name}: All chunks already exist in DB.\")\n                        shutil.move(str(file_path), str(self.processed_dir / file_path.name))\n                        return True\n                    elif duplicates:\n                        logger.info(f\"Partial insert for {file_path.name}: {len(duplicates)} duplicates skipped.\")\n                    else:\n                        error_msg = str(bwe).encode('ascii', 'replace').decode('ascii')\n                        logger.warning(f\"MongoDB Bulk Write Error: {error_msg}\")\n            if chroma_ids:\n                try:\n                    self.collection_index.add(\n                        ids=chroma_ids,\n                        embeddings=cast(Sequence[float], chroma_embeddings),\n                        metadatas=cast(List[Mapping[str, Any]], chroma_metadatas),\n                        documents=[d['content'] for d in mongo_docs]\n                    )\n                except Exception as e:\n                    logger.warning(f\"ChromaDB Write Warning for {file_path.name}: {e}\")\n            logger.info(f\"Successfully processed: {file_path.name}\")\n            shutil.move(str(file_path), str(self.processed_dir / file_path.name))\n            return True\n        except Exception as e:\n            safe_error = str(e).encode('ascii', 'replace').decode('ascii')\n            logger.error(f\"Error processing file {file_path.name}: {safe_error}\")\n            try:\n                shutil.move(str(file_path), str(self.failed_dir / file_path.name))\n            except Exception as move_err:\n                logger.error(f\"Failed to move {file_path.name} to failed dir: {move_err}\")\n            return False\n    \n    def process_all(self):\n        \"\"\"Processes all supported files in the raw landing directory recursively.\"\"\"\n        allowed_suffixes = {\".pdf\", \".txt\", \".py\", \".md\", \".json\", \".sh\", \".ps1\"}\n        all_files = []\n        for ext in allowed_suffixes:\n            for path in settings.RAW_LANDING_DIR.rglob(f\"*{ext}\"):\n                if path.is_file() and path.suffix.lower() in allowed_suffixes:\n                    all_files.append(path)\n\n        if not all_files:\n            logger.info(\"No files found to process.\")\n            return 0\n\n        processed_count = 0\n        for f in all_files:\n            if self.process_file(f):\n                processed_count += 1\n\n        logger.info(f\"Ingestion completed. Processed {processed_count}/{len(all_files)}.\")\n        return processed_count\n\nif __name__ == \"__main__\":\n    manager = IngestManager()\n    count = manager.process_all()\n    print(f\"Ingestion finished. Processed {count} files.\")\n\n----------------------------------------\n",
  "token_estimate": 8932.0
}